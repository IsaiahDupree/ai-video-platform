Gmail	Isaiah Dupree <isaiahdupree33@gmail.com>
(no subject)
Isaiah Dupree <isaiahdupree33@gmail.com>	Sun, Dec 28, 2025 at 6:15 PM
To: Isaiah Dupree <isaiahdupree33@gmail.com>
Skip to content
Chat history

You said:

how would we programatically call sound effects and how would we source these downloaded sound effects with using tools like re motion or motion canavas and also be easily referencedable using context for referencing them with ai
ChatGPT said:

You want 3 things working together:

A sound-effects “library” you own locally (downloaded + normalized)

A neutral way to “call” SFX on a timeline (so Remotion and Motion Canvas can consume it)

A manifest that AI can reference (IDs + tags + licensing + vibes)

Here’s the clean pattern.

1) Build an SFX library that’s AI-addressable

Folder layout (works for both engines)

assets/sfx/ → actual audio files (.wav preferred for editing; .mp3 ok)

assets/sfx/manifest.json → the “index” AI and renderers use

assets/sfx/CREDITS.md → attribution if needed

manifest.json (what AI references)

Each sound gets a stable ID + metadata:

{
  "version": "1.0",
  "items": [
    {
      "id": "ui_pop_01",
      "file": "ui/pop_01.wav",
      "tags": ["ui", "pop", "button", "light"],
      "description": "Short UI pop for button taps / bullet reveals",
      "intensity": 2,
      "category": "ui",
      "license": {
        "source": "mixkit",
        "requiresAttribution": false
      }
    },
    {
      "id": "whoosh_fast_02",
      "file": "transitions/whoosh_fast_02.wav",
      "tags": ["whoosh", "swoosh", "transition", "fast"],
      "description": "Fast whoosh for slide/zoom transitions",
      "intensity": 6,
      "category": "transition",
      "license": {
        "source": "zapsplat",
        "requiresAttribution": true,
        "attributionText": "Sound effects obtained from https://www.zapsplat.com"
      }
    }
  ]
}
Where to source SFX (and what to store in the manifest)

Mixkit: free SFX with a “free sound effects license” (generally usable in commercial/personal projects). Mixkit+1

Pixabay: content under their terms/license; they explicitly say CC0 content can be used commercially without attribution (subject to CC0 terms / their rules). Pixabay

Freesound: huge library, but sounds are under various Creative Commons licenses; use their API if you want programmatic search/download + metadata. Freesound+1

ZapSplat: attribution is required under their standard license for many free/basic use cases (store the attribution text!). ZapSplat - Download free sound effects+1

2) A neutral “audio events” timeline schema (engine-agnostic)

This is the simplest adapter-friendly representation:

{
  "fps": 30,
  "events": [
    { "type": "sfx", "sfxId": "ui_pop_01", "frame": 90, "volume": 0.8 },
    { "type": "sfx", "sfxId": "whoosh_fast_02", "frame": 210, "volume": 1.0 },
    { "type": "music", "src": "bg/lofi_01.mp3", "frame": 0, "volume": 0.25 }
  ]
}
AI’s job is to output only these sfxIds (validated against the manifest).

3) How to “call” SFX in Remotion (easy + fully programmatic)

Remotion expects assets in a public/ folder and you reference them using staticFile() Remotion+1.
Audio is added with <Html5Audio /> (it used to be called <Audio />). Remotion+1

Practical pattern

Put your sounds in: public/sfx/...

Render SFX by mapping timeline events → <Sequence from={frame}> + <Html5Audio />

Conceptually:

{events.filter(e => e.type === 'sfx').map(e => (
  <Sequence key={`${e.sfxId}-${e.frame}`} from={e.frame}>
    <Html5Audio
      src={staticFile(`sfx/${manifest[e.sfxId].file}`)}
      volume={e.volume ?? 1}
    />
  </Sequence>
))}
That’s the “programmatic call”: your JSON timeline is literally driving audio placement in the render graph.

4) How to do it in Motion Canvas (important constraint)

Motion Canvas supports adding an audio track by importing it and setting audio: in project.ts. Motion Canvas

But: Motion Canvas’ built-in workflow is essentially “one audio track on the project timeline” (great for voiceover + single bed). Motion Canvas
If you want lots of SFX hits like Remotion, you typically do:

Option A (recommended): pre-mix SFX into one track

Keep your events.json as the source of truth

Run a build step that mixes voiceover + music + sfx events into dist/audio_mix.wav

Point Motion Canvas audio: to that single mixed file

There’s even an open issue about “programmatically playing sound effects” in scene code (so treat pre-mix as the reliable route). GitHub

Option B: bake SFX into video clips

If a segment is a video file that already contains the sound, Motion Canvas can play/pause the video element, but that’s less flexible for lots of small SFX hits. Motion Canvas

5) Making it “AI-referenceable” (the trick that makes this scale)

The workflow that works:

AI retrieves candidate SFX from your manifest.json (by tags/description)

AI outputs only sfxIds + frame offsets (or “beat index” that you convert to frames)

Your renderer validates IDs and fails fast if an ID isn’t real

If you want it really smart:

Store embeddings for each SFX using its description + tags + category

Then AI can do “search SFX library for ‘sparkly magical reveal’” and get back IDs

You can also store “do-not-use-for” notes (e.g., “too harsh”, “low quality”)

The simplest “best” setup (so you can ship fast)

Remotion: use timeline events → <Sequence from> + <Html5Audio src={staticFile(...)}> Remotion+2Remotion+2

Motion Canvas: generate a single mixed audio file each render → set audio: in project.ts Motion Canvas

AI: only ever references sfxIds from manifest.json

If you want, I’ll give you:

a ready-to-drop manifest.json schema + Zod validator

an audio_events_v1 schema

a Remotion SfxLayer component

and a Node “audio mix builder” (FFmpeg-based) that turns events.json → audio_mix.wav for Motion Canvas.

You said:

yes that would be cool
ChatGPT said:

Bet — here’s a drop-in, adapter-friendly SFX system that:

stores your downloaded SFX in a library

lets AI reference them by stable IDs

renders them as native layered SFX in Remotion

and generates a single mixed audio track for Motion Canvas via FFmpeg

You asked for code in 6 segments — done.

1) Library layout + JSON schemas (manifest + events)

your-repo/
  assets/
    sfx/
      ui/
        pop_01.wav
      transitions/
        whoosh_fast_02.wav
      manifest.json
      CREDITS.md
  data/
    audio_events.json
  src/
    shared/
      audio-types.ts
  public/
    sfx/           (Remotion uses public/ for staticFile)
      ui/pop_01.wav
      transitions/whoosh_fast_02.wav
  dist/
    audio_mix.wav  (Motion Canvas will point to this)
// src/shared/audio-types.ts

export type SfxManifest = {
  version: string;
  items: Array<{
    id: string;              // stable key AI uses
    file: string;            // relative path inside sfx root (ui/pop_01.wav)
    tags: string[];
    description: string;
    intensity?: number;      // 1-10 (optional)
    category?: string;       // ui | transition | impact | glitch | ...
    license?: {
      source?: string;
      requiresAttribution?: boolean;
      attributionText?: string;
      url?: string;
    };
  }>;
};

export type AudioEvents = {
  fps: number;
  events: Array<
    | {
        type: "sfx";
        sfxId: string;        // must exist in manifest
        frame: number;        // when it triggers
        volume?: number;      // 0..1 (recommended)
      }
    | {
        type: "music";
        src: string;          // file path (e.g., assets/music/lofi.mp3)
        frame: number;
        volume?: number;
      }
    | {
        type: "voiceover";
        src: string;          // file path (e.g., assets/vo/vo.wav)
        frame: number;        // usually 0
        volume?: number;
      }
  >;
};
Example assets/sfx/manifest.json:

{
  "version": "1.0",
  "items": [
    {
      "id": "ui_pop_01",
      "file": "ui/pop_01.wav",
      "tags": ["ui", "pop", "button", "reveal", "light"],
      "description": "Short UI pop for bullet reveals",
      "intensity": 2,
      "category": "ui",
      "license": { "source": "mixkit", "requiresAttribution": false }
    },
    {
      "id": "whoosh_fast_02",
      "file": "transitions/whoosh_fast_02.wav",
      "tags": ["whoosh", "transition", "fast", "zoom"],
      "description": "Fast whoosh for slide/zoom transitions",
      "intensity": 6,
      "category": "transition",
      "license": {
        "source": "zapsplat",
        "requiresAttribution": true,
        "attributionText": "Sound effects obtained from https://www.zapsplat.com"
      }
    }
  ]
}
2) Zod validation + “safe lookup” helpers (AI-proofing)

// src/shared/audio-validate.ts
import fs from "node:fs";
import path from "node:path";
import { z } from "zod";

export const SfxItemZ = z.object({
  id: z.string().min(1),
  file: z.string().min(1),
  tags: z.array(z.string()).default([]),
  description: z.string().default(""),
  intensity: z.number().min(1).max(10).optional(),
  category: z.string().optional(),
  license: z
    .object({
      source: z.string().optional(),
      requiresAttribution: z.boolean().optional(),
      attributionText: z.string().optional(),
      url: z.string().optional(),
    })
    .optional(),
});

export const SfxManifestZ = z.object({
  version: z.string().min(1),
  items: z.array(SfxItemZ).min(1),
});

export const AudioEventZ = z.discriminatedUnion("type", [
  z.object({
    type: z.literal("sfx"),
    sfxId: z.string().min(1),
    frame: z.number().int().min(0),
    volume: z.number().min(0).max(2).optional(),
  }),
  z.object({
    type: z.literal("music"),
    src: z.string().min(1),
    frame: z.number().int().min(0),
    volume: z.number().min(0).max(2).optional(),
  }),
  z.object({
    type: z.literal("voiceover"),
    src: z.string().min(1),
    frame: z.number().int().min(0),
    volume: z.number().min(0).max(2).optional(),
  }),
]);

export const AudioEventsZ = z.object({
  fps: z.number().min(1).max(240),
  events: z.array(AudioEventZ),
});

export function loadManifest(manifestPath: string) {
  const raw = fs.readFileSync(manifestPath, "utf-8");
  const parsed = SfxManifestZ.parse(JSON.parse(raw));
  const map = new Map(parsed.items.map((it) => [it.id, it]));
  return { manifest: parsed, map };
}

export function loadAudioEvents(eventsPath: string) {
  const raw = fs.readFileSync(eventsPath, "utf-8");
  return AudioEventsZ.parse(JSON.parse(raw));
}

export function requireSfxFileById(opts: {
  sfxRootDir: string;         // absolute path to folder that contains files referenced by manifest.file
  sfxId: string;
  manifestMap: Map<string, { file: string }>;
}) {
  const item = opts.manifestMap.get(opts.sfxId);
  if (!item) throw new Error(`Unknown sfxId: ${opts.sfxId}`);
  const abs = path.join(opts.sfxRootDir, item.file);
  if (!fs.existsSync(abs)) throw new Error(`Missing SFX file for ${opts.sfxId}: ${abs}`);
  return abs;
}
Why this matters: AI can only output sfxIds — your validator blocks hallucinated IDs instantly.

3) Remotion: SfxLayer that programmatically places effects

// src/remotion/SfxLayer.tsx
import React, { useMemo } from "react";
import { Sequence, staticFile } from "remotion";
import { Html5Audio } from "remotion";
import type { AudioEvents } from "../shared/audio-types";

type Props = {
  events: AudioEvents;
  manifest: { [sfxId: string]: { file: string } };
  basePath?: string; // "sfx/" by default (inside public/)
};

export const SfxLayer: React.FC<Props> = ({ events, manifest, basePath = "sfx/" }) => {
  const sfxEvents = useMemo(
    () => events.events.filter((e) => e.type === "sfx"),
    [events.events]
  );

  return (
    <>
      {sfxEvents.map((e, idx) => {
        const item = manifest[e.sfxId];
        if (!item) {
          // Fail loud during dev; in prod you can choose to ignore
          throw new Error(`SfxLayer: Unknown sfxId "${e.sfxId}"`);
        }

        const src = staticFile(`${basePath}${item.file}`);
        const vol = e.volume ?? 1;

        return (
          <Sequence key={`${e.sfxId}-${e.frame}-${idx}`} from={e.frame}>
            <Html5Audio src={src} volume={vol} />
          </Sequence>
        );
      })}
    </>
  );
};
Usage:

// src/remotion/Root.tsx (or your composition component)
import React from "react";
import type { AudioEvents } from "../shared/audio-types";
import sfxManifest from "../../assets/sfx/manifest.json"; // or load JSON
import audioEvents from "../../data/audio_events.json";
import { SfxLayer } from "./SfxLayer";

export const MyComp: React.FC = () => {
  const manifestMap = Object.fromEntries(sfxManifest.items.map((it) => [it.id, { file: it.file }]));
  const events = audioEvents as AudioEvents;

  return (
    <>
      {/* visuals */}
      <SfxLayer events={events} manifest={manifestMap} />
    </>
  );
};
✅ Result: you can generate or edit audio_events.json programmatically and Remotion will place the SFX.

4) Motion Canvas: point to a single generated mix (dist/audio_mix.wav)

// motion-canvas/project.ts
import { makeProject } from "@motion-canvas/core";

export default makeProject({
  scenes: [
    // your scenes
  ],
  audio: "dist/audio_mix.wav", // generated by the FFmpeg builder (Segment 5)
});
Workflow:

Keep writing data/audio_events.json (AI can do it)

Run pnpm build:audio (or npm/yarn)

Motion Canvas renders with that one mixed track

This is the cleanest path because Motion Canvas is not “multi-SFX-layer native” the same way Remotion is.

5) Node + FFmpeg: build a mixed track from audio_events.json

This script:

reads manifest + events

converts frames → milliseconds

applies per-event volume

delays each SFX to its start time

mixes voiceover + music + all SFX into one file

// scripts/build-audio-mix.ts
import path from "node:path";
import fs from "node:fs";
import { spawnSync } from "node:child_process";
import { loadManifest, loadAudioEvents, requireSfxFileById } from "../src/shared/audio-validate";

type Args = {
  manifest: string;
  events: string;
  sfxRoot: string;
  out: string;
};

function parseArgs(): Args {
  const get = (k: string, def?: string) => {
    const idx = process.argv.indexOf(`--${k}`);
    return idx >= 0 ? process.argv[idx + 1] : def;
  };

  const manifest = get("manifest", "assets/sfx/manifest.json")!;
  const events = get("events", "data/audio_events.json")!;
  const sfxRoot = get("sfxRoot", "assets/sfx")!;
  const out = get("out", "dist/audio_mix.wav")!;

  return { manifest, events, sfxRoot, out };
}

function msFromFrame(frame: number, fps: number) {
  return Math.round((frame / fps) * 1000);
}

function ensureDirForFile(filePath: string) {
  fs.mkdirSync(path.dirname(filePath), { recursive: true });
}

function main() {
  const args = parseArgs();
  const cwd = process.cwd();

  const manifestPath = path.resolve(cwd, args.manifest);
  const eventsPath = path.resolve(cwd, args.events);
  const sfxRootDir = path.resolve(cwd, args.sfxRoot);
  const outPath = path.resolve(cwd, args.out);

  const { map: manifestMap } = loadManifest(manifestPath);
  const audioEvents = loadAudioEvents(eventsPath);

  const fps = audioEvents.fps;

  const voice = audioEvents.events.find((e) => e.type === "voiceover") ?? null;
  const music = audioEvents.events.find((e) => e.type === "music") ?? null;
  const sfx = audioEvents.events.filter((e) => e.type === "sfx");

  const inputs: string[] = [];
  const inputLabels: string[] = [];

  // Input 0: voiceover (optional)
  if (voice?.type === "voiceover") {
    inputs.push("-i", path.resolve(cwd, voice.src));
    inputLabels.push("voice");
  }

  // Input 1: music (optional)
  if (music?.type === "music") {
    inputs.push("-i", path.resolve(cwd, music.src));
    inputLabels.push("music");
  }

  // Next inputs: each sfx
  const sfxInputStartIndex = inputLabels.length;
  for (const ev of sfx) {
    const sfxPath = requireSfxFileById({
      sfxRootDir,
      sfxId: ev.sfxId,
      manifestMap,
    });
    inputs.push("-i", sfxPath);
    inputLabels.push(ev.sfxId);
  }

  if (inputs.length === 0) {
    throw new Error("No audio inputs found. Add voiceover/music/sfx to events.");
  }

  // Build filter_complex
  // For each input stream, apply volume, then (for sfx) apply adelay, then mix.
  const filters: string[] = [];
  const mixInputs: string[] = [];

  let streamIndex = 0;

  // Voiceover stream
  if (voice?.type === "voiceover") {
    const vol = voice.volume ?? 1;
    filters.push(`[${streamIndex}:a]volume=${vol}[v0]`);
    mixInputs.push("[v0]");
    streamIndex += 1;
  }

  // Music stream
  if (music?.type === "music") {
    const vol = music.volume ?? 0.25;
    // optional: loop music if short -> leave off for simplicity; better to supply correct length track
    filters.push(`[${streamIndex}:a]volume=${vol}[m0]`);
    mixInputs.push("[m0]");
    streamIndex += 1;
  }

  // SFX streams
  for (let i = 0; i < sfx.length; i++) {
    const ev = sfx[i];
    const vol = ev.volume ?? 1;
    const delayMs = msFromFrame(ev.frame, fps);
    // adelay needs "ms|ms" for stereo; if mono still fine
    const outTag = `s${i}`;
    filters.push(`[${streamIndex}:a]volume=${vol},adelay=${delayMs}|${delayMs}[${outTag}]`);
    mixInputs.push(`[${outTag}]`);
    streamIndex += 1;
  }

  // Mix them all. "duration=longest" keeps full length.
  const amixOut = "[mixout]";
  filters.push(`${mixInputs.join("")}amix=inputs=${mixInputs.length}:duration=longest:dropout_transition=0${amixOut}`);

  const filterComplex = filters.join(";");

  ensureDirForFile(outPath);

  const ffmpegArgs = [
    "-y",
    ...inputs,
    "-filter_complex",
    filterComplex,
    "-map",
    amixOut,
    // WAV output is easiest/most compatible
    "-ac",
    "2",
    "-ar",
    "48000",
    outPath,
  ];

  const result = spawnSync("ffmpeg", ffmpegArgs, { stdio: "inherit" });

  if (result.status !== 0) {
    throw new Error(`ffmpeg failed with exit code ${result.status}`);
  }

  console.log(`✅ Wrote mixed audio: ${outPath}`);
}

main();
6) package.json scripts + example events + quick run

{
  "scripts": {
    "build:audio": "ts-node scripts/build-audio-mix.ts --manifest assets/sfx/manifest.json --events data/audio_events.json --sfxRoot assets/sfx --out dist/audio_mix.wav",
    "render:motion": "motion-canvas render",
    "render:remotion": "remotion render"
  },
  "devDependencies": {
    "ts-node": "^10.9.2",
    "zod": "^3.24.0"
  }
}
Example data/audio_events.json:

{
  "fps": 30,
  "events": [
    { "type": "voiceover", "src": "assets/vo/episode_01.wav", "frame": 0, "volume": 1.0 },
    { "type": "music", "src": "assets/music/lofi_01.mp3", "frame": 0, "volume": 0.25 },

    { "type": "sfx", "sfxId": "ui_pop_01", "frame": 45, "volume": 0.8 },
    { "type": "sfx", "sfxId": "whoosh_fast_02", "frame": 120, "volume": 1.0 }
  ]
}
Install FFmpeg (mac):

brew install ffmpeg
Run:

pnpm build:audio
pnpm render:motion
And Remotion will play SFX directly from public/sfx/... when you render.

If you want the “AI context” piece to be chef’s kiss

Next step is I’ll give you a tiny function that turns your manifest.json into an LLM prompt context block (compact, token-efficient) + a validator that auto-corrects missing IDs by nearest-tag match.

Say the word: “context pack” and I’ll drop that in.

You said:

yes that would be cool
ChatGPT said:

Alright — here’s the “context pack” you can feed to an LLM (token-efficient), plus an auto-fix validator that:

rejects hallucinated sfxIds

optionally repairs them by finding the closest match using tags/description (cheap + deterministic)

outputs a clean audio_events.json your renderers can trust

(Still in 6 code segments.)

1) Compact “AI Context Pack” generator (manifest → small prompt payload)

// src/ai/sfx-context-pack.ts
import type { SfxManifest } from "../shared/audio-types";

export type SfxContextPack = {
  version: string;
  rules: string[];
  sfxIndex: Array<{
    id: string;
    tags: string[];
    desc: string;
    intensity?: number;
    category?: string;
  }>;
};

export function buildSfxContextPack(manifest: SfxManifest, opts?: { maxItems?: number }) {
  const maxItems = opts?.maxItems ?? 500;

  const sfxIndex = manifest.items
    .slice(0, maxItems)
    .map((it) => ({
      id: it.id,
      tags: it.tags?.slice(0, 10) ?? [],
      desc: (it.description ?? "").slice(0, 120),
      intensity: it.intensity,
      category: it.category,
    }));

  const pack: SfxContextPack = {
    version: manifest.version,
    rules: [
      "You MUST ONLY use sfxId values that exist in sfxIndex.id.",
      "Return ONLY JSON. No extra commentary.",
      "Prefer exact tag matches; if unsure, choose a safe generic UI pop or soft whoosh.",
      "Keep volumes between 0.3 and 1.0 unless asked otherwise.",
    ],
    sfxIndex,
  };

  return pack;
}
2) The prompt template (drop-in for your agent)

// src/ai/sfx-prompt.ts
import type { SfxContextPack } from "./sfx-context-pack";

export function makeSfxSelectionPrompt(args: {
  contextPack: SfxContextPack;
  narrativeBeats: Array<{ beatId: string; frame: number; text: string; action?: string }>;
  fps: number;
}) {
  return `
You are selecting sound effects for a video timeline.

SFX_LIBRARY_CONTEXT:
${JSON.stringify(args.contextPack)}

VIDEO_CONTEXT:
- fps: ${args.fps}
- beats: ${JSON.stringify(args.narrativeBeats)}

TASK:
Create audio events for SFX only.
Output JSON with this shape:
{
  "fps": ${args.fps},
  "events": [
    { "type": "sfx", "sfxId": "...", "frame": 0, "volume": 0.8 }
  ]
}

CONSTRAINTS:
- sfxId MUST match one of SFX_LIBRARY_CONTEXT.sfxIndex[].id exactly.
- Use frames from beats or nearby offsets (±10 frames).
- Avoid spamming: max 1 sfx per beat unless action is "transition".
`.trim();
}
3) Deterministic “closest match” fixer (no embeddings needed)

This uses a simple scoring:

+3 per overlapping tag

+2 if category matches

+1 if description contains key token

tie-break by intensity closeness (optional)

// src/ai/sfx-autofix.ts
import type { SfxManifest } from "../shared/audio-types";

function tokenize(s: string) {
  return s
    .toLowerCase()
    .replace(/[^a-z0-9\s_-]/g, " ")
    .split(/\s+/)
    .filter(Boolean);
}

export function bestSfxMatch(args: {
  manifest: SfxManifest;
  requestedIdOrHint: string;
  hintTags?: string[];
  hintCategory?: string;
}) {
  const hintTokens = new Set(tokenize(args.requestedIdOrHint));
  const hintTags = new Set((args.hintTags ?? []).map((t) => t.toLowerCase()));
  const hintCategory = args.hintCategory?.toLowerCase();

  let best: { id: string; score: number } | null = null;

  for (const it of args.manifest.items) {
    let score = 0;

    const tags = (it.tags ?? []).map((t) => t.toLowerCase());
    const descTokens = new Set(tokenize(it.description ?? ""));

    // Tag overlap
    for (const t of tags) {
      if (hintTokens.has(t)) score += 3;
      if (hintTags.has(t)) score += 3;
    }

    // Category match
    if (hintCategory && it.category?.toLowerCase() === hintCategory) score += 2;

    // Description overlap
    for (const tok of hintTokens) {
      if (descTokens.has(tok)) score += 1;
    }

    if (!best || score > best.score) best = { id: it.id, score };
  }

  return best;
}
4) Event validator + auto-fix pass (hallucination-proof)

// src/ai/validate-and-fix-events.ts
import type { AudioEvents, SfxManifest } from "../shared/audio-types";
import { bestSfxMatch } from "./sfx-autofix";

export type FixReport = {
  fixed: Array<{ from: string; to: string; frame: number; reason: string }>;
  rejected: Array<{ sfxId: string; frame: number; reason: string }>;
};

export function validateAndFixEvents(args: {
  events: AudioEvents;
  manifest: SfxManifest;
  allowAutoFix?: boolean; // true in dev, maybe false in prod
}) {
  const allowAutoFix = args.allowAutoFix ?? true;
  const idSet = new Set(args.manifest.items.map((i) => i.id));

  const report: FixReport = { fixed: [], rejected: [] };

  const cleaned: AudioEvents = {
    fps: args.events.fps,
    events: [],
  };

  for (const ev of args.events.events) {
    if (ev.type !== "sfx") {
      cleaned.events.push(ev);
      continue;
    }

    if (idSet.has(ev.sfxId)) {
      cleaned.events.push(ev);
      continue;
    }

    if (!allowAutoFix) {
      report.rejected.push({ sfxId: ev.sfxId, frame: ev.frame, reason: "Unknown sfxId" });
      continue;
    }

    // Try to infer from the requested string itself (often contains hint words)
    const best = bestSfxMatch({
      manifest: args.manifest,
      requestedIdOrHint: ev.sfxId,
    });

    if (!best || best.score <= 0) {
      report.rejected.push({ sfxId: ev.sfxId, frame: ev.frame, reason: "No reasonable match" });
      continue;
    }

    report.fixed.push({
      from: ev.sfxId,
      to: best.id,
      frame: ev.frame,
      reason: `Auto-mapped by tag/desc similarity (score=${best.score})`,
    });

    cleaned.events.push({ ...ev, sfxId: best.id });
  }

  return { cleaned, report };
}
5) “AI → validated events” pipeline glue (one function to rule it)

This is the piece you call in your orchestration (n8n / Make / server).

// src/ai/sfx-pipeline.ts
import fs from "node:fs";
import path from "node:path";
import type { AudioEvents, SfxManifest } from "../shared/audio-types";
import { buildSfxContextPack } from "./sfx-context-pack";
import { makeSfxSelectionPrompt } from "./sfx-prompt";
import { validateAndFixEvents } from "./validate-and-fix-events";

// Replace this with your actual LLM call
async function callLLM(prompt: string): Promise<string> {
  // MUST return JSON string from your model
  throw new Error("callLLM not implemented");
}

export async function generateSfxEventsWithGuardrails(args: {
  manifestPath: string;
  fps: number;
  beats: Array<{ beatId: string; frame: number; text: string; action?: string }>;
  allowAutoFix?: boolean;
  outPath?: string;
}) {
  const manifest: SfxManifest = JSON.parse(fs.readFileSync(args.manifestPath, "utf-8"));
  const contextPack = buildSfxContextPack(manifest);

  const prompt = makeSfxSelectionPrompt({
    contextPack,
    narrativeBeats: args.beats,
    fps: args.fps,
  });

  const raw = await callLLM(prompt);
  const proposed: AudioEvents = JSON.parse(raw);

  const { cleaned, report } = validateAndFixEvents({
    events: proposed,
    manifest,
    allowAutoFix: args.allowAutoFix ?? true,
  });

  if (args.outPath) {
    fs.mkdirSync(path.dirname(args.outPath), { recursive: true });
    fs.writeFileSync(args.outPath, JSON.stringify(cleaned, null, 2));
  }

  return { cleanedEvents: cleaned, report, prompt };
}
6) The “LLM-safe minimal manifest” trick (for huge libraries)

If your library grows, don’t send everything. Send a filtered context pack based on the scene/beat text.

// src/ai/sfx-context-filter.ts
import type { SfxManifest } from "../shared/audio-types";
import { buildSfxContextPack } from "./sfx-context-pack";

function tokens(s: string) {
  return new Set(
    s.toLowerCase().replace(/[^a-z0-9\s_-]/g, " ").split(/\s+/).filter(Boolean)
  );
}

export function buildFilteredSfxContextPack(args: {
  manifest: SfxManifest;
  beatText: string;
  maxItems?: number;
}) {
  const t = tokens(args.beatText);

  const scored = args.manifest.items
    .map((it) => {
      let score = 0;
      for (const tag of it.tags ?? []) if (t.has(tag.toLowerCase())) score += 3;
      const descTokens = tokens(it.description ?? "");
      for (const tok of t) if (descTokens.has(tok)) score += 1;
      return { it, score };
    })
    .sort((a, b) => b.score - a.score);

  const top = scored
    .filter((x) => x.score > 0)
    .slice(0, args.maxItems ?? 80)
    .map((x) => x.it);

  return buildSfxContextPack({ version: args.manifest.version, items: top });
}
Use it like:

per scene: build a filtered pack using that scene’s script text

reduces tokens massively

improves relevance

How you use this end-to-end (your “one-liner” mental model)

AI sees SfxContextPack → outputs audio_events.json using only sfxIds

validateAndFixEvents() cleans it (repair or reject)

Remotion: read events → place <Html5Audio/> sequences

Motion Canvas: run build:audio → mix to dist/audio_mix.wav → project uses that

If you want, I can also drop:

a tiny “beat extractor” that turns your script into beats (hook, reveal, transition, punchline)

a strict JSON response parser (so LLM output can’t break your pipeline)

and a “SFX anti-spam” pass (max density per 5 seconds)

You said:

yes that would be cool
ChatGPT said:

Cool — here are the 3 missing pieces you asked for (beat extractor, strict JSON parser, anti-spam), wired into an end-to-end pipeline.

1) Beat extractor (script → beats with frames + action)

// src/ai/beat-extractor.ts
export type Beat = {
  beatId: string;
  frame: number;      // start frame
  text: string;       // beat text
  action?: "hook" | "reveal" | "transition" | "punchline" | "cta" | "explain";
};

function slug(s: string) {
  return s
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, "_")
    .replace(/^_+|_+$/g, "")
    .slice(0, 40);
}

function detectAction(t: string): Beat["action"] {
  const s = t.toLowerCase();

  if (/(subscribe|follow|download|join|link in bio|waitlist|comment)/.test(s)) return "cta";
  if (/(but here’s the thing|plot twist|you won’t believe|here’s why|the truth|most people)/.test(s)) return "hook";
  if (/(so next|now let’s|moving on|meanwhile|then|next up|switch to)/.test(s)) return "transition";
  if (/(boom|gotcha|and that’s it|that’s the trick|mic drop)/.test(s)) return "punchline";
  if (/(here’s how|step|first|second|third|do this|use this)/.test(s)) return "explain";
  return "reveal";
}

/**
 * Simple, robust beat extractor:
 * - Prefer explicit markers like "Hook:", "Step 1:", "CTA:" if you use them
 * - Otherwise split into paragraphs/sentences
 * - Assign frames using a speech-rate estimate (WPM)
 */
export function extractBeatsFromScript(args: {
  script: string;
  fps: number;
  wpm?: number;            // default 165
  startFrame?: number;     // default 0
  minBeatWords?: number;   // default 6
}) {
  const fps = args.fps;
  const wpm = args.wpm ?? 165;
  const startFrame = args.startFrame ?? 0;
  const minBeatWords = args.minBeatWords ?? 6;

  const wordsPerSecond = wpm / 60;
  const framesPerWord = fps / wordsPerSecond;

  const rawLines = args.script
    .split(/\n+/)
    .map((l) => l.trim())
    .filter(Boolean);

  // If you have explicit labeled lines, keep them. Else chunk by punctuation.
  const chunks: string[] = [];
  for (const line of rawLines) {
    const hasLabel = /^[A-Za-z ]{2,12}:\s+/.test(line);
    if (hasLabel) {
      chunks.push(line.replace(/^[A-Za-z ]{2,12}:\s+/, "").trim());
      continue;
    }
    // Split long lines into sentence-ish beats
    const parts = line.split(/(?<=[.!?])\s+/);
    for (const p of parts) if (p.trim()) chunks.push(p.trim());
  }

  // Merge tiny chunks so we don’t create micro-beats
  const merged: string[] = [];
  let buf: string[] = [];
  let bufWords = 0;

  const countWords = (t: string) => t.split(/\s+/).filter(Boolean).length;

  for (const c of chunks) {
    const w = countWords(c);
    if (bufWords < minBeatWords) {
      buf.push(c);
      bufWords += w;
      continue;
    }
    merged.push(buf.join(" ").trim());
    buf = [c];
    bufWords = w;
  }
  if (buf.length) merged.push(buf.join(" ").trim());

  // Assign frames based on word count
  const beats: Array<{ beatId: string; frame: number; text: string; action?: Beat["action"] }> = [];
  let frameCursor = startFrame;

  for (let i = 0; i < merged.length; i++) {
    const text = merged[i];
    const wc = countWords(text);
    const durFrames = Math.max(1, Math.round(wc * framesPerWord));

    const action = detectAction(text);
    const beatId = `${i + 1}_${slug(text)}`;

    beats.push({ beatId, frame: frameCursor, text, action });
    frameCursor += durFrames;
  }

  return { beats, estimatedTotalFrames: frameCursor };
}
2) Strict JSON parser (extract JSON from messy model output + validate)

// src/ai/strict-json.ts
import { z } from "zod";

export const AudioEventZ = z.discriminatedUnion("type", [
  z.object({
    type: z.literal("sfx"),
    sfxId: z.string().min(1),
    frame: z.number().int().min(0),
    volume: z.number().min(0).max(2).optional(),
  }),
  z.object({
    type: z.literal("music"),
    src: z.string().min(1),
    frame: z.number().int().min(0),
    volume: z.number().min(0).max(2).optional(),
  }),
  z.object({
    type: z.literal("voiceover"),
    src: z.string().min(1),
    frame: z.number().int().min(0),
    volume: z.number().min(0).max(2).optional(),
  }),
]);

export const AudioEventsZ = z.object({
  fps: z.number().min(1).max(240),
  events: z.array(AudioEventZ),
});

function extractJsonCandidate(text: string) {
  // Prefer fenced JSON blocks first
  const fenced = text.match(/```(?:json)?\s*([\s\S]*?)\s*```/i);
  if (fenced?.[1]) return fenced[1].trim();

  // Otherwise try to slice from first "{" to last "}"
  const first = text.indexOf("{");
  const last = text.lastIndexOf("}");
  if (first >= 0 && last > first) return text.slice(first, last + 1).trim();

  // If no object, try array (rare)
  const aFirst = text.indexOf("[");
  const aLast = text.lastIndexOf("]");
  if (aFirst >= 0 && aLast > aFirst) return text.slice(aFirst, aLast + 1).trim();

  return null;
}

export function parseAndValidateAudioEvents(rawModelOutput: string) {
  const candidate = extractJsonCandidate(rawModelOutput);
  if (!candidate) {
    throw new Error("Model output did not contain JSON.");
  }

  let parsed: unknown;
  try {
    parsed = JSON.parse(candidate);
  } catch (e) {
    throw new Error(`Invalid JSON from model. Candidate snippet start: ${candidate.slice(0, 120)}`);
  }

  const validated = AudioEventsZ.parse(parsed);
  return validated;
}
3) Anti-spam + de-dupe (limit SFX density, keep transitions allowed)

// src/ai/sfx-thin.ts
import type { AudioEvents, SfxManifest } from "../shared/audio-types";

export function thinSfxEvents(args: {
  events: AudioEvents;
  manifest: SfxManifest;
  minSecondsBetweenSfx?: number;    // default 0.35
  allowBurstForTransitions?: boolean; // default true
  maxSfxPerSecond?: number;         // default 2 (hard ceiling)
}) {
  const minSecondsBetweenSfx = args.minSecondsBetweenSfx ?? 0.35;
  const allowBurstForTransitions = args.allowBurstForTransitions ?? true;
  const maxSfxPerSecond = args.maxSfxPerSecond ?? 2;

  const fps = args.events.fps;
  const minFrames = Math.max(1, Math.round(minSecondsBetweenSfx * fps));

  const byId = new Map(args.manifest.items.map((i) => [i.id, i]));
  const sorted = [...args.events.events].sort((a, b) => a.frame - b.frame);

  const out: typeof sorted = [];
  let lastSfxFrame = -Infinity;

  // For hard ceiling
  const sfxCountBySecond = new Map<number, number>();

  for (const ev of sorted) {
    if (ev.type !== "sfx") {
      out.push(ev);
      continue;
    }

    const meta = byId.get(ev.sfxId);
    const category = meta?.category?.toLowerCase() ?? "";
    const isTransition = category === "transition" || (meta?.tags ?? []).some((t) => t.toLowerCase() === "transition");

    const secondBucket = Math.floor(ev.frame / fps);
    const countThisSecond = sfxCountBySecond.get(secondBucket) ?? 0;

    // Hard ceiling: max SFX per second
    if (countThisSecond >= maxSfxPerSecond) continue;

    // Soft spacing rule: allow tighter spacing for transitions if enabled
    const tooClose = ev.frame - lastSfxFrame < minFrames;
    if (tooClose && !(allowBurstForTransitions && isTransition)) continue;

    // De-dupe: if same sfxId within 0.2s, drop
    const dedupeFrames = Math.max(1, Math.round(0.2 * fps));
    const recentSame = out
      .slice(-6)
      .some((x) => x.type === "sfx" && x.sfxId === ev.sfxId && Math.abs(x.frame - ev.frame) <= dedupeFrames);
    if (recentSame) continue;

    out.push(ev);
    lastSfxFrame = ev.frame;
    sfxCountBySecond.set(secondBucket, countThisSecond + 1);
  }

  return { ...args.events, events: out };
}
4) Context-pack filtering per script (keeps prompts small + relevant)

// src/ai/sfx-context-filter.ts
import type { SfxManifest } from "../shared/audio-types";
import { buildSfxContextPack } from "./sfx-context-pack";

function tokens(s: string) {
  return new Set(
    s.toLowerCase().replace(/[^a-z0-9\s_-]/g, " ").split(/\s+/).filter(Boolean)
  );
}

export function buildFilteredSfxContextPack(args: {
  manifest: SfxManifest;
  scriptText: string;
  maxItems?: number; // default 80
}) {
  const t = tokens(args.scriptText);
  const scored = args.manifest.items
    .map((it) => {
      let score = 0;
      for (const tag of it.tags ?? []) if (t.has(tag.toLowerCase())) score += 3;
      const dt = tokens(it.description ?? "");
      for (const tok of t) if (dt.has(tok)) score += 1;
      if (it.category && t.has(it.category.toLowerCase())) score += 2;
      return { it, score };
    })
    .sort((a, b) => b.score - a.score);

  const top = scored
    .filter((x) => x.score > 0)
    .slice(0, args.maxItems ?? 80)
    .map((x) => x.it);

  // Fallback: if nothing matched, send a small “safe defaults” subset
  const items = top.length
    ? top
    : args.manifest.items
        .filter((it) => ["ui", "transition"].includes((it.category ?? "").toLowerCase()))
        .slice(0, 40);

  return buildSfxContextPack({ version: args.manifest.version, items });
}
5) End-to-end pipeline: script → beats → LLM → strict parse → validate/fix → thin → write events

// src/ai/sfx-e2e.ts
import fs from "node:fs";
import path from "node:path";
import type { SfxManifest, AudioEvents } from "../shared/audio-types";
import { extractBeatsFromScript } from "./beat-extractor";
import { buildFilteredSfxContextPack } from "./sfx-context-filter";
import { makeSfxSelectionPrompt } from "./sfx-prompt";
import { parseAndValidateAudioEvents } from "./strict-json";
import { validateAndFixEvents } from "./validate-and-fix-events";
import { thinSfxEvents } from "./sfx-thin";

// Replace with your actual model call
async function callLLM(prompt: string): Promise<string> {
  throw new Error("callLLM not implemented");
}

export async function generateSfxEventsFromScript(args: {
  scriptPath: string;                 // e.g. data/script.txt
  manifestPath: string;               // assets/sfx/manifest.json
  fps: number;                        // e.g. 30
  outEventsPath: string;              // data/audio_events.sfx.json
  allowAutoFix?: boolean;             // default true
}) {
  const script = fs.readFileSync(args.scriptPath, "utf-8");
  const manifest: SfxManifest = JSON.parse(fs.readFileSync(args.manifestPath, "utf-8"));

  const { beats } = extractBeatsFromScript({ script, fps: args.fps });

  // Keep prompt small/relevant
  const contextPack = buildFilteredSfxContextPack({
    manifest,
    scriptText: script,
    maxItems: 80,
  });

  const prompt = makeSfxSelectionPrompt({
    contextPack,
    narrativeBeats: beats,
    fps: args.fps,
  });

  const raw = await callLLM(prompt);

  // Strict parse + schema validation
  const proposed = parseAndValidateAudioEvents(raw);

  // Fix hallucinated ids (optional)
  const { cleaned, report } = validateAndFixEvents({
    events: proposed as AudioEvents,
    manifest,
    allowAutoFix: args.allowAutoFix ?? true,
  });

  // Anti-spam
  const thinned = thinSfxEvents({
    events: cleaned,
    manifest,
    minSecondsBetweenSfx: 0.35,
    maxSfxPerSecond: 2,
    allowBurstForTransitions: true,
  });

  fs.mkdirSync(path.dirname(args.outEventsPath), { recursive: true });
  fs.writeFileSync(args.outEventsPath, JSON.stringify(thinned, null, 2));

  return { events: thinned, report, beats, prompt };
}
6) Quick usage + recommended command wiring

// scripts/run-sfx.ts
import { generateSfxEventsFromScript } from "../src/ai/sfx-e2e";

(async () => {
  const result = await generateSfxEventsFromScript({
    scriptPath: "data/script.txt",
    manifestPath: "assets/sfx/manifest.json",
    fps: 30,
    outEventsPath: "data/audio_events.sfx.json",
    allowAutoFix: true,
  });

  console.log("✅ wrote data/audio_events.sfx.json");
  console.log("Fix report:", result.report);
})();
// package.json (snippets)
{
  "scripts": {
    "sfx:events": "ts-node scripts/run-sfx.ts",
    "build:audio": "ts-node scripts/build-audio-mix.ts --manifest assets/sfx/manifest.json --events data/audio_events.json --sfxRoot assets/sfx --out dist/audio_mix.wav"
  }
}
Recommended flow

pnpm sfx:events → writes data/audio_events.sfx.json

Merge that into your main data/audio_events.json (voiceover/music + sfx)

Remotion uses events directly (layered)

Motion Canvas runs pnpm build:audio to output dist/audio_mix.wav

If you want, next I can drop a merge utility that combines:

voiceover/music events + audio_events.sfx.json

and ensures SFX frames don’t exceed your comp duration (hard clamp).

You said:

yes thats cool
ChatGPT said:

Nice — here’s the merge utility + duration clamp + “don’t exceed comp length” guard, plus an optional “snap-to-beat” helper so SFX always land cleanly.

1) Merge utility (base events + sfx events → final audio_events.json)

// src/audio/merge-events.ts
import type { AudioEvents } from "../shared/audio-types";

export function mergeAudioEvents(args: {
  base: AudioEvents;         // typically includes voiceover + music
  sfxOnly: AudioEvents;      // typically only sfx
}) {
  if (args.base.fps !== args.sfxOnly.fps) {
    throw new Error(`FPS mismatch: base=${args.base.fps} sfx=${args.sfxOnly.fps}`);
  }

  // Merge and sort by frame (stable tie-breaker keeps deterministic output)
  const merged = [...args.base.events, ...args.sfxOnly.events].sort((a, b) => {
    if (a.frame !== b.frame) return a.frame - b.frame;
    // deterministic ordering: voiceover/music first, then sfx
    const rank = (t: string) => (t === "voiceover" ? 0 : t === "music" ? 1 : 2);
    return rank(a.type) - rank(b.type);
  });

  return { fps: args.base.fps, events: merged } satisfies AudioEvents;
}
2) Clamp to composition duration (hard safety)

// src/audio/clamp-duration.ts
import type { AudioEvents } from "../shared/audio-types";

export function clampEventsToDuration(args: {
  events: AudioEvents;
  durationInFrames: number;
  dropIfBeyond?: boolean; // default true
}) {
  const dropIfBeyond = args.dropIfBeyond ?? true;

  const clamped = args.events.events
    .map((ev) => {
      if (ev.frame < 0) return { ...ev, frame: 0 };
      if (ev.frame >= args.durationInFrames) {
        if (dropIfBeyond) return null;
        return { ...ev, frame: Math.max(0, args.durationInFrames - 1) };
      }
      return ev;
    })
    .filter(Boolean) as AudioEvents["events"];

  return { ...args.events, events: clamped };
}
3) Snap SFX to nearest beat (optional but makes timing feel intentional)

// src/audio/snap-to-beats.ts
import type { AudioEvents } from "../shared/audio-types";

export type BeatRef = { beatId: string; frame: number; action?: string };

export function snapSfxToBeats(args: {
  events: AudioEvents;
  beats: BeatRef[];
  maxSnapFrames?: number; // default 6 frames
  onlyForActions?: Array<string>; // e.g. ["transition","hook","reveal"]
}) {
  const maxSnapFrames = args.maxSnapFrames ?? 6;
  const onlyForActions = args.onlyForActions;

  const beats = [...args.beats].sort((a, b) => a.frame - b.frame);

  const nearestBeatFrame = (frame: number) => {
    let best = beats[0]?.frame ?? frame;
    let bestDist = Infinity;
    for (const b of beats) {
      const d = Math.abs(b.frame - frame);
      if (d < bestDist) {
        bestDist = d;
        best = b.frame;
      }
    }
    return { best, bestDist };
  };

  const out = args.events.events.map((ev) => {
    if (ev.type !== "sfx") return ev;

    if (onlyForActions?.length) {
      // If you store action on event later, you can gate snapping here.
      // For now, we snap all unless you extend your event schema with action.
    }

    const { best, bestDist } = nearestBeatFrame(ev.frame);
    if (bestDist <= maxSnapFrames) return { ...ev, frame: best };
    return ev;
  });

  return { ...args.events, events: out };
}
4) One “finalize events” function (merge + snap + clamp)

// src/audio/finalize-events.ts
import type { AudioEvents } from "../shared/audio-types";
import { mergeAudioEvents } from "./merge-events";
import { clampEventsToDuration } from "./clamp-duration";
import { snapSfxToBeats, type BeatRef } from "./snap-to-beats";

export function finalizeAudioEvents(args: {
  base: AudioEvents;
  sfxOnly: AudioEvents;
  durationInFrames: number;
  beats?: BeatRef[];
  snap?: boolean;
}) {
  let merged = mergeAudioEvents({ base: args.base, sfxOnly: args.sfxOnly });

  if (args.snap && args.beats?.length) {
    merged = snapSfxToBeats({ events: merged, beats: args.beats, maxSnapFrames: 6 });
  }

  merged = clampEventsToDuration({ events: merged, durationInFrames: args.durationInFrames, dropIfBeyond: true });

  return merged;
}
5) Script that writes data/audio_events.json from base + sfx-only

Assumes:

data/audio_base.json contains voiceover/music

data/audio_events.sfx.json contains sfx

you provide duration (or you can import from Remotion later)

// scripts/merge-audio.ts
import fs from "node:fs";
import path from "node:path";
import type { AudioEvents } from "../src/shared/audio-types";
import { finalizeAudioEvents } from "../src/audio/finalize-events";

type Args = {
  base: string;
  sfx: string;
  out: string;
  duration: number;
  snap: boolean;
};

function parseArgs(): Args {
  const get = (k: string, def?: string) => {
    const idx = process.argv.indexOf(`--${k}`);
    return idx >= 0 ? process.argv[idx + 1] : def;
  };

  return {
    base: get("base", "data/audio_base.json")!,
    sfx: get("sfx", "data/audio_events.sfx.json")!,
    out: get("out", "data/audio_events.json")!,
    duration: Number(get("duration", "900")!), // default 30s @ 30fps
    snap: (get("snap", "true")! === "true"),
  };
}

(() => {
  const args = parseArgs();
  const base: AudioEvents = JSON.parse(fs.readFileSync(args.base, "utf-8"));
  const sfx: AudioEvents = JSON.parse(fs.readFileSync(args.sfx, "utf-8"));

  const finalEvents = finalizeAudioEvents({
    base,
    sfxOnly: sfx,
    durationInFrames: args.duration,
    snap: args.snap,
    // beats: optionally pass if you save beats.json
  });

  fs.mkdirSync(path.dirname(args.out), { recursive: true });
  fs.writeFileSync(args.out, JSON.stringify(finalEvents, null, 2));
  console.log(`✅ Wrote ${args.out}`);
})();
6) package.json wiring (the full “audio build” loop)

{
  "scripts": {
    "sfx:events": "ts-node scripts/run-sfx.ts",
    "audio:merge": "ts-node scripts/merge-audio.ts --duration 1800 --snap true",
    "audio:mix": "ts-node scripts/build-audio-mix.ts --manifest assets/sfx/manifest.json --events data/audio_events.json --sfxRoot assets/sfx --out dist/audio_mix.wav",
    "audio:all": "pnpm sfx:events && pnpm audio:merge && pnpm audio:mix"
  }
}
Run:

pnpm audio:all
Now you’ve got:

data/audio_events.json (final)

dist/audio_mix.wav (Motion Canvas ready)

Remotion can read data/audio_events.json and layer SFX directly.

If you want, I can add one more nice thing: a duration auto-detect that reads your Remotion composition duration (or Motion Canvas timeline length) so you don’t pass --duration manually.

You said:

yes that would be cool
ChatGPT said:

Perfect — here’s duration auto-detect with three fallbacks, so you never pass --duration again:

Remotion composition metadata (best) Remotion+1

Motion Canvas src/project.meta (if you use MC editor settings) motioncanvas.io

Beat-extractor estimate (always available)

All in 6 segments.

1) Shared VideoMeta type

// src/shared/video-meta.ts
export type VideoMeta = {
  source: "remotion" | "motion_canvas" | "beats";
  fps: number;
  durationInFrames: number;
  width?: number;
  height?: number;
};
2) Remotion: get durationInFrames programmatically (selectComposition)

// scripts/get-remotion-meta.ts
import path from "node:path";
import fs from "node:fs";
import { bundle } from "@remotion/bundler";
import { selectComposition } from "@remotion/renderer";
import type { VideoMeta } from "../src/shared/video-meta";

type Args = {
  entry: string;
  compositionId: string;
  out: string;
  props?: string; // JSON string for dynamic metadata
};

function arg(k: string, def?: string) {
  const i = process.argv.indexOf(`--${k}`);
  return i >= 0 ? process.argv[i + 1] : def;
}

(async () => {
  const args: Args = {
    entry: arg("entry", "src/remotion/index.ts")!,
    compositionId: arg("comp", "Main")!,
    out: arg("out", "data/video_meta.json")!,
    props: arg("props"),
  };

  const props = args.props ? JSON.parse(args.props) : {};

  const bundleLocation = await bundle({
    entryPoint: path.resolve(process.cwd(), args.entry),
    // you can pass webpackOverride if needed
  });

  const comp = await selectComposition({
    serveUrl: bundleLocation,
    id: args.compositionId,
    inputProps: props,
  });

  const meta: VideoMeta = {
    source: "remotion",
    fps: comp.fps,
    durationInFrames: comp.durationInFrames,
    width: comp.width,
    height: comp.height,
  };

  fs.mkdirSync(path.dirname(args.out), { recursive: true });
  fs.writeFileSync(args.out, JSON.stringify(meta, null, 2));
  console.log(`✅ wrote ${args.out}`);
})();
Remotion’s selectComposition() evaluates the composition metadata (including calculateMetadata()), which is exactly what you want for dynamic durations. Remotion+1

3) Motion Canvas: read duration + fps from src/project.meta

Motion Canvas v3 stores project settings in a meta file like src/project.meta, including rendering.fps and a shared.range field. motioncanvas.io

// scripts/get-motioncanvas-meta.ts
import fs from "node:fs";
import path from "node:path";
import type { VideoMeta } from "../src/shared/video-meta";

type Args = {
  meta: string; // src/project.meta
  out: string;
};

function arg(k: string, def?: string) {
  const i = process.argv.indexOf(`--${k}`);
  return i >= 0 ? process.argv[i + 1] : def;
}

(() => {
  const args: Args = {
    meta: arg("meta", "src/project.meta")!,
    out: arg("out", "data/video_meta.json")!,
  };

  const raw = fs.readFileSync(args.meta, "utf-8");
  const j = JSON.parse(raw);

  const fps = j?.rendering?.fps ?? j?.preview?.fps;
  const range = j?.shared?.range as [number | null, number | null] | undefined;

  if (!fps) throw new Error(`No fps found in ${args.meta}`);

  // Motion Canvas range is stored in the meta file; end can be null (meaning "full"). :contentReference[oaicite:4]{index=4}
  if (!range || range[1] == null) {
    throw new Error(`Range end is null/unknown in ${args.meta}. Set an end range in the editor or use beats/remotion meta.`);
  }

  const startSec = range[0] ?? 0;
  const endSec = range[1];

  const durationSeconds = Math.max(0, endSec - startSec);
  const durationInFrames = Math.max(1, Math.round(durationSeconds * fps));

  const meta: VideoMeta = {
    source: "motion_canvas",
    fps,
    durationInFrames,
    width: j?.shared?.size?.x,
    height: j?.shared?.size?.y,
  };

  fs.mkdirSync(path.dirname(args.out), { recursive: true });
  fs.writeFileSync(args.out, JSON.stringify(meta, null, 2));
  console.log(`✅ wrote ${args.out}`);
})();
4) Beats fallback: estimate duration from script (always works)

// scripts/get-beats-meta.ts
import fs from "node:fs";
import path from "node:path";
import type { VideoMeta } from "../src/shared/video-meta";
import { extractBeatsFromScript } from "../src/ai/beat-extractor";

type Args = { script: string; fps: number; out: string };

function arg(k: string, def?: string) {
  const i = process.argv.indexOf(`--${k}`);
  return i >= 0 ? process.argv[i + 1] : def;
}

(() => {
  const args: Args = {
    script: arg("script", "data/script.txt")!,
    fps: Number(arg("fps", "30")!),
    out: arg("out", "data/video_meta.json")!,
  };

  const script = fs.readFileSync(args.script, "utf-8");
  const { estimatedTotalFrames } = extractBeatsFromScript({ script, fps: args.fps });

  const meta: VideoMeta = {
    source: "beats",
    fps: args.fps,
    durationInFrames: estimatedTotalFrames,
  };

  fs.mkdirSync(path.dirname(args.out), { recursive: true });
  fs.writeFileSync(args.out, JSON.stringify(meta, null, 2));
  console.log(`✅ wrote ${args.out}`);
})();
5) Update merge script: read data/video_meta.json automatically

// scripts/merge-audio.ts  (replace duration args with --meta)
import fs from "node:fs";
import path from "node:path";
import type { AudioEvents } from "../src/shared/audio-types";
import type { VideoMeta } from "../src/shared/video-meta";
import { finalizeAudioEvents } from "../src/audio/finalize-events";

type Args = {
  base: string;
  sfx: string;
  out: string;
  meta: string;
  snap: boolean;
};

function arg(k: string, def?: string) {
  const i = process.argv.indexOf(`--${k}`);
  return i >= 0 ? process.argv[i + 1] : def;
}

(() => {
  const args: Args = {
    base: arg("base", "data/audio_base.json")!,
    sfx: arg("sfx", "data/audio_events.sfx.json")!,
    out: arg("out", "data/audio_events.json")!,
    meta: arg("meta", "data/video_meta.json")!,
    snap: (arg("snap", "true")! === "true"),
  };

  const base: AudioEvents = JSON.parse(fs.readFileSync(args.base, "utf-8"));
  const sfx: AudioEvents = JSON.parse(fs.readFileSync(args.sfx, "utf-8"));
  const meta: VideoMeta = JSON.parse(fs.readFileSync(args.meta, "utf-8"));

  if (base.fps !== meta.fps) {
    // optional: you can auto-resample later; for now fail loud
    throw new Error(`FPS mismatch: audio_base fps=${base.fps} vs meta fps=${meta.fps}`);
  }

  const finalEvents = finalizeAudioEvents({
    base,
    sfxOnly: sfx,
    durationInFrames: meta.durationInFrames,
    snap: args.snap,
  });

  fs.mkdirSync(path.dirname(args.out), { recursive: true });
  fs.writeFileSync(args.out, JSON.stringify(finalEvents, null, 2));
  console.log(`✅ Wrote ${args.out} using duration=${meta.durationInFrames} frames (${meta.source})`);
})();
6) package.json: one command for each “meta source” + auto chain

{
  "scripts": {
    "meta:remotion": "ts-node scripts/get-remotion-meta.ts --entry src/remotion/index.ts --comp Main --out data/video_meta.json",
    "meta:motion": "ts-node scripts/get-motioncanvas-meta.ts --meta src/project.meta --out data/video_meta.json",
    "meta:beats": "ts-node scripts/get-beats-meta.ts --script data/script.txt --fps 30 --out data/video_meta.json",

    "sfx:events": "ts-node scripts/run-sfx.ts",
    "audio:merge": "ts-node scripts/merge-audio.ts --meta data/video_meta.json --snap true",
    "audio:mix": "ts-node scripts/build-audio-mix.ts --manifest assets/sfx/manifest.json --events data/audio_events.json --sfxRoot assets/sfx --out dist/audio_mix.wav",

    "audio:all:remotion": "pnpm meta:remotion && pnpm sfx:events && pnpm audio:merge && pnpm audio:mix",
    "audio:all:motion": "pnpm meta:motion && pnpm sfx:events && pnpm audio:merge && pnpm audio:mix",
    "audio:all:beats": "pnpm meta:beats && pnpm sfx:events && pnpm audio:merge && pnpm audio:mix"
  }
}
If you’re using Remotion as the main renderer, run:

pnpm audio:all:remotion
If you’re using Motion Canvas and you’ve set a finite range in the editor (so shared.range has an end), run:

pnpm audio:all:motion
Want me to add the next upgrade: auto-pick meta source (try Remotion → else Motion → else beats) in a single meta:auto script?

You said:

yes lets continue with motion canavas
ChatGPT said:

Bet — let’s make Motion Canvas the “source of truth” renderer, and plug your AI SFX pipeline into it cleanly.

What Motion Canvas wants (audio + range + exporter)

Audio is attached at the project level (in src/project.ts) by importing a file and passing it into makeProject({ audio }). Motion Canvas

Range controls what gets rendered, and it’s part of the editor’s Video Settings. Motion Canvas

Audio offset is stored in seconds and can be adjusted in the editor (SHIFT-drag on the waveform). Motion Canvas+1

For direct “finished video” export, use the Video (FFmpeg) exporter (it installs FFmpeg automatically). Motion Canvas+2GitHub+2

1) Hook your generated mix into Motion Canvas (src/project.ts)

Assume you generate: dist/audio_mix.wav (recommended).

// src/project.ts
import {makeProject} from '@motion-canvas/core';
import main from './scenes/main?scene';

// Import the generated audio mix
import audio from '../dist/audio_mix.wav';

export default makeProject({
  scenes: [main],
  audio,
});
Motion Canvas docs show this exact “import audio + add to project” pattern. Motion Canvas

Tip: keep your mix as .wav or .ogg (those are the default “buffered assets” types). Motion Canvas

2) Make duration/range deterministic (so your auto-meta script works)

In the editor:

Open Video Settings

Set the Range to the exact portion you want rendered Motion Canvas

If range editing feels “locked”, Motion Canvas notes range editing is a SHIFT interaction in newer versions. Motion Canvas

The quickstart troubleshooting also calls out the timeline range selector bar and how to adjust it. Motion Canvas

Once your range end is set, your src/project.meta will contain a concrete end time, and your meta:motion approach becomes reliable.

3) Rendering path (Motion Canvas-native)

Rendering is done by clicking RENDER in the editor, and output goes into /output in the project directory. Motion Canvas

For a “finished mp4” workflow, install/use the Video (FFmpeg) exporter. Motion Canvas+2GitHub+2

4) “Automation-ish” rendering (no true CLI promised)

Motion Canvas has a handy feature: adding ?render to the editor URL will immediately start rendering when the editor opens. Motion Canvas

That means you can automate renders with something like Playwright by:

starting the editor server

opening http://localhost:9000/?render

waiting for output files to appear

(It’s not a pure headless CLI guarantee, but it’s very workable in CI.)

5) Recommended Motion Canvas pipeline commands

If you adopt what we built earlier:

pnpm meta:motion → reads src/project.meta for fps + range → writes data/video_meta.json

pnpm sfx:events → AI picks SFX with guardrails → writes data/audio_events.sfx.json

pnpm audio:merge → merges base+SFX + clamps to duration from meta → writes data/audio_events.json

pnpm audio:mix → builds dist/audio_mix.wav

Open editor and render (or ?render automation)

If you want, I’ll drop the next Motion Canvas-specific piece: a Playwright render runner (scripts/render-motion.ts) that opens ?render, watches /output for the mp4/image sequence, and exits with a clean success/fail code (perfect for n8n/Make triggers).

You said:

yes that would be cool
ChatGPT said:

Awesome — here’s a Playwright “render runner” for Motion Canvas that:

starts your Motion Canvas editor dev server

opens the editor with ?render (auto-start render) Motion Canvas

waits until the output video appears (or fails with logs)

exits cleanly (great for n8n/Make/CI)

And I’ll keep it in 6 code segments.

1) Install deps + package scripts

// package.json (additions)
{
  "devDependencies": {
    "@playwright/test": "^1.49.0"
  },
  "scripts": {
    "mc:dev": "motion-canvas",
    "mc:render:auto": "ts-node scripts/mc-render.ts",
    "audio:all:motion": "pnpm meta:motion && pnpm sfx:events && pnpm audio:merge && pnpm audio:mix && pnpm mc:render:auto"
  }
}
Motion Canvas supports triggering render from the editor URL using ?render. Motion Canvas
If you want finished MP4s, use the Video (FFmpeg) exporter (it installs FFmpeg automatically). Motion Canvas+1

2) Config file (where’s the editor, where’s output)

// scripts/mc-render.config.ts
export const mcRenderConfig = {
  editorUrl: "http://localhost:9000/?render", // ?render triggers rendering on open :contentReference[oaicite:3]{index=3}
  outputDir: "output",                        // Motion Canvas exporters write into the output directory :contentReference[oaicite:4]{index=4}
  // If you use the Video (FFmpeg) exporter, you’ll get a finished video. :contentReference[oaicite:5]{index=5}
  expectedExtensions: [".mp4", ".mov", ".webm", ".mkv"],

  // Timeouts
  serverStartupMs: 25_000,
  renderTimeoutMs: 15 * 60_000,

  // Optional: If your project is heavy, increase this.
  pollIntervalMs: 1000
};
3) Server launcher (spawns Motion Canvas editor)

// scripts/mc-start-server.ts
import { spawn } from "node:child_process";

export function startMcDevServer() {
  // Uses your package script so pnpm/npm/yarn config stays consistent
  const child = spawn(process.platform === "win32" ? "pnpm.cmd" : "pnpm", ["mc:dev"], {
    stdio: ["ignore", "pipe", "pipe"],
    env: process.env,
  });

  const logs: string[] = [];

  child.stdout.on("data", (d) => logs.push(d.toString()));
  child.stderr.on("data", (d) => logs.push(d.toString()));

  const stop = () => {
    if (!child.killed) child.kill("SIGTERM");
  };

  return { child, logs, stop };
}
4) Output watcher (detects the newest rendered video)

// scripts/mc-output-watch.ts
import fs from "node:fs";
import path from "node:path";

export function listFilesRecursive(dir: string): string[] {
  if (!fs.existsSync(dir)) return [];
  const entries = fs.readdirSync(dir, { withFileTypes: true });
  const out: string[] = [];
  for (const e of entries) {
    const p = path.join(dir, e.name);
    if (e.isDirectory()) out.push(...listFilesRecursive(p));
    else out.push(p);
  }
  return out;
}

export function newestFileWithExt(dir: string, exts: string[]) {
  const files = listFilesRecursive(dir).filter((f) =>
    exts.some((x) => f.toLowerCase().endsWith(x))
  );

  if (!files.length) return null;

  let best = files[0];
  let bestTime = fs.statSync(best).mtimeMs;

  for (const f of files.slice(1)) {
    const t = fs.statSync(f).mtimeMs;
    if (t > bestTime) {
      best = f;
      bestTime = t;
    }
  }
  return { path: best, mtimeMs: bestTime };
}

export function fileSizeStable(filePath: string, samples = 3, delayMs = 750) {
  // Checks that file size is no longer growing (render finished writing)
  let last = -1;
  for (let i = 0; i < samples; i++) {
    const size = fs.statSync(filePath).size;
    if (last !== -1 && size !== last) {
      last = size;
      Atomics.wait(new Int32Array(new SharedArrayBuffer(4)), 0, 0, delayMs);
      continue;
    }
    last = size;
    Atomics.wait(new Int32Array(new SharedArrayBuffer(4)), 0, 0, delayMs);
  }
  return true;
}
5) Playwright runner (opens ?render, waits for output, exits 0/1)

// scripts/mc-render.ts
import path from "node:path";
import fs from "node:fs";
import { chromium } from "@playwright/test";
import { mcRenderConfig as cfg } from "./mc-render.config";
import { startMcDevServer } from "./mc-start-server";
import { newestFileWithExt, fileSizeStable } from "./mc-output-watch";

function sleep(ms: number) {
  return new Promise((r) => setTimeout(r, ms));
}

async function waitForServer(url: string, timeoutMs: number) {
  const start = Date.now();
  while (Date.now() - start < timeoutMs) {
    try {
      const res = await fetch(url, { method: "GET" });
      if (res.ok) return true;
    } catch {}
    await sleep(500);
  }
  return false;
}

(async () => {
  const server = startMcDevServer();

  const ok = await waitForServer(cfg.editorUrl.replace("?render", ""), cfg.serverStartupMs);
  if (!ok) {
    console.error("❌ Motion Canvas server did not start in time.");
    console.error(server.logs.slice(-50).join(""));
    server.stop();
    process.exit(1);
  }

  const outputDir = path.resolve(process.cwd(), cfg.outputDir);
  fs.mkdirSync(outputDir, { recursive: true });

  const before = newestFileWithExt(outputDir, cfg.expectedExtensions)?.mtimeMs ?? 0;

  const browser = await chromium.launch({ headless: true });
  const page = await browser.newPage();

  // Visiting editor with ?render starts rendering immediately. :contentReference[oaicite:6]{index=6}
  await page.goto(cfg.editorUrl, { waitUntil: "domcontentloaded" });

  const start = Date.now();
  let found: { path: string; mtimeMs: number } | null = null;

  while (Date.now() - start < cfg.renderTimeoutMs) {
    const newest = newestFileWithExt(outputDir, cfg.expectedExtensions);
    if (newest && newest.mtimeMs > before) {
      // Wait until file size stabilizes (finished writing)
      fileSizeStable(newest.path);
      found = newest;
      break;
    }
    await sleep(cfg.pollIntervalMs);
  }

  await browser.close();
  server.stop();

  if (!found) {
    console.error("❌ Render timed out — no new output video detected.");
    console.error("Last logs:");
    console.error(server.logs.slice(-80).join(""));
    process.exit(1);
  }

  console.log(`✅ Render complete: ${found.path}`);
  process.exit(0);
})();
6) Make sure Motion Canvas is exporting video (FFmpeg exporter)

If you haven’t installed the Video (FFmpeg) exporter, Motion Canvas defaults to image sequences. Motion Canvas+1
The exporter docs note you don’t need to install FFmpeg manually; it’s installed with the exporter. Motion Canvas

Minimal checklist:

Install Video (FFmpeg) exporter in your project setup (or via editor)

Render once in editor to confirm .mp4 lands in output/

Then pnpm mc:render:auto will detect it

Run it

pnpm audio:all:motion
If you want, next we can make this even tighter by:

detecting the exact output filename (instead of “newest file wins”)

reading the configured exporter settings from the meta file (so it knows whether to expect mp4 or image sequence) Motion Canvas+1

You said:

yes that would be cool
ChatGPT said:

Awesome — here’s a Motion Canvas–first way to make SFX callable, sourceable, and AI-referenceable, without turning your project into chaos.

The reality check (Motion Canvas audio today)

Motion Canvas officially supports adding one audio track to the project (great for voiceover sync). Motion Canvas
There’s an open request to support programmatic SFX playback from scene code, because it’s not really built-in for renders right now. GitHub

So the clean production approach is:

AI picks SFX + timestamps (cue sheet)

you mix VO + SFX into one “audio bus” (FFmpeg)

Motion Canvas uses that bus as the project audio

you still use Time Events to sync visuals to spoken moments Motion Canvas

1) Build a “SFX Library” that AI can reference

Put downloaded files in a predictable folder, and create a manifest that the LLM can “see”.

(Code 1/6) audio/sfx/sfx.manifest.json

{
  "version": "1.0.0",
  "items": [
    {
      "id": "whoosh_short_01",
      "file": "audio/sfx/whoosh_short_01.wav",
      "tags": ["whoosh", "transition", "swipe", "fast"],
      "description": "Quick whoosh for zoom-in / cut transitions",
      "loudness": -14,
      "license": "pixabay-content-license",
      "sourceUrl": "https://pixabay.com/sound-effects/"
    },
    {
      "id": "pop_ui_02",
      "file": "audio/sfx/pop_ui_02.wav",
      "tags": ["pop", "ui", "button", "ping"],
      "description": "Small pop for UI click / point reveal",
      "loudness": -16,
      "license": "mixkit-sfx-free-license",
      "sourceUrl": "https://mixkit.co/free-sound-effects/"
    }
  ]
}
Why this works for AI: you paste (or auto-inject) this manifest into the prompt context, and the model can reliably output ids.

2) Auto-generate a typed import map (so humans + code stay sane)

(Code 2/6) scripts/build-sfx-index.ts

import fs from "node:fs";
import path from "node:path";

type Manifest = {
  version: string;
  items: Array<{ id: string; file: string }>;
};

const repoRoot = process.cwd();
const manifestPath = path.join(repoRoot, "audio/sfx/sfx.manifest.json");
const outPath = path.join(repoRoot, "src/sfx/index.ts");

const manifest = JSON.parse(fs.readFileSync(manifestPath, "utf8")) as Manifest;

const lines: string[] = [];
lines.push("// AUTO-GENERATED. DO NOT EDIT.");
lines.push("/* eslint-disable */");
lines.push("");

for (const item of manifest.items) {
  // Convert filesystem path to relative import from src/sfx/index.ts
  const relFromSrc = path.relative(path.join(repoRoot, "src"), path.join(repoRoot, item.file));
  const importPath = relFromSrc.replaceAll(path.sep, "/");
  lines.push(`import ${item.id} from "../${importPath}";`);
}

lines.push("");
lines.push("export const SFX = {");
for (const item of manifest.items) {
  lines.push(`  "${item.id}": ${item.id},`);
}
lines.push("} as const;");
lines.push("");
lines.push("export type SfxId = keyof typeof SFX;");

fs.mkdirSync(path.dirname(outPath), { recursive: true });
fs.writeFileSync(outPath, lines.join("\n"), "utf8");

console.log(`✅ Wrote ${outPath}`);
Run it:

node scripts/build-sfx-index.ts
Now you have SFX["whoosh_short_01"] available everywhere.

3) Let AI produce a cue sheet (the “programmatic call” layer)

(Code 3/6) audio/sfx/sfx.cues.schema.json (simple schema)

{
  "type": "object",
  "properties": {
    "sampleRate": { "type": "integer", "default": 48000 },
    "baseAudio": { "type": "string", "description": "Path to VO/music stem" },
    "cues": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "id": { "type": "string", "description": "SFX id from manifest" },
          "startSec": { "type": "number" },
          "gainDb": { "type": "number", "default": -6 },
          "pan": { "type": "number", "minimum": -1, "maximum": 1, "default": 0 }
        },
        "required": ["id", "startSec"]
      }
    }
  },
  "required": ["baseAudio", "cues"]
}
Example cue sheet (what AI outputs):

{
  "baseAudio": "audio/voice/vo.wav",
  "cues": [
    { "id": "pop_ui_02", "startSec": 1.20, "gainDb": -10 },
    { "id": "whoosh_short_01", "startSec": 3.85, "gainDb": -8 }
  ]
}
4) Mix VO + SFX into one audio file (FFmpeg)

Motion Canvas rendering saves output under ./output by default (configurable), with path format shown in the docs. Motion Canvas
For audio: you set audio: in the project config. Motion Canvas

(Code 4/6) scripts/mix-audio-bus.ts (FFmpeg adelay + amix)

import fs from "node:fs";
import path from "node:path";
import { spawnSync } from "node:child_process";

type CueSheet = {
  baseAudio: string;
  cues: Array<{ id: string; startSec: number; gainDb?: number }>;
};

const repoRoot = process.cwd();
const cuePath = path.join(repoRoot, "audio/sfx/sfx.cues.json");
const manifestPath = path.join(repoRoot, "audio/sfx/sfx.manifest.json");

const cue = JSON.parse(fs.readFileSync(cuePath, "utf8")) as CueSheet;
const manifest = JSON.parse(fs.readFileSync(manifestPath, "utf8")) as {
  items: Array<{ id: string; file: string }>;
};

const idToFile = new Map(manifest.items.map(i => [i.id, i.file]));
const outBus = path.join(repoRoot, "audio/voice/audio_bus.wav");
fs.mkdirSync(path.dirname(outBus), { recursive: true });

const inputs: string[] = ["-i", cue.baseAudio];
const filters: string[] = [];
const mixInputs: string[] = ["[0:a]"];

cue.cues.forEach((c, idx) => {
  const file = idToFile.get(c.id);
  if (!file) throw new Error(`Unknown SFX id: ${c.id}`);

  inputs.push("-i", file);

  const delayMs = Math.max(0, Math.round(c.startSec * 1000));
  const gain = c.gainDb ?? -8;
  const label = `s${idx}`;

  // [n:a]adelay=... , volume=... [sX]
  const inputIndex = idx + 1;
  filters.push(
    `[${inputIndex}:a]adelay=${delayMs}|${delayMs},volume=${gain}dB[${label}]`
  );
  mixInputs.push(`[${label}]`);
});

// Mix everything together
filters.push(`${mixInputs.join("")}amix=inputs=${mixInputs.length}:normalize=0[aout]`);

const args = [
  ...inputs,
  "-filter_complex", filters.join(";"),
  "-map", "[aout]",
  "-c:a", "pcm_s16le",
  outBus
];

console.log("ffmpeg", args.join(" "));
const r = spawnSync("ffmpeg", args, { stdio: "inherit" });
if (r.status !== 0) process.exit(r.status ?? 1);

console.log(`✅ Wrote audio bus: ${outBus}`);
5) Use the mixed bus in Motion Canvas

(Code 5/6) src/project.ts

import {makeProject} from '@motion-canvas/core';

import main from './scenes/main?scene';
import audioBus from '../audio/voice/audio_bus.wav';

export default makeProject({
  scenes: [main],
  audio: audioBus,
});
Now your SFX are “programmatically called” via the cue sheet (which AI generates), but the final render stays deterministic.

6) “Start rendering immediately” + detect the exact output artifact (robust way)

Motion Canvas output location is configurable via Vite plugin output and defaults to ./output. Motion Canvas
Because exporter settings live in meta files in v3+ Motion Canvas and exporter naming isn’t always predictable, the most reliable detection is:

snapshot the output directory before render

run render (your puppeteer flow)

after completion, diff filesystem and pick:

newest *.mp4/*.mov/*.webm or

newest frame directory containing lots of *.png/*.jpg

(Code 6/6) scripts/find-render-output.ts

import fs from "node:fs";
import path from "node:path";

type FileRec = { p: string; mtimeMs: number; isFile: boolean };

function walk(dir: string, out: FileRec[] = []): FileRec[] {
  if (!fs.existsSync(dir)) return out;
  for (const name of fs.readdirSync(dir)) {
    const p = path.join(dir, name);
    const st = fs.statSync(p);
    out.push({ p, mtimeMs: st.mtimeMs, isFile: st.isFile() });
    if (st.isDirectory()) walk(p, out);
  }
  return out;
}

function toMap(recs: FileRec[]) {
  return new Map(recs.map(r => [r.p, r.mtimeMs]));
}

function pickNewestVideo(changed: FileRec[]) {
  const vids = changed.filter(r =>
    r.isFile && /\.(mp4|mov|webm|mkv)$/i.test(r.p)
  );
  vids.sort((a, b) => b.mtimeMs - a.mtimeMs);
  return vids[0]?.p;
}

function pickNewestFrameDir(changed: FileRec[]) {
  // Find dirs that contain many images
  const dirs = new Set(
    changed.filter(r => r.isFile && /\.(png|jpg|jpeg|webp)$/i.test(r.p))
      .map(r => path.dirname(r.p))
  );
  let best: { dir: string; count: number; newest: number } | null = null;

  for (const d of dirs) {
    const files = fs.readdirSync(d).filter(f => /\.(png|jpg|jpeg|webp)$/i.test(f));
    if (files.length < 10) continue; // avoid stills
    const newest = Math.max(...files.map(f => fs.statSync(path.join(d, f)).mtimeMs));
    const cand = { dir: d, count: files.length, newest };
    if (!best || cand.newest > best.newest) best = cand;
  }
  return best?.dir;
}

// USAGE:
// 1) const before = snapshot()
// 2) render...
// 3) const after = snapshot(); diff + pick

const outputDir = process.argv[2] ?? path.join(process.cwd(), "output");
const beforeJson = process.argv[3]; // path to before snapshot JSON
const afterJson = process.argv[4];  // path to after snapshot JSON

if (!beforeJson || !afterJson) {
  console.error("Usage: node scripts/find-render-output.ts <outputDir> <before.json> <after.json>");
  process.exit(1);
}

const before = JSON.parse(fs.readFileSync(beforeJson, "utf8")) as FileRec[];
const after = JSON.parse(fs.readFileSync(afterJson, "utf8")) as FileRec[];

const beforeMap = toMap(before);
const changed = after.filter(r => (beforeMap.get(r.p) ?? 0) !== r.mtimeMs);

const newestVideo = pickNewestVideo(changed);
if (newestVideo) {
  console.log(newestVideo);
  process.exit(0);
}

const newestFrames = pickNewestFrameDir(changed);
if (newestFrames) {
  console.log(newestFrames);
  process.exit(0);
}

console.log("NO_OUTPUT_FOUND");
Where to source SFX safely (fast, practical)

Pixabay Sound Effects: marketed as royalty-free / no attribution required (subject to their content license). Pixabay+1

Mixkit Sound Effects: royalty-free, no attribution required, with their own license page. Mixkit+1

YouTube Audio Library: “copyright-safe” within YouTube Studio (still watch the license column). Google Help

Freesound: Creative Commons–licensed audio; license varies per asset (CC0 vs attribution, etc.). Freesound

If you want the “real” programmatic SFX (like Remotion has)

Remotion can layer many <Audio /> tracks with per-clip timing (so SFX are truly code-driven). Motion Canvas doesn’t quite ship that workflow yet (hence the issue). GitHub

If you tell me your current pipeline direction (Motion Canvas-only vs hybrid where Remotion handles final audio layering), I’ll snap this into a clean trend → brief → script → sfx_cues → audio_bus → render adapter that fits your Media Factory system.

You said:

Motion Canvas-only
ChatGPT said:

Yep — Motion Canvas can absolutely do “programmatic SFX,” but the clean way is not “play 30 little audio clips at runtime.” In Motion Canvas, you typically set one project audio track (voice/music/SFX all mixed together), and then drive visuals with time events (waitUntil) so everything stays perfectly in sync. Motion Canvas+1

Below is a Motion Canvas-only setup that makes SFX:

easy to download + organize

easy for AI to reference by context

easy to render (audio included in final video)

The Motion Canvas pattern that scales

1) Build an SFX “library” with AI-friendly metadata

You keep SFX files in a folder, and generate a sfx.manifest.json that contains:

id, file, tags, aliases

“use when…” descriptions (the part AI uses)

loudness/intensity hints (so mixing can be automated)

2) Let AI output a cue sheet (what to play, when)

AI produces sfx.cues.json like:

at 12.40s play whoosh_short_02 at -10 dB

at 12.55s play ui_click_07 at -18 dB

3) Render a single audio_bus.wav using FFmpeg

You mix voiceover + music + SFX cues into one file.

4) Motion Canvas imports audio_bus.wav as the project audio

Motion Canvas supports adding an audio track by importing it in project.ts and passing it via the audio property. Motion Canvas
If you use the Video (FFmpeg) exporter, you can include that audio in the final rendered video. Motion Canvas

Where Motion Canvas stores “timing truth” (meta file)

Modern Motion Canvas projects store a bunch of render/preview settings in src/project.meta (range, fps, resolution scale, etc.). That means your pipeline can read that meta file to know what it’s rendering against. Motion Canvas

Output location is also predictable:

default output dir is ./output, configurable in vite.config.ts Motion Canvas

editor rendering saves to /output (and has an “OUTPUT DIRECTORY” button) Motion Canvas

Implementation (6 code segments)

1) Folder layout + schemas (manifest + cues)

my-mc-project/
  audio/
    voiceover.wav
    music_bed.wav
    audio_bus.wav         <-- generated
    sfx/
      whoosh_short_02.wav
      ui_click_07.wav
      impact_soft_01.wav
  data/
    sfx.manifest.json     <-- generated
    sfx.cues.json         <-- AI generated (or edited)
  src/
    project.ts
    project.meta
    scenes/
      main.tsx
  tools/
    sfx/
      build_sfx_manifest.ts
      build_audio_bus.ts
// data/sfx.manifest.json (schema-ish example)
{
  "version": "1.0.0",
  "libraryName": "default_sfx_pack",
  "items": [
    {
      "id": "whoosh_short_02",
      "file": "audio/sfx/whoosh_short_02.wav",
      "tags": ["whoosh", "transition", "fast", "air"],
      "aliases": ["swipe", "swoosh", "scene_change"],
      "intensity": 0.55,
      "use_when": [
        "quick transition",
        "zoom-in",
        "topic switch",
        "before a reveal"
      ],
      "avoid_when": ["quiet emotional moment"],
      "notes": "Short, clean whoosh. No tail."
    }
  ]
}
// data/sfx.cues.json (AI output example)
{
  "version": "1.0.0",
  "sampleRate": 48000,
  "cues": [
    { "t": 3.20, "id": "ui_click_07", "gainDb": -18 },
    { "t": 6.90, "id": "whoosh_short_02", "gainDb": -10 },
    { "t": 7.05, "id": "impact_soft_01", "gainDb": -12 }
  ]
}
2) tools/sfx/build_sfx_manifest.ts (scan folder → manifest)

import fs from "node:fs";
import path from "node:path";

type ManifestItem = {
  id: string;
  file: string;
  tags: string[];
  aliases: string[];
  intensity: number;
  use_when: string[];
  avoid_when: string[];
  notes: string;
};

function slugId(filename: string) {
  return filename
    .replace(path.extname(filename), "")
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, "_")
    .replace(/^_+|_+$/g, "");
}

const projectRoot = process.cwd();
const sfxDir = path.join(projectRoot, "audio", "sfx");
const outPath = path.join(projectRoot, "data", "sfx.manifest.json");

if (!fs.existsSync(sfxDir)) {
  throw new Error(`Missing folder: ${sfxDir}`);
}

const files = fs
  .readdirSync(sfxDir)
  .filter((f) => /\.(wav|mp3|ogg|m4a)$/i.test(f));

const items: ManifestItem[] = files.map((f) => {
  const id = slugId(f);

  // You can enrich these fields later (or have AI fill them).
  return {
    id,
    file: path.posix.join("audio/sfx", f),
    tags: [],
    aliases: [],
    intensity: 0.5,
    use_when: [],
    avoid_when: [],
    notes: ""
  };
});

const manifest = {
  version: "1.0.0",
  libraryName: "default_sfx_pack",
  items
};

fs.mkdirSync(path.dirname(outPath), { recursive: true });
fs.writeFileSync(outPath, JSON.stringify(manifest, null, 2), "utf8");
console.log(`✅ Wrote ${outPath} (${items.length} items)`);
3) tools/sfx/build_audio_bus.ts (FFmpeg mix: voice + music + SFX cues)

import fs from "node:fs";
import path from "node:path";
import { spawnSync } from "node:child_process";

type Cue = { t: number; id: string; gainDb?: number };

const projectRoot = process.cwd();

const manifestPath = path.join(projectRoot, "data", "sfx.manifest.json");
const cuesPath = path.join(projectRoot, "data", "sfx.cues.json");

const voicePath = path.join(projectRoot, "audio", "voiceover.wav");
const musicPath = path.join(projectRoot, "audio", "music_bed.wav");
const outBusPath = path.join(projectRoot, "audio", "audio_bus.wav");

if (!fs.existsSync(manifestPath)) throw new Error("Missing sfx.manifest.json");
if (!fs.existsSync(cuesPath)) throw new Error("Missing sfx.cues.json");
if (!fs.existsSync(voicePath)) throw new Error("Missing voiceover.wav");

const manifest = JSON.parse(fs.readFileSync(manifestPath, "utf8"));
const cues: { cues: Cue[] } = JSON.parse(fs.readFileSync(cuesPath, "utf8"));

const idToFile = new Map<string, string>();
for (const item of manifest.items ?? []) {
  idToFile.set(item.id, path.join(projectRoot, item.file));
}

// Build ffmpeg inputs: voice, music (optional), then each SFX file.
const inputs: string[] = ["-i", voicePath];
const hasMusic = fs.existsSync(musicPath);
if (hasMusic) inputs.push("-i", musicPath);

// For each cue, add input -i sfxfile and later delay it with adelay
const cueInputs: { cue: Cue; inputIndex: number; file: string }[] = [];
for (const cue of cues.cues) {
  const file = idToFile.get(cue.id);
  if (!file) throw new Error(`Cue references unknown sfx id: ${cue.id}`);
  if (!fs.existsSync(file)) throw new Error(`Missing SFX file: ${file}`);

  const inputIndex = hasMusic ? 2 + cueInputs.length : 1 + cueInputs.length;
  inputs.push("-i", file);
  cueInputs.push({ cue, inputIndex, file });
}

// Filter graph:
// [0:a] is voice
// [1:a] is music (if present)
// each sfx input gets: volume + adelay
const filters: string[] = [];
const mixInputs: string[] = [];

mixInputs.push("[voice]");
filters.push(`[0:a]aresample=48000,volume=1.0[voice]`);

if (hasMusic) {
  mixInputs.push("[music]");
  filters.push(`[1:a]aresample=48000,volume=0.35[music]`);
}

let sfxCount = 0;
for (const { cue, inputIndex } of cueInputs) {
  const delayMs = Math.max(0, Math.round(cue.t * 1000));
  const gainDb = cue.gainDb ?? -12;
  const label = `sfx${sfxCount++}`;
  mixInputs.push(`[${label}]`);
  filters.push(
    `[${inputIndex}:a]aresample=48000,volume=${gainDb}dB,adelay=${delayMs}|${delayMs}[${label}]`
  );
}

const amix = `${mixInputs.join("")}amix=inputs=${mixInputs.length}:normalize=0[aout]`;
filters.push(amix);

const ffmpegArgs = [
  ...inputs,
  "-filter_complex",
  filters.join(";"),
  "-map",
  "[aout]",
  "-ac",
  "2",
  "-ar",
  "48000",
  "-y",
  outBusPath
];

console.log("ffmpeg", ffmpegArgs.join(" "));
const run = spawnSync("ffmpeg", ffmpegArgs, { stdio: "inherit" });

if (run.status !== 0) {
  throw new Error(`FFmpeg failed with exit code ${run.status}`);
}

console.log(`✅ Wrote audio bus: ${outBusPath}`);
4) src/project.ts (use the generated audio bus)

Motion Canvas audio track setup is literally: import the file and pass audio: in makeProject. Motion Canvas

import {makeProject} from "@motion-canvas/core";
import main from "./scenes/main?scene";

import audioBus from "../audio/audio_bus.wav";

export default makeProject({
  scenes: [main],
  audio: audioBus,
});
5) src/scenes/main.tsx (sync visuals to editable timeline events)

Use Time Events (waitUntil) so you can drag timing in the editor without rewriting code. Motion Canvas+1

import {makeScene2D, Txt} from "@motion-canvas/2d";
import {createRef} from "@motion-canvas/core";
import {waitUntil} from "@motion-canvas/core/lib/flow";

export default makeScene2D(function* (view) {
  const title = createRef<Txt>();

  view.add(
    <Txt
      ref={title}
      text={"TOPIC 1"}
      fontSize={96}
      opacity={0}
    />
  );

  // These events show up on the timeline and are draggable in the editor.
  yield* waitUntil("intro_in");
  yield* title().opacity(1, 0.25);

  yield* waitUntil("topic_switch");
  title().text("TOPIC 2");

  yield* waitUntil("outro");
  yield* title().opacity(0, 0.25);
});
6) tools/sfx/find_latest_render_output.ts (locate rendered output for automation)

Rendering output goes under the configured output directory (default ./output). Motion Canvas+1

import fs from "node:fs";
import path from "node:path";

const outDir = path.join(process.cwd(), "output");

// Heuristic: find newest file anywhere under /output (works for image sequences or ffmpeg exporter outputs)
function walk(dir: string, acc: string[] = []) {
  for (const name of fs.readdirSync(dir)) {
    const p = path.join(dir, name);
    const st = fs.statSync(p);
    if (st.isDirectory()) walk(p, acc);
    else acc.push(p);
  }
  return acc;
}

if (!fs.existsSync(outDir)) {
  console.error("No output directory found yet:", outDir);
  process.exit(1);
}

const files = walk(outDir).filter((p) => !p.includes(`${path.sep}still${path.sep}`));
if (files.length === 0) {
  console.error("No rendered files found under:", outDir);
  process.exit(1);
}

files.sort((a, b) => fs.statSync(b).mtimeMs - fs.statSync(a).mtimeMs);
console.log(files[0]);
Two practical notes (so you don’t get surprised)

Including audio in final video: if you use the Video (FFmpeg) exporter, enable “Include audio.” Motion Canvas

Audio offset: Motion Canvas lets you offset your audio track in the Video Settings timeline (SHIFT-drag) so your visuals can start before the audio hits. Motion Canvas+1

If you want the “AI context” part to be stupid simple: we can add a second file like data/sfx.macros.json that maps human intent (e.g. "minor_success", "soft_emphasis", "hard_cut") to 2–3 SFX IDs, so the model never has to browse your whole library—just pick a macro and intensity.

You said:

yes that would be cool
ChatGPT said:

Nice — SFX macros are the cheat code for Motion Canvas-only, because they let AI think in intent (emphasis, transition, success) instead of guessing individual files.

Here’s a full setup: macros → AI cue sheet → expand to real SFX IDs → FFmpeg audio_bus.wav.

1) data/sfx.macros.json (the macro library AI uses)

{
  "version": "1.0.0",
  "macros": [
    {
      "macroId": "soft_emphasis",
      "description": "Small accent on a key word, bullet, or highlight",
      "defaultGainDb": -16,
      "gainDbRange": [-22, -10],
      "candidates": [
        { "sfxId": "pop_ui_02", "weight": 0.55 },
        { "sfxId": "ui_click_07", "weight": 0.30 },
        { "sfxId": "tick_soft_01", "weight": 0.15 }
      ]
    },
    {
      "macroId": "transition_fast",
      "description": "Quick section switch, zoom, swipe, topic change",
      "defaultGainDb": -12,
      "gainDbRange": [-18, -6],
      "candidates": [
        { "sfxId": "whoosh_short_02", "weight": 0.70 },
        { "sfxId": "whoosh_fast_01", "weight": 0.30 }
      ]
    },
    {
      "macroId": "reveal",
      "description": "Moment of surprise, 'here’s the trick', a key reveal",
      "defaultGainDb": -12,
      "gainDbRange": [-18, -6],
      "candidates": [
        { "sfxId": "riser_short_01", "weight": 0.55 },
        { "sfxId": "sparkle_01", "weight": 0.45 }
      ]
    }
  ]
}
2) AI cue sheet becomes macro-based (not raw SFX)

// data/sfx.cues.macro.json (what AI outputs)
{
  "version": "1.0.0",
  "sampleRate": 48000,
  "cues": [
    { "t": 1.20, "macroId": "soft_emphasis", "intensity": 0.35 },
    { "t": 3.85, "macroId": "transition_fast", "intensity": 0.70 },
    { "t": 6.10, "macroId": "reveal", "intensity": 0.90 }
  ]
}
intensity is 0..1 and drives:

which candidate gets picked (optionally weighted randomness)

gain within the macro’s range (quiet → loud)

3) Types + Zod validation (reject hallucinated macros & IDs)

// src/sfx/macro-types.ts
import { z } from "zod";

export const MacroCandidateZ = z.object({
  sfxId: z.string().min(1),
  weight: z.number().min(0).max(1).default(1),
});

export const SfxMacroZ = z.object({
  macroId: z.string().min(1),
  description: z.string().default(""),
  defaultGainDb: z.number().default(-12),
  gainDbRange: z.tuple([z.number(), z.number()]).default([-18, -6]),
  candidates: z.array(MacroCandidateZ).min(1),
});

export const SfxMacrosZ = z.object({
  version: z.string().min(1),
  macros: z.array(SfxMacroZ).min(1),
});

export const MacroCueZ = z.object({
  t: z.number().min(0),
  macroId: z.string().min(1),
  intensity: z.number().min(0).max(1).default(0.5),
});

export const MacroCueSheetZ = z.object({
  version: z.string().min(1),
  sampleRate: z.number().optional().default(48000),
  cues: z.array(MacroCueZ).default([]),
});

export type SfxMacros = z.infer<typeof SfxMacrosZ>;
export type MacroCueSheet = z.infer<typeof MacroCueSheetZ>;
4) Macro expander (macro cues → real SFX cues with sfxId + gainDb)

// src/sfx/expand-macro-cues.ts
import type { SfxMacros, MacroCueSheet } from "./macro-types";

type ExpandedCue = { t: number; id: string; gainDb: number };

function clamp(n: number, a: number, b: number) {
  return Math.max(a, Math.min(b, n));
}

function weightedPick<T extends { weight: number }>(items: T[], seed: number) {
  // deterministic-ish pick using seed (no external RNG needed)
  const total = items.reduce((s, x) => s + (x.weight ?? 1), 0) || 1;
  const r = (Math.abs(Math.sin(seed)) % 1) * total;

  let acc = 0;
  for (const it of items) {
    acc += it.weight ?? 1;
    if (r <= acc) return it;
  }
  return items[items.length - 1];
}

export function expandMacroCueSheet(args: {
  macros: SfxMacros;
  sheet: MacroCueSheet;
  // Optional: validate that sfxIds exist in your manifest map
  validSfxIds?: Set<string>;
}) {
  const macroMap = new Map(args.macros.macros.map((m) => [m.macroId, m]));
  const out: ExpandedCue[] = [];

  for (const cue of args.sheet.cues) {
    const macro = macroMap.get(cue.macroId);
    if (!macro) throw new Error(`Unknown macroId: ${cue.macroId}`);

    const intensity = clamp(cue.intensity ?? 0.5, 0, 1);

    // Pick candidate (deterministic by time + macro length)
    const seed = cue.t * 997 + cue.macroId.length * 13;
    const candidate = weightedPick(macro.candidates, seed);

    if (args.validSfxIds && !args.validSfxIds.has(candidate.sfxId)) {
      throw new Error(`Macro "${macro.macroId}" candidate references unknown sfxId: ${candidate.sfxId}`);
    }

    // Gain interpolation: quieter at low intensity, louder at high intensity
    const [minDb, maxDb] = macro.gainDbRange;
    const gainDb = minDb + (maxDb - minDb) * intensity;

    out.push({ t: cue.t, id: candidate.sfxId, gainDb });
  }

  return out;
}
5) Build audio bus from macro cues (macro → expand → reuse your FFmpeg mixer)

This writes data/sfx.cues.json (expanded) so your existing build_audio_bus.ts keeps working.

// tools/sfx/compile-macro-cues.ts
import fs from "node:fs";
import path from "node:path";
import { SfxMacrosZ, MacroCueSheetZ } from "../../src/sfx/macro-types";
import { expandMacroCueSheet } from "../../src/sfx/expand-macro-cues";

const root = process.cwd();

const macrosPath = path.join(root, "data", "sfx.macros.json");
const macroCuesPath = path.join(root, "data", "sfx.cues.macro.json");
const expandedOutPath = path.join(root, "data", "sfx.cues.json"); // consumed by FFmpeg mixer
const manifestPath = path.join(root, "data", "sfx.manifest.json"); // optional for validating ids exist

const macros = SfxMacrosZ.parse(JSON.parse(fs.readFileSync(macrosPath, "utf-8")));
const macroSheet = MacroCueSheetZ.parse(JSON.parse(fs.readFileSync(macroCuesPath, "utf-8")));

let validSfxIds: Set<string> | undefined;
if (fs.existsSync(manifestPath)) {
  const manifest = JSON.parse(fs.readFileSync(manifestPath, "utf-8"));
  validSfxIds = new Set((manifest.items ?? []).map((x: any) => x.id));
}

const expanded = expandMacroCueSheet({ macros, sheet: macroSheet, validSfxIds });

const out = {
  version: "1.0.0",
  sampleRate: macroSheet.sampleRate ?? 48000,
  cues: expanded.map((c) => ({ t: c.t, id: c.id, gainDb: c.gainDb })),
};

fs.mkdirSync(path.dirname(expandedOutPath), { recursive: true });
fs.writeFileSync(expandedOutPath, JSON.stringify(out, null, 2));
console.log(`✅ Wrote expanded cues: ${expandedOutPath}`);
6) “AI context pack” for macros (tiny + reliable prompt payload)

Instead of dumping the entire SFX manifest into the model, give it only macros (and optionally a few candidate IDs per macro). That cuts tokens and prevents hallucinations.

// src/ai/macro-context-pack.ts
import type { SfxMacros } from "../sfx/macro-types";

export function buildMacroContextPack(macros: SfxMacros) {
  return {
    version: macros.version,
    rules: [
      "Use ONLY macroId values listed below.",
      "Return ONLY JSON matching the macro cue sheet shape.",
      "Use intensity 0..1 (0.2 subtle, 0.6 normal, 0.9 hype).",
      "Max 1 cue per beat unless it is a transition."
    ],
    macros: macros.macros.map((m) => ({
      macroId: m.macroId,
      description: m.description,
      // expose candidates to help the model understand what it will trigger
      candidates: m.candidates.map((c) => c.sfxId)
    }))
  };
}
How this plugs into Motion Canvas-only (your final loop)

AI outputs data/sfx.cues.macro.json (macro + time + intensity)

pnpm sfx:compile → expands to data/sfx.cues.json

pnpm audio:mix → produces audio/audio_bus.wav

Motion Canvas project.ts uses audio_bus.wav as project audio

You drive visuals with waitUntil("event_name") markers (timeline-editable)

Suggested scripts

{
  "scripts": {
    "sfx:manifest": "ts-node tools/sfx/build_sfx_manifest.ts",
    "sfx:compile": "ts-node tools/sfx/compile-macro-cues.ts",
    "audio:mix": "ts-node tools/sfx/build_audio_bus.ts",
    "audio:all:mc": "pnpm sfx:manifest && pnpm sfx:compile && pnpm audio:mix"
  }
}
If you want, I’ll also generate a starter macro set for your vibe (dev vlog + explainer + meme timing) with macros like:

micro_joke_hit

glitch_cut

text_ping

tiny_success

dramatic_pause_riser

error_buzz_soft

…and map them to whatever SFX you already have in audio/sfx/.

You said:

yes that would be cool
ChatGPT said:

Cool — here’s a starter macro set tuned for your formats (dev vlog + explainer + meme timing). You can paste this in as data/sfx.macros.json, then either:

rename candidate sfxIds to match your existing manifest IDs, or

run the “auto-map candidates” script below to map them to the closest tags in your manifest.

6 segments, as promised.

1) Starter macro set (data/sfx.macros.json)

{
  "version": "1.0.0",
  "macros": [
    {
      "macroId": "text_ping",
      "description": "When a keyword appears on screen, subtitle highlight, or bullet point pops in",
      "defaultGainDb": -16,
      "gainDbRange": [-24, -10],
      "candidates": [
        { "sfxId": "ui_pop_01", "weight": 0.40 },
        { "sfxId": "ui_click_01", "weight": 0.35 },
        { "sfxId": "tick_soft_01", "weight": 0.25 }
      ]
    },
    {
      "macroId": "transition_fast",
      "description": "Topic switch, zoom transition, swipe cut, jump to next section",
      "defaultGainDb": -12,
      "gainDbRange": [-18, -6],
      "candidates": [
        { "sfxId": "whoosh_short_01", "weight": 0.55 },
        { "sfxId": "whoosh_fast_01", "weight": 0.30 },
        { "sfxId": "swipe_whoosh_01", "weight": 0.15 }
      ]
    },
    {
      "macroId": "reveal_riser",
      "description": "Build up to a reveal (the 'here’s the trick' moment)",
      "defaultGainDb": -12,
      "gainDbRange": [-18, -6],
      "candidates": [
        { "sfxId": "riser_short_01", "weight": 0.60 },
        { "sfxId": "riser_sweep_01", "weight": 0.40 }
      ]
    },
    {
      "macroId": "impact_soft",
      "description": "Land a key fact, number, or 'this is why it matters' punch",
      "defaultGainDb": -12,
      "gainDbRange": [-18, -6],
      "candidates": [
        { "sfxId": "impact_soft_01", "weight": 0.70 },
        { "sfxId": "thump_soft_01", "weight": 0.30 }
      ]
    },
    {
      "macroId": "micro_joke_hit",
      "description": "Tiny comedic hit after a joke or sarcastic line (subtle, not obnoxious)",
      "defaultGainDb": -16,
      "gainDbRange": [-24, -10],
      "candidates": [
        { "sfxId": "rimshot_tiny_01", "weight": 0.40 },
        { "sfxId": "boing_soft_01", "weight": 0.35 },
        { "sfxId": "pop_ui_01", "weight": 0.25 }
      ]
    },
    {
      "macroId": "glitch_cut",
      "description": "Dev vlog 'bug moment', error reveal, or abrupt cut with glitch energy",
      "defaultGainDb": -10,
      "gainDbRange": [-16, -4],
      "candidates": [
        { "sfxId": "glitch_short_01", "weight": 0.60 },
        { "sfxId": "digital_zap_01", "weight": 0.40 }
      ]
    },
    {
      "macroId": "success_ding",
      "description": "Build passed, deploy succeeded, demo works, milestone achieved",
      "defaultGainDb": -14,
      "gainDbRange": [-22, -8],
      "candidates": [
        { "sfxId": "success_ding_01", "weight": 0.65 },
        { "sfxId": "chime_soft_01", "weight": 0.35 }
      ]
    },
    {
      "macroId": "warning_buzz_soft",
      "description": "Light warning / error / 'don’t do this' moment (not harsh)",
      "defaultGainDb": -14,
      "gainDbRange": [-22, -8],
      "candidates": [
        { "sfxId": "error_buzz_soft_01", "weight": 0.70 },
        { "sfxId": "notification_low_01", "weight": 0.30 }
      ]
    },
    {
      "macroId": "dramatic_pause_riser",
      "description": "Before a big statement, dramatic pause, or 'and then…' moment",
      "defaultGainDb": -16,
      "gainDbRange": [-24, -8],
      "candidates": [
        { "sfxId": "subtle_riser_01", "weight": 0.65 },
        { "sfxId": "air_swell_01", "weight": 0.35 }
      ]
    },
    {
      "macroId": "cta_sparkle",
      "description": "When CTA appears (comment keyword / join waitlist / link in bio)",
      "defaultGainDb": -16,
      "gainDbRange": [-24, -10],
      "candidates": [
        { "sfxId": "sparkle_short_01", "weight": 0.60 },
        { "sfxId": "chime_up_01", "weight": 0.40 }
      ]
    }
  ]
}
2) Tag intent map (so we can auto-map candidate IDs to your real manifest)

This maps each macro to “what it wants” (tags/keywords). Used by the auto-mapper.

// tools/sfx/macro-intents.ts
export const MACRO_INTENTS: Record<string, { tags: string[]; category?: string }> = {
  text_ping: { tags: ["ui", "pop", "click", "tick", "ping"], category: "ui" },
  transition_fast: { tags: ["whoosh", "swipe", "transition", "fast"], category: "transition" },
  reveal_riser: { tags: ["riser", "sweep", "reveal", "build"], category: "transition" },
  impact_soft: { tags: ["impact", "thump", "hit", "boom", "land"], category: "impact" },
  micro_joke_hit: { tags: ["rimshot", "boing", "funny", "pop"], category: "ui" },
  glitch_cut: { tags: ["glitch", "zap", "digital", "static"], category: "glitch" },
  success_ding: { tags: ["success", "ding", "chime", "win"], category: "ui" },
  warning_buzz_soft: { tags: ["error", "buzz", "warning", "alert"], category: "ui" },
  dramatic_pause_riser: { tags: ["riser", "swell", "air", "tension"], category: "transition" },
  cta_sparkle: { tags: ["sparkle", "chime", "shine"], category: "ui" }
};
3) Auto-map macro candidates to your real SFX manifest IDs

This takes your data/sfx.manifest.json and rewrites macro candidates so they actually exist.

// tools/sfx/auto-map-macros.ts
import fs from "node:fs";
import path from "node:path";
import { MACRO_INTENTS } from "./macro-intents";

type Manifest = {
  items: Array<{
    id: string;
    tags?: string[];
    description?: string;
    category?: string;
  }>;
};

type Macros = {
  version: string;
  macros: Array<{
    macroId: string;
    description: string;
    defaultGainDb: number;
    gainDbRange: [number, number];
    candidates: Array<{ sfxId: string; weight: number }>;
  }>;
};

function tokenize(s: string) {
  return new Set(
    s
      .toLowerCase()
      .replace(/[^a-z0-9\s_-]/g, " ")
      .split(/\s+/)
      .filter(Boolean)
  );
}

function scoreSfx(item: Manifest["items"][number], want: { tags: string[]; category?: string }) {
  const itTags = new Set((item.tags ?? []).map((t) => t.toLowerCase()));
  const descTokens = tokenize(item.description ?? "");
  const wantTags = want.tags.map((t) => t.toLowerCase());

  let score = 0;
  for (const t of wantTags) {
    if (itTags.has(t)) score += 4;
    if (descTokens.has(t)) score += 2;
    if (item.id.toLowerCase().includes(t)) score += 1;
  }
  if (want.category && (item.category ?? "").toLowerCase() === want.category.toLowerCase()) score += 2;

  return score;
}

function pickTop(manifest: Manifest, want: { tags: string[]; category?: string }, n = 3) {
  return [...manifest.items]
    .map((it) => ({ it, s: scoreSfx(it, want) }))
    .sort((a, b) => b.s - a.s)
    .filter((x) => x.s > 0)
    .slice(0, n)
    .map((x) => x.it);
}

const root = process.cwd();
const manifestPath = path.join(root, "data", "sfx.manifest.json");
const macrosPath = path.join(root, "data", "sfx.macros.json");
const outPath = path.join(root, "data", "sfx.macros.mapped.json");

const manifest = JSON.parse(fs.readFileSync(manifestPath, "utf-8")) as Manifest;
const macros = JSON.parse(fs.readFileSync(macrosPath, "utf-8")) as Macros;

const mapped: Macros = {
  ...macros,
  macros: macros.macros.map((m) => {
    const intent = MACRO_INTENTS[m.macroId];
    if (!intent) return m;

    const top = pickTop(manifest, intent, 3);
    if (top.length === 0) return m;

    // Replace candidates with actual ids from manifest
    const candidates = top.map((it, idx) => ({
      sfxId: it.id,
      weight: idx === 0 ? 0.6 : idx === 1 ? 0.25 : 0.15
    }));

    return { ...m, candidates };
  })
};

fs.writeFileSync(outPath, JSON.stringify(mapped, null, 2));
console.log(`✅ Wrote ${outPath}`);
Run:

pnpm ts-node tools/sfx/auto-map-macros.ts
Then rename data/sfx.macros.mapped.json → data/sfx.macros.json.

4) Macro cue sheet prompt (what AI should output)

Use this as your standard prompt wrapper.

// src/ai/macro-prompt.ts
export function makeMacroCuePrompt(args: {
  macroContextPack: unknown; // from buildMacroContextPack()
  beats: Array<{ beatId: string; t: number; text: string; action?: string }>;
}) {
  return `
You are creating sound effect cues for a Motion Canvas video.

MACRO_LIBRARY_CONTEXT:
${JSON.stringify(args.macroContextPack)}

BEATS:
${JSON.stringify(args.beats)}

OUTPUT:
Return ONLY JSON:
{
  "version": "1.0.0",
  "sampleRate": 48000,
  "cues": [
    { "t": 0.0, "macroId": "transition_fast", "intensity": 0.7 }
  ]
}

RULES:
- macroId MUST be one of the macros provided.
- intensity: 0.2 subtle, 0.6 normal, 0.9 hype.
- Max 1 cue per beat unless beat.action is "transition".
`.trim();
}
5) Convert beats (frames) → seconds (for macro cues)

Motion Canvas cues are easiest in seconds.

// src/ai/beats-to-seconds.ts
export function beatsFramesToSeconds(args: {
  beats: Array<{ beatId: string; frame: number; text: string; action?: string }>;
  fps: number;
}) {
  return args.beats.map((b) => ({
    beatId: b.beatId,
    t: Number((b.frame / args.fps).toFixed(3)),
    text: b.text,
    action: b.action
  }));
}
6) One “Motion Canvas audio build” command (macros → cues → bus)

Add this to scripts:

{
  "scripts": {
    "sfx:manifest": "ts-node tools/sfx/build_sfx_manifest.ts",
    "sfx:macros:map": "ts-node tools/sfx/auto-map-macros.ts",
    "sfx:compile": "ts-node tools/sfx/compile-macro-cues.ts",
    "audio:bus": "ts-node tools/sfx/build_audio_bus.ts",
    "audio:all:mc": "pnpm sfx:manifest && pnpm sfx:macros:map && pnpm sfx:compile && pnpm audio:bus"
  }
}
Run:

pnpm audio:all:mc
Next “Motion Canvas-only” upgrade (worth doing)

Want me to add a “macro placement policy” that automatically:

avoids stacking cues closer than 0.35s

adds an automatic transition_fast when your beats jump topics

adds text_ping only when a keyword is visually revealed (based on your scene data)

If yes, tell me your main format first: explainer listicle, dev vlog, or hybrid.

You said:

hybrid
ChatGPT said:

Hybrid = perfect for a policy-driven SFX layer because you have two rhythms:

Explainer rhythm (beats, steps, transitions, reveals)

Dev vlog rhythm (errors, wins, glitch cuts, comedic hits)

So we’ll do this:

Build a baseline macro cue plan deterministically from beats + visual reveals

(Optional) Let AI add extra cues on top, then run one thinning + de-dupe pass

Compile macros → SFX IDs → mix audio_bus → Motion Canvas renders

Below is the “macro placement policy” you asked for.

1) Data you already have + one tiny new file (visual reveal events)

You already have beats (from script extractor). Add a lightweight data/visual_reveals.json that your scene (or editor) can produce:

// src/shared/visual-reveals.ts
export type VisualReveal = {
  t: number; // seconds
  kind: "keyword" | "bullet" | "code" | "chart" | "cta" | "error" | "success";
  key?: string; // optional label like "RLS", "Supabase", "Deploy"
};

export type VisualRevealsFile = {
  version: "1.0.0";
  reveals: VisualReveal[];
};
Example file:

{
  "version": "1.0.0",
  "reveals": [
    { "t": 2.10, "kind": "keyword", "key": "Supabase" },
    { "t": 6.85, "kind": "error", "key": "Build failed" },
    { "t": 9.40, "kind": "success", "key": "Deploy OK" },
    { "t": 14.20, "kind": "cta", "key": "Comment TECH" }
  ]
}
2) Hybrid macro placement rules (the actual policy)

// src/sfx/policy-types.ts
export type BeatSec = { beatId: string; t: number; text: string; action?: string };

export type MacroCue = { t: number; macroId: string; intensity?: number; reason?: string };

export type PolicyConfig = {
  // spacing rules
  minGapSec: number;              // global min spacing
  minGapByMacroSec: Record<string, number>; // per-macro cooldown

  // limits
  maxCuesPerBeat: number;
  allowDoubleOnTransition: boolean;

  // thresholds for auto transition detection
  transitionGapSec: number;       // if beats are far apart, likely transition
  lowSimilarityThreshold: number; // if content changes a lot, transition
};
Recommended defaults for hybrid:

minGapSec = 0.35

transition cues allowed to cluster slightly (transition_fast cooldown ~0.20s)

one cue per beat, two only if beat.action === "transition"

3) The planner (beats + reveals → macro cues)

// src/sfx/macro-policy-hybrid.ts
import type { BeatSec, MacroCue, PolicyConfig } from "./policy-types";
import type { VisualReveal } from "../shared/visual-reveals";

function clamp01(n: number) {
  return Math.max(0, Math.min(1, n));
}

function tokenize(s: string) {
  return s.toLowerCase().replace(/[^a-z0-9\s]/g, " ").split(/\s+/).filter(Boolean);
}

// super cheap similarity: Jaccard overlap
function similarity(a: string, b: string) {
  const A = new Set(tokenize(a));
  const B = new Set(tokenize(b));
  if (A.size === 0 || B.size === 0) return 0;
  let inter = 0;
  for (const x of A) if (B.has(x)) inter++;
  const union = A.size + B.size - inter;
  return inter / union;
}

function intensityFromAction(action?: string) {
  if (action === "hook") return 0.75;
  if (action === "transition") return 0.70;
  if (action === "punchline") return 0.80;
  if (action === "cta") return 0.70;
  if (action === "reveal") return 0.75;
  return 0.55;
}

export function planHybridMacroCues(args: {
  beats: BeatSec[];
  reveals: VisualReveal[];
  config: PolicyConfig;
}) {
  const beats = [...args.beats].sort((a, b) => a.t - b.t);
  const reveals = [...args.reveals].sort((a, b) => a.t - b.t);

  const cues: MacroCue[] = [];

  // A) Visual reveals drive “text_ping” / CTA sparkle / dev moments
  for (const r of reveals) {
    if (r.kind === "keyword" || r.kind === "bullet" || r.kind === "code" || r.kind === "chart") {
      cues.push({ t: r.t, macroId: "text_ping", intensity: 0.45, reason: `visual:${r.kind}` });
    }
    if (r.kind === "cta") {
      cues.push({ t: r.t, macroId: "cta_sparkle", intensity: 0.70, reason: "visual:cta" });
    }
    if (r.kind === "error") {
      cues.push({ t: r.t, macroId: "warning_buzz_soft", intensity: 0.70, reason: "visual:error" });
      cues.push({ t: r.t + 0.08, macroId: "glitch_cut", intensity: 0.65, reason: "visual:error_followup" });
    }
    if (r.kind === "success") {
      cues.push({ t: r.t, macroId: "success_ding", intensity: 0.65, reason: "visual:success" });
    }
  }

  // B) Beats drive transitions + reveals + impacts + jokes
  for (let i = 0; i < beats.length; i++) {
    const b = beats[i];
    const prev = beats[i - 1];
    const next = beats[i + 1];

    const baseIntensity = intensityFromAction(b.action);

    // If beat explicitly marked transition, always add transition_fast
    if (b.action === "transition") {
      cues.push({ t: b.t, macroId: "transition_fast", intensity: baseIntensity, reason: "beat:transition" });
    }

    // Detect topic jump from prev->current
    if (prev) {
      const dt = b.t - prev.t;
      const sim = similarity(prev.text, b.text);

      const isBigJump = dt >= args.config.transitionGapSec;
      const isLowSim = sim <= args.config.lowSimilarityThreshold;

      if (isBigJump || isLowSim) {
        cues.push({
          t: b.t - 0.03, // slightly before the new beat lands
          macroId: "transition_fast",
          intensity: clamp01(0.6 + (isBigJump ? 0.15 : 0.0) + (isLowSim ? 0.15 : 0.0)),
          reason: `auto:topic_jump dt=${dt.toFixed(2)} sim=${sim.toFixed(2)}`
        });
      }
    }

    // Reveal moment (hook/reveal)
    if (b.action === "hook" || b.action === "reveal") {
      cues.push({ t: b.t - 0.08, macroId: "reveal_riser", intensity: clamp01(baseIntensity + 0.1), reason: `beat:${b.action}` });
      cues.push({ t: b.t + 0.02, macroId: "impact_soft", intensity: clamp01(baseIntensity), reason: "beat:land_reveal" });
    }

    // Punchline gets a micro joke hit
    if (b.action === "punchline") {
      cues.push({ t: b.t + 0.05, macroId: "micro_joke_hit", intensity: clamp01(baseIntensity), reason: "beat:punchline" });
    }

    // CTA beat gets sparkle (even if there wasn’t a visual CTA marker)
    if (b.action === "cta") {
      cues.push({ t: b.t, macroId: "cta_sparkle", intensity: clamp01(baseIntensity), reason: "beat:cta" });
    }

    // Optional: last beat tends to need a soft land
    if (!next) {
      cues.push({ t: b.t + 0.08, macroId: "impact_soft", intensity: 0.45, reason: "auto:outro_land" });
    }
  }

  return cues.sort((a, b) => a.t - b.t);
}
4) The “policy guard” pass (anti-spam + per-macro cooldown + per-beat limits)

// src/sfx/policy-guard.ts
import type { BeatSec, MacroCue, PolicyConfig } from "./policy-types";

export function applyPolicyGuards(args: {
  cues: MacroCue[];
  beats: BeatSec[];
  config: PolicyConfig;
}) {
  const cues = [...args.cues].sort((a, b) => a.t - b.t);
  const beats = [...args.beats].sort((a, b) => a.t - b.t);

  // Map time → beat index (nearest beat)
  const nearestBeatIndex = (t: number) => {
    let best = 0;
    let bestDist = Infinity;
    for (let i = 0; i < beats.length; i++) {
      const d = Math.abs(beats[i].t - t);
      if (d < bestDist) {
        bestDist = d;
        best = i;
      }
    }
    return best;
  };

  const out: MacroCue[] = [];
  const lastByMacro = new Map<string, number>();
  let lastGlobal = -Infinity;

  const beatCounts = new Map<number, number>();

  for (const c of cues) {
    const t = Math.max(0, c.t);

    // global spacing
    if (t - lastGlobal < args.config.minGapSec) continue;

    // macro cooldown
    const cooldown = args.config.minGapByMacroSec[c.macroId] ?? args.config.minGapSec;
    const last = lastByMacro.get(c.macroId) ?? -Infinity;
    if (t - last < cooldown) continue;

    // per beat limit
    const bi = nearestBeatIndex(t);
    const beat = beats[bi];
    const count = beatCounts.get(bi) ?? 0;

    const allowTwo =
      args.config.allowDoubleOnTransition && (beat?.action === "transition");

    const limit = allowTwo ? Math.max(args.config.maxCuesPerBeat, 2) : args.config.maxCuesPerBeat;
    if (count >= limit) continue;

    out.push({ ...c, t });
    lastGlobal = t;
    lastByMacro.set(c.macroId, t);
    beatCounts.set(bi, count + 1);
  }

  return out;
}
Suggested config for hybrid:

export const HYBRID_POLICY: PolicyConfig = {
  minGapSec: 0.35,
  minGapByMacroSec: {
    text_ping: 0.30,
    transition_fast: 0.20,
    reveal_riser: 0.60,
    impact_soft: 0.45,
    micro_joke_hit: 0.80,
    glitch_cut: 0.60,
    success_ding: 0.70,
    warning_buzz_soft: 0.70,
    dramatic_pause_riser: 0.80,
    cta_sparkle: 1.20
  },
  maxCuesPerBeat: 1,
  allowDoubleOnTransition: true,
  transitionGapSec: 1.2,
  lowSimilarityThreshold: 0.22
};
5) Convert policy cues → macro cue sheet file (then compile → audio_bus)

// tools/sfx/write-macro-cues-from-policy.ts
import fs from "node:fs";
import path from "node:path";
import type { VisualRevealsFile } from "../../src/shared/visual-reveals";
import type { BeatSec } from "../../src/sfx/policy-types";
import { planHybridMacroCues } from "../../src/sfx/macro-policy-hybrid";
import { applyPolicyGuards } from "../../src/sfx/policy-guard";
import { HYBRID_POLICY } from "../../src/sfx/policy-defaults";

const root = process.cwd();
const beatsPath = path.join(root, "data", "beats.sec.json");
const revealsPath = path.join(root, "data", "visual_reveals.json");
const outPath = path.join(root, "data", "sfx.cues.macro.json");

const beats = JSON.parse(fs.readFileSync(beatsPath, "utf-8")) as BeatSec[];
const revealsFile = fs.existsSync(revealsPath)
  ? (JSON.parse(fs.readFileSync(revealsPath, "utf-8")) as VisualRevealsFile)
  : { version: "1.0.0", reveals: [] };

const planned = planHybridMacroCues({ beats, reveals: revealsFile.reveals, config: HYBRID_POLICY });
const guarded = applyPolicyGuards({ cues: planned, beats, config: HYBRID_POLICY });

fs.mkdirSync(path.dirname(outPath), { recursive: true });
fs.writeFileSync(
  outPath,
  JSON.stringify({ version: "1.0.0", sampleRate: 48000, cues: guarded.map(c => ({ t: c.t, macroId: c.macroId, intensity: c.intensity ?? 0.6 })) }, null, 2)
);

console.log(`✅ Wrote ${outPath} (${guarded.length} cues)`);
6) The hybrid pipeline command (policy-only OR policy + AI)

Two modes:

A) Policy-only (fast, deterministic)

{
  "scripts": {
    "sfx:policy": "ts-node tools/sfx/write-macro-cues-from-policy.ts",
    "sfx:compile": "ts-node tools/sfx/compile-macro-cues.ts",
    "audio:bus": "ts-node tools/sfx/build_audio_bus.ts",
    "audio:all:mc:policy": "pnpm sfx:manifest && pnpm sfx:macros:map && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus"
  }
}
B) Policy + AI (AI can add spice, guards keep it sane)

Flow:

policy generates baseline sfx.cues.macro.json

AI generates sfx.cues.macro.ai.json

merge both → run guards → compile → bus

If you want, I’ll drop the merge script (merge-macro-cues.ts) that:

unions cues by time buckets (±0.08s)

prefers policy for “structural” macros (transition/reveal/cta)

lets AI add jokes/glitch hits when it makes sense

If you tell me one thing: where do your visual reveals come from today (subtitle keyword highlights? bullet animations? code blocks?), I can give you a tiny helper in your Motion Canvas scene that writes visual_reveals.json automatically during authoring (so you never hand-edit it).

You said:

yes that would be cool
ChatGPT said:

Bet — we can auto-generate data/visual_reveals.json straight from your Motion Canvas scenes during your normal ?render run, Motion Canvas-only, no manual editing.

The trick:

your scene code calls a tiny reveal() helper at the exact moments visuals appear

the helper timestamps using Motion Canvas useTime() (seconds since start) Motion Canvas

it stores reveals on globalThis

your Playwright render runner (already in your pipeline) captures the reveals and writes the JSON file

Also works beautifully with Time Events (waitUntil) because when you slide events on the timeline, the captured reveal times change automatically. Motion Canvas+1
And ?render auto-starts the render on open. Motion Canvas

1/6 — Types: src/shared/visual-reveals.ts

export type VisualRevealKind =
  | "keyword"
  | "bullet"
  | "code"
  | "chart"
  | "cta"
  | "error"
  | "success";

export type VisualReveal = {
  t: number; // seconds
  kind: VisualRevealKind;
  key?: string;
};

export type VisualRevealsFile = {
  version: "1.0.0";
  reveals: VisualReveal[];
};
2/6 — Recorder: src/sfx/reveal-recorder.ts

Uses useTime() for timestamps Motion Canvas and Motion Canvas useLogger() for clean logs. Motion Canvas

import {useLogger, useTime} from "@motion-canvas/core";
import {waitUntil} from "@motion-canvas/core/lib/flow";
import type {VisualReveal, VisualRevealKind} from "../shared/visual-reveals";

declare global {
  // eslint-disable-next-line no-var
  var __MC_REVEALS__: VisualReveal[] | undefined;
}

function ensureStore() {
  if (!globalThis.__MC_REVEALS__) globalThis.__MC_REVEALS__ = [];
  return globalThis.__MC_REVEALS__!;
}

export function resetReveals() {
  globalThis.__MC_REVEALS__ = [];
}

export function reveal(kind: VisualRevealKind, key?: string) {
  const t = Number(useTime().toFixed(3)); // seconds :contentReference[oaicite:5]{index=5}
  ensureStore().push({t, kind, key});
}

/**
 * Use with Time Events so the timestamp follows timeline edits. :contentReference[oaicite:6]{index=6}
 */
export function* revealAt(eventName: string, kind: VisualRevealKind, key?: string) {
  yield* waitUntil(eventName);
  reveal(kind, key);
}

/**
 * Print a single JSON blob your Playwright runner can capture.
 */
export function flushRevealsToLog() {
  const logger = useLogger(); // built-in logger :contentReference[oaicite:7]{index=7}
  const payload = {
    version: "1.0.0" as const,
    reveals: ensureStore(),
  };
  logger.info(`__MC_REVEALS__:${JSON.stringify(payload)}`);
}
3/6 — Use it in your scene: src/scenes/main.tsx

This is the “hybrid” feel: explainer pings + dev vlog error/success beats.

import {makeScene2D, Txt} from "@motion-canvas/2d";
import {createRef} from "@motion-canvas/core";
import {waitUntil} from "@motion-canvas/core/lib/flow";
import {resetReveals, reveal, revealAt, flushRevealsToLog} from "../sfx/reveal-recorder";

export default makeScene2D(function* (view) {
  resetReveals();

  const title = createRef<Txt>();
  view.add(<Txt ref={title} text={"BOOT"} fontSize={96} opacity={0} />);

  // EXPLAINER: keyword reveal (timeline-editable)
  yield* revealAt("kw_supabase", "keyword", "Supabase");
  title().text("Supabase");
  yield* title().opacity(1, 0.2);

  // EXPLAINER: bullet ping (direct reveal at the moment you animate it in)
  yield* waitUntil("bullet_1_in");
  reveal("bullet", "RLS policy");
  title().text("• RLS policy");

  // DEV VLOG: error moment
  yield* waitUntil("build_failed");
  reveal("error", "Build failed");
  title().text("Build failed 😭");

  // DEV VLOG: success moment
  yield* waitUntil("deploy_ok");
  reveal("success", "Deploy OK");
  title().text("Deploy OK ✅");

  // CTA moment (either waitUntil-based or direct)
  yield* revealAt("cta", "cta", "Comment TECH");

  // IMPORTANT: dump reveal JSON to logs at the end so your runner can write the file
  flushRevealsToLog();
});
4/6 — Capture reveals during ?render and write data/visual_reveals.json

Rendering saves outputs to /output by default Motion Canvas, and ?render starts rendering on open Motion Canvas.

Drop this into your existing Playwright runner (or replace your mc-render.ts with this version).

// scripts/mc-render.ts
import fs from "node:fs";
import path from "node:path";
import {chromium} from "@playwright/test";
import {mcRenderConfig as cfg} from "./mc-render.config";
import {startMcDevServer} from "./mc-start-server";
import {newestFileWithExt, fileSizeStable} from "./mc-output-watch";

function sleep(ms: number) {
  return new Promise((r) => setTimeout(r, ms));
}

async function waitForServer(url: string, timeoutMs: number) {
  const start = Date.now();
  while (Date.now() - start < timeoutMs) {
    try {
      const res = await fetch(url, {method: "GET"});
      if (res.ok) return true;
    } catch {}
    await sleep(500);
  }
  return false;
}

(async () => {
  const server = startMcDevServer();

  const ok = await waitForServer(cfg.editorUrl.replace("?render", ""), cfg.serverStartupMs);
  if (!ok) {
    console.error("❌ Motion Canvas server did not start in time.");
    console.error(server.logs.slice(-50).join(""));
    server.stop();
    process.exit(1);
  }

  const outputDir = path.resolve(process.cwd(), cfg.outputDir);
  fs.mkdirSync(outputDir, {recursive: true});

  const before = newestFileWithExt(outputDir, cfg.expectedExtensions)?.mtimeMs ?? 0;

  let revealPayloadText: string | null = null;

  const browser = await chromium.launch({headless: true});
  const page = await browser.newPage();

  page.on("console", (msg) => {
    const t = msg.text();
    if (t.startsWith("__MC_REVEALS__:")) {
      revealPayloadText = t.slice("__MC_REVEALS__:".length);
    }
  });

  // ?render auto-starts rendering when editor opens :contentReference[oaicite:10]{index=10}
  await page.goto(cfg.editorUrl, {waitUntil: "domcontentloaded"});

  const start = Date.now();
  let found: {path: string; mtimeMs: number} | null = null;

  while (Date.now() - start < cfg.renderTimeoutMs) {
    const newest = newestFileWithExt(outputDir, cfg.expectedExtensions);
    if (newest && newest.mtimeMs > before) {
      fileSizeStable(newest.path);
      found = newest;
      break;
    }
    await sleep(cfg.pollIntervalMs);
  }

  // Fallback: if we missed the console log, pull from globalThis
  let revealJson: any = null;
  if (revealPayloadText) {
    revealJson = JSON.parse(revealPayloadText);
  } else {
    revealJson = await page.evaluate(() => (globalThis as any).__MC_REVEALS__ ?? null);
    if (revealJson && Array.isArray(revealJson)) {
      revealJson = {version: "1.0.0", reveals: revealJson};
    }
  }

  await browser.close();
  server.stop();

  if (!found) {
    console.error("❌ Render timed out — no new output video detected.");
    console.error(server.logs.slice(-80).join(""));
    process.exit(1);
  }

  // Write reveals file for your SFX policy planner
  const outRevealsPath = path.join(process.cwd(), "data", "visual_reveals.json");
  fs.mkdirSync(path.dirname(outRevealsPath), {recursive: true});
  fs.writeFileSync(outRevealsPath, JSON.stringify(revealJson ?? {version: "1.0.0", reveals: []}, null, 2));
  console.log(`✅ Wrote visual reveals: ${outRevealsPath}`);

  console.log(`✅ Render complete: ${found.path}`);
  process.exit(0);
})();
5/6 — Your hybrid SFX policy now “just works”

Because your existing policy script already reads data/visual_reveals.json, you can now do:

pnpm mc:render:auto
pnpm sfx:policy
pnpm sfx:compile
pnpm audio:bus
pnpm mc:render:auto
If you want it in one command, see next segment.

6/6 — One command: “render → capture reveals → plan cues → build bus → render final”

{
  "scripts": {
    "mc:render:auto": "ts-node scripts/mc-render.ts",

    "sfx:policy": "ts-node tools/sfx/write-macro-cues-from-policy.ts",
    "sfx:compile": "ts-node tools/sfx/compile-macro-cues.ts",
    "audio:bus": "ts-node tools/sfx/build_audio_bus.ts",

    "mc:hybrid:final": "pnpm mc:render:auto && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus && pnpm mc:render:auto"
  }
}
If you tell me your 3 most common reveal primitives in your format engine (ex: showKeyword(), showBullet(), showCodeLine()), I’ll wire reveal() directly into those helpers so you never have to think about it again—every generated video automatically emits perfect visual_reveals.json.

You said:

yes that would be cool
ChatGPT said:

Perfect — I’ll assume your hybrid format engine uses (or should use) these 3 primitives:

showKeyword(text) (big highlighted term)

showBullet(text) (listicle / explainer step)

showCodeLine(text) (dev vlog / code snippet moment)

I’ll give you a drop-in “Reveal-aware UI helpers” module where every time you call these, it automatically logs a reveal (and keeps working with your mc-render capture flow).

1/6 — One reveal-aware UI helper module

// src/format/reveal-ui.tsx
import React from "react";
import {Txt} from "@motion-canvas/2d";
import {SignalValue, createSignal} from "@motion-canvas/core";
import {reveal} from "../sfx/reveal-recorder";

type RevealOptions = {
  kind?: "keyword" | "bullet" | "code" | "chart" | "cta" | "error" | "success";
  key?: string;          // label to store in visual_reveals.json
  revealOnCall?: boolean; // default true
};

function doReveal(kind: RevealOptions["kind"], key?: string, revealOnCall = true) {
  if (!revealOnCall) return;
  reveal(kind ?? "keyword", key);
}

/**
 * Keyword: big punchy text
 */
export function Keyword(args: {
  text: SignalValue<string>;
  fontSize?: SignalValue<number>;
  opacity?: SignalValue<number>;
  reveal?: RevealOptions;
}) {
  const key = args.reveal?.key ?? (typeof args.text === "string" ? args.text : undefined);
  doReveal(args.reveal?.kind ?? "keyword", key, args.reveal?.revealOnCall ?? true);

  return (
    <Txt
      text={args.text}
      fontSize={args.fontSize ?? 96}
      opacity={args.opacity ?? 1}
    />
  );
}

/**
 * Bullet: list item text
 */
export function Bullet(args: {
  text: SignalValue<string>;
  fontSize?: SignalValue<number>;
  opacity?: SignalValue<number>;
  reveal?: RevealOptions;
}) {
  const key = args.reveal?.key ?? (typeof args.text === "string" ? args.text : undefined);
  doReveal(args.reveal?.kind ?? "bullet", key, args.reveal?.revealOnCall ?? true);

  return (
    <Txt
      text={args.text}
      fontSize={args.fontSize ?? 60}
      opacity={args.opacity ?? 1}
    />
  );
}

/**
 * CodeLine: monospaced-ish line
 */
export function CodeLine(args: {
  text: SignalValue<string>;
  fontSize?: SignalValue<number>;
  opacity?: SignalValue<number>;
  reveal?: RevealOptions;
}) {
  const key = args.reveal?.key ?? (typeof args.text === "string" ? args.text : undefined);
  doReveal(args.reveal?.kind ?? "code", key, args.reveal?.revealOnCall ?? true);

  return (
    <Txt
      text={args.text}
      fontSize={args.fontSize ?? 44}
      opacity={args.opacity ?? 1}
      fontFamily={"Menlo, Monaco, Consolas, monospace"}
    />
  );
}

/**
 * Optional: helper for progressive bullets/code that you control via signals.
 * It reveals only when you flip visibility from 0->1.
 */
export function useRevealSignal(kind: RevealOptions["kind"], key?: string) {
  const shown = createSignal(0);
  let didReveal = false;

  const setShown = (v: number) => {
    shown(v);
    if (!didReveal && v > 0) {
      reveal(kind ?? "keyword", key);
      didReveal = true;
    }
  };

  return {shown, setShown};
}
Why this is clean:

Your generator only needs to call Keyword/Bullet/CodeLine

Every call automatically emits visual_reveals

If you need “reveal when it fades in,” use useRevealSignal

2/6 — Use the helpers in a hybrid scene (explainer + dev vlog)

// src/scenes/main.tsx
import React from "react";
import {makeScene2D} from "@motion-canvas/2d";
import {createRef} from "@motion-canvas/core";
import {waitUntil} from "@motion-canvas/core/lib/flow";
import {resetReveals, flushRevealsToLog} from "../sfx/reveal-recorder";
import {Keyword, Bullet, CodeLine, useRevealSignal} from "../format/reveal-ui";

export default makeScene2D(function* (view) {
  resetReveals();

  // Explainer keyword (reveals immediately on mount if revealOnCall true)
  const kwOpacity = useRevealSignal("keyword", "Supabase");
  view.add(
    <Keyword
      text={"Supabase"}
      opacity={kwOpacity.shown}
      reveal={{kind: "keyword", key: "Supabase", revealOnCall: false}} // reveal when shown flips
    />
  );

  yield* waitUntil("kw_supabase_in");
  kwOpacity.setShown(1);

  // Bullet reveal
  const b1 = useRevealSignal("bullet", "RLS policies");
  view.add(
    <Bullet
      text={"• RLS policies"}
      opacity={b1.shown}
      reveal={{kind: "bullet", key: "RLS policies", revealOnCall: false}}
    />
  );

  yield* waitUntil("bullet_1_in");
  b1.setShown(1);

  // Dev vlog code line reveal
  const c1 = useRevealSignal("code", "npm run build");
  view.add(
    <CodeLine
      text={"$ npm run build"}
      opacity={c1.shown}
      reveal={{kind: "code", key: "npm run build", revealOnCall: false}}
    />
  );

  yield* waitUntil("code_1_in");
  c1.setShown(1);

  // Error & success moments can be recorded with reveal-recorder directly
  // (or you can create helpers later: ErrorBanner/SuccessBanner)
  yield* waitUntil("build_failed");
  // record error reveal
  // reveal("error", "Build failed")  // if you import reveal()

  yield* waitUntil("deploy_ok");
  // reveal("success", "Deploy OK")

  flushRevealsToLog();
});
3/6 — Optional: Error/Success banner components (auto-reveal)

// src/format/status-banners.tsx
import React from "react";
import {Rect, Txt} from "@motion-canvas/2d";
import {reveal} from "../sfx/reveal-recorder";

export function ErrorBanner({text}: {text: string}) {
  reveal("error", text);
  return (
    <Rect radius={24} padding={24}>
      <Txt text={text} fontSize={56} />
    </Rect>
  );
}

export function SuccessBanner({text}: {text: string}) {
  reveal("success", text);
  return (
    <Rect radius={24} padding={24}>
      <Txt text={text} fontSize={56} />
    </Rect>
  );
}
4/6 — “Reveal registry” for your format generator (super AI-friendly)

This is the schema your generator can target.

// src/format/format-dsl.ts
export type FormatBlock =
  | {type: "keyword"; text: string; event?: string}
  | {type: "bullet"; text: string; event?: string}
  | {type: "code"; text: string; event?: string}
  | {type: "error"; text: string; event?: string}
  | {type: "success"; text: string; event?: string}
  | {type: "cta"; text: string; event?: string};

export type HybridFormat = {
  fps: number;
  blocks: FormatBlock[];
};
Then your generator just emits a list of blocks + optional time events, and your renderer maps them to components that auto-reveal.

5/6 — Renderer for the DSL (blocks → Motion Canvas nodes + reveals)

// src/format/render-hybrid.tsx
import React from "react";
import {makeScene2D} from "@motion-canvas/2d";
import {waitUntil} from "@motion-canvas/core/lib/flow";
import type {HybridFormat} from "./format-dsl";
import {resetReveals, flushRevealsToLog, revealAt} from "../sfx/reveal-recorder";
import {Keyword, Bullet, CodeLine} from "./reveal-ui";
import {ErrorBanner, SuccessBanner} from "./status-banners";

export function makeHybridScene(format: HybridFormat) {
  return makeScene2D(function* (view) {
    resetReveals();

    for (const block of format.blocks) {
      if (block.event) {
        // If you tie blocks to timeline events, reveals become timeline-editable.
        yield* revealAt(block.event, block.type === "cta" ? "cta" : (block.type as any), block.text);
        yield* waitUntil(block.event);
      }

      if (block.type === "keyword") view.add(<Keyword text={block.text} />);
      if (block.type === "bullet") view.add(<Bullet text={block.text} />);
      if (block.type === "code") view.add(<CodeLine text={block.text} />);
      if (block.type === "error") view.add(<ErrorBanner text={block.text} />);
      if (block.type === "success") view.add(<SuccessBanner text={block.text} />);
      if (block.type === "cta") view.add(<Keyword text={block.text} reveal={{kind: "cta", key: block.text}} />);
    }

    flushRevealsToLog();
  });
}
6/6 — One pipeline command for hybrid

This is the full Motion Canvas-only hybrid loop:

{
  "scripts": {
    "mc:render:auto": "ts-node scripts/mc-render.ts",
    "sfx:policy": "ts-node tools/sfx/write-macro-cues-from-policy.ts",
    "sfx:compile": "ts-node tools/sfx/compile-macro-cues.ts",
    "audio:bus": "ts-node tools/sfx/build_audio_bus.ts",
    "mc:hybrid:final": "pnpm mc:render:auto && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus && pnpm mc:render:auto"
  }
}
What happens:

First render: captures visual_reveals.json from your scene’s reveals

Policy: turns reveals+beats into macro cues

Compile: macro cues → real SFX IDs (mapped)

Mix: generates audio_bus.wav

Second render: final video includes audio bus

If you want the generator to be even easier to drive, tell me whether your hybrid format is usually:

Listicle-first (10 points with dev moments sprinkled), or

Devlog-first (story arc with explainers inserted)

…and I’ll give you a minimal HybridFormat JSON template + the exact “beat actions” (hook, transition, reveal, punchline, cta) your pipeline should standardize on.

You said:

yes that would be cool
ChatGPT said:

Cool — I’m going to pick the best “hybrid” default for you without asking questions:

✅ Default hybrid format: Devlog-first with explainer inserts

Because it naturally supports:

narrative tension (bug → fix → win)

explainers as “clarity breaks”

meme beats (glitch, joke hit) without feeling random

Here’s the template + standardized beat actions your whole pipeline should use.

1) HybridFormat JSON template (devlog-first + explainer inserts)

{
  "version": "1.0.0",
  "fps": 30,
  "style": {
    "theme": "clean_dark",
    "fontScale": 1.0
  },
  "blocks": [
    {
      "id": "hook_kw",
      "type": "keyword",
      "text": "I tried to automate SFX in Motion Canvas…",
      "event": "hook"
    },

    {
      "id": "problem",
      "type": "bullet",
      "text": "Problem: SFX timing gets messy fast.",
      "event": "problem"
    },

    {
      "id": "dev_error",
      "type": "error",
      "text": "Build failed: audio drift 😭",
      "event": "error_hit"
    },

    {
      "id": "explain_1_kw",
      "type": "keyword",
      "text": "The fix is: one Audio Bus",
      "event": "reveal"
    },
    {
      "id": "explain_1_b1",
      "type": "bullet",
      "text": "AI outputs macro cues (intent-based).",
      "event": "explain_b1"
    },
    {
      "id": "explain_1_b2",
      "type": "bullet",
      "text": "Policy guards prevent spam.",
      "event": "explain_b2"
    },

    {
      "id": "code_1",
      "type": "code",
      "text": "$ pnpm mc:hybrid:final",
      "event": "code_in"
    },

    {
      "id": "success",
      "type": "success",
      "text": "Deploy OK ✅ Audio locked.",
      "event": "success_hit"
    },

    {
      "id": "cta",
      "type": "cta",
      "text": "Comment TECH for the template",
      "event": "cta"
    }
  ]
}
2) Standard “beat actions” (the contract)

These power:

the SFX policy rules (transition, reveal, punchline)

your visuals (waitUntil)

your AI prompt conditioning

Use these exact strings:

export type BeatAction =
  | "hook"
  | "problem"
  | "transition"
  | "reveal"
  | "explain"
  | "code"
  | "error"
  | "success"
  | "punchline"
  | "cta"
  | "outro";
Mapping blocks → actions (default):

keyword → reveal (unless first one = hook)

bullet → explain

code → code

error → error

success → success

cta → cta

3) The macro policy mapping (hybrid defaults)

This is what your auto policy should do:

hook → dramatic_pause_riser (pre) + impact_soft (land)

transition → transition_fast

reveal → reveal_riser (pre) + impact_soft (land)

explain (bullets/keywords that pop) → text_ping

error → warning_buzz_soft + optional glitch_cut

success → success_ding

punchline → micro_joke_hit

cta → cta_sparkle

And then the guard pass enforces:

min gap

per-macro cooldown

per-beat max cues

4) Minimal “format generator” prompt for AI (super consistent)

When you want AI to produce the HybridFormat JSON:

Give it:

topic

3–5 bullets of key points

the target CTA string

Tell it:

use this schema

keep it 45–60 seconds

include at least: hook → error → reveal → code → success → cta

5) What your Motion Canvas scene generator should do

When rendering that HybridFormat:

create nodes for each block using your Keyword/Bullet/CodeLine/ErrorBanner/SuccessBanner

waitUntil(block.event) controls timing

auto emit visual_reveals.json from those helper components (already set up)

This gives you:

timeline-editable timing

automatic reveal capture

deterministic audio + SFX

6) Your “one command” hybrid pipeline stays the same

pnpm mc:hybrid:final
Which runs:

render (capture reveals)

policy cues

compile cues → SFX IDs

FFmpeg audio bus

render final

If you want the next piece: I can give you a tools/format/generate-hybrid.ts that takes a plain text script (or bullet outline) and auto-produces this HybridFormat JSON + beats.sec.json in one shot (so the only human input is “topic + CTA”).

You said:

yes that would be cool
ChatGPT said:

Alright — here’s a Motion Canvas-only “HybridFormat generator” that takes a plain text outline (or script) and outputs:

data/hybrid_format.json (blocks + events)

data/beats.sec.json (for your SFX policy + spacing rules)

No web calls, no external services required. It’s deterministic and you can later swap the “heuristics” with an LLM call if you want.

1/6 — Input file format (data/outline.txt)

Use simple markers. This is easy to generate from AI or write yourself.

HOOK: I tried to automate SFX in Motion Canvas…
PROBLEM: SFX timing gets messy fast.
ERROR: Build failed: audio drift 😭
REVEAL: The fix is: one Audio Bus
EXPLAIN: AI outputs macro cues (intent-based).
EXPLAIN: Policy guards prevent spam.
CODE: pnpm mc:hybrid:final
SUCCESS: Deploy OK ✅ Audio locked.
CTA: Comment TECH for the template
2/6 — Types (src/format/hybrid-types.ts)

export type BeatAction =
  | "hook"
  | "problem"
  | "transition"
  | "reveal"
  | "explain"
  | "code"
  | "error"
  | "success"
  | "punchline"
  | "cta"
  | "outro";

export type FormatBlock =
  | { id: string; type: "keyword"; text: string; event?: string }
  | { id: string; type: "bullet"; text: string; event?: string }
  | { id: string; type: "code"; text: string; event?: string }
  | { id: string; type: "error"; text: string; event?: string }
  | { id: string; type: "success"; text: string; event?: string }
  | { id: string; type: "cta"; text: string; event?: string };

export type HybridFormat = {
  version: "1.0.0";
  fps: number;
  style?: { theme?: string; fontScale?: number };
  blocks: FormatBlock[];
};

export type BeatSec = {
  beatId: string;
  t: number; // seconds
  text: string;
  action: BeatAction;
  event?: string;
};
3/6 — Heuristic timing model (words → seconds)

This gives you a solid default tempo:

hooks land fast

reveal gets a tiny pause

code gets a bit more time

// tools/format/timing.ts
import type { BeatAction } from "../../src/format/hybrid-types";

export function estimateSeconds(text: string, action: BeatAction) {
  const words = text.trim().split(/\s+/).filter(Boolean).length;

  // baseline reading speed (words/sec)
  const wps =
    action === "hook" ? 3.2 :
    action === "reveal" ? 2.8 :
    action === "code" ? 2.2 :
    action === "error" ? 2.6 :
    action === "success" ? 2.8 :
    action === "cta" ? 2.6 :
    3.0;

  const base = Math.max(0.75, words / wps);

  // add intentional pauses for pacing
  const pause =
    action === "reveal" ? 0.35 :
    action === "hook" ? 0.20 :
    action === "transition" ? 0.15 :
    action === "cta" ? 0.25 :
    0.10;

  // cap per line so it doesn’t drag
  return Math.min(base + pause, 4.25);
}
4/6 — Generator: outline → hybrid_format.json + beats.sec.json

// tools/format/generate-hybrid.ts
import fs from "node:fs";
import path from "node:path";
import crypto from "node:crypto";
import type { HybridFormat, BeatSec, BeatAction, FormatBlock } from "../../src/format/hybrid-types";
import { estimateSeconds } from "./timing";

function slug(s: string) {
  return s
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, "_")
    .replace(/^_+|_+$/g, "")
    .slice(0, 48);
}

function idHash(s: string) {
  return crypto.createHash("md5").update(s).digest("hex").slice(0, 6);
}

function parseLine(line: string): { action: BeatAction; text: string } | null {
  const m = line.match(/^([A-Z_]+)\s*:\s*(.+)$/);
  if (!m) return null;

  const key = m[1].toUpperCase();
  const text = m[2].trim();

  const map: Record<string, BeatAction> = {
    HOOK: "hook",
    PROBLEM: "problem",
    TRANSITION: "transition",
    REVEAL: "reveal",
    EXPLAIN: "explain",
    CODE: "code",
    ERROR: "error",
    SUCCESS: "success",
    PUNCHLINE: "punchline",
    CTA: "cta",
    OUTRO: "outro",
  };

  const action = map[key];
  if (!action) return null;

  return { action, text };
}

function actionToBlockType(action: BeatAction): FormatBlock["type"] {
  if (action === "hook" || action === "reveal") return "keyword";
  if (action === "explain" || action === "problem" || action === "transition" || action === "outro") return "bullet";
  if (action === "code") return "code";
  if (action === "error") return "error";
  if (action === "success") return "success";
  if (action === "cta") return "cta";
  if (action === "punchline") return "bullet";
  return "bullet";
}

const root = process.cwd();
const inputPath = path.join(root, "data", "outline.txt");
const outFormatPath = path.join(root, "data", "hybrid_format.json");
const outBeatsPath = path.join(root, "data", "beats.sec.json");

const fps = 30;

if (!fs.existsSync(inputPath)) {
  throw new Error(`Missing input outline: ${inputPath}`);
}

const lines = fs
  .readFileSync(inputPath, "utf-8")
  .split(/\r?\n/)
  .map((l) => l.trim())
  .filter((l) => l.length > 0 && !l.startsWith("#"));

const parsed = lines
  .map(parseLine)
  .filter((x): x is { action: BeatAction; text: string } => Boolean(x));

if (parsed.length === 0) throw new Error("No valid lines found. Use 'HOOK: ...' etc.");

let t = 0;
const beats: BeatSec[] = [];
const blocks: FormatBlock[] = [];

for (let i = 0; i < parsed.length; i++) {
  const { action, text } = parsed[i];
  const dur = estimateSeconds(text, action);

  // create a stable event name for timeline markers
  const event = `${action}_${slug(text)}_${idHash(text)}`;

  const beatId = `b${String(i + 1).padStart(2, "0")}_${action}`;
  beats.push({ beatId, t: Number(t.toFixed(3)), text, action, event });

  const blockType = actionToBlockType(action);
  const blockId = `${blockType}_${slug(text)}_${idHash(text)}`;

  blocks.push({ id: blockId, type: blockType as any, text, event });

  t += dur;
}

const format: HybridFormat = {
  version: "1.0.0",
  fps,
  style: { theme: "clean_dark", fontScale: 1.0 },
  blocks,
};

fs.mkdirSync(path.dirname(outFormatPath), { recursive: true });
fs.writeFileSync(outFormatPath, JSON.stringify(format, null, 2));
fs.writeFileSync(outBeatsPath, JSON.stringify(beats, null, 2));

console.log(`✅ Wrote ${outFormatPath}`);
console.log(`✅ Wrote ${outBeatsPath}`);
console.log(`⏱️ Estimated duration: ${t.toFixed(1)}s (${Math.round(t * fps)} frames @ ${fps}fps)`);
5/6 — Wire it into your Motion Canvas render path

You’ll read data/hybrid_format.json and render it via makeHybridScene(format).

// src/scenes/main.tsx
import fs from "node:fs";
import path from "node:path";
import { makeHybridScene } from "../format/render-hybrid";
import type { HybridFormat } from "../format/hybrid-types";

// Motion Canvas scene factory expects a scene function export.
// We load format JSON at runtime (dev + render).
const formatPath = path.join(process.cwd(), "data", "hybrid_format.json");
const format = JSON.parse(fs.readFileSync(formatPath, "utf-8")) as HybridFormat;

export default makeHybridScene(format);
(If your bundler complains about fs in the browser context, we can switch to import format from "../../data/hybrid_format.json" instead — but this version is the most flexible for generator-driven updates.)

6/6 — Add scripts: one command from outline → final render

{
  "scripts": {
    "format:gen": "ts-node tools/format/generate-hybrid.ts",
    "mc:render:auto": "ts-node scripts/mc-render.ts",

    "sfx:policy": "ts-node tools/sfx/write-macro-cues-from-policy.ts",
    "sfx:compile": "ts-node tools/sfx/compile-macro-cues.ts",
    "audio:bus": "ts-node tools/sfx/build_audio_bus.ts",

    "mc:hybrid:final": "pnpm format:gen && pnpm mc:render:auto && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus && pnpm mc:render:auto"
  }
}
Run:

pnpm mc:hybrid:final
If you want the next upgrade (super worth it): I can make the generator optionally take a raw script and auto-chunk it into HOOK/PROBLEM/ERROR/REVEAL/EXPLAIN/CODE/SUCCESS/CTA using a tiny ruleset (no LLM) — so your only input is script.txt.

You said:

yes lets that would be cool
ChatGPT said:

Got you — let’s do the no-LLM script → structured outline → HybridFormat + beats upgrade.

You’ll feed data/script.txt (raw text), and it will output:

data/outline.txt (HOOK/PROBLEM/ERROR/REVEAL/EXPLAIN/CODE/SUCCESS/CTA)

data/hybrid_format.json

data/beats.sec.json

All deterministic.

1/6 — Input: data/script.txt (raw)

Example (this can be messy, paragraphs, whatever):

I tried to automate sound effects in Motion Canvas and it sounded good… until the timing drifted.
The problem is you end up with tons of little clips and it’s a nightmare to keep in sync.

Here’s what broke: my build passed locally, but the final render had audio offset.
So I changed the approach.

The fix is simple: generate one audio bus.
AI outputs macro cues like transition_fast and text_ping, then a policy prevents spam.
Then FFmpeg mixes voice, music, and SFX into audio_bus.wav.

Command is pnpm mc:hybrid:final.

Now renders are locked and the audio stays perfect.
Comment TECH if you want the template.
2/6 — Rule-based classifier: sentences → “intent buckets”

// tools/format/script-classify.ts
export type Bucket =
  | "HOOK"
  | "PROBLEM"
  | "ERROR"
  | "REVEAL"
  | "EXPLAIN"
  | "CODE"
  | "SUCCESS"
  | "CTA"
  | "OTHER";

const RX = {
  code: /(^\s*[$>]\s*)|(\bpnpm\b|\bnpm\b|\byarn\b|\bbun\b|\bgit\b|\bnode\b|\bts-node\b|\bffmpeg\b)/i,
  cta: /\b(comment|dm|download|template|follow|subscribe|waitlist|link in bio|join)\b/i,
  error: /\b(error|failed|fail|broken|bug|crash|drift|offset|issue|problem)\b/i,
  success: /\b(success|fixed|works|working|passed|deploy|deployed|ship|shipped|done|solved)\b/i,
  reveal: /\b(the fix|here'?s the fix|the trick|the secret|what changed|solution is|i changed the approach)\b/i,
  problem: /\b(the problem|issue is|hard part|nightmare|messy|pain|annoying)\b/i,
  hook: /\b(i tried|today i|i built|i was trying|i attempted)\b/i
};

export function classifySentence(s: string): Bucket {
  const t = s.trim();
  if (!t) return "OTHER";

  // priority order matters
  if (RX.code.test(t)) return "CODE";
  if (RX.cta.test(t)) return "CTA";
  if (RX.reveal.test(t)) return "REVEAL";
  if (RX.error.test(t)) return "ERROR";
  if (RX.success.test(t)) return "SUCCESS";
  if (RX.problem.test(t)) return "PROBLEM";
  if (RX.hook.test(t)) return "HOOK";

  return "EXPLAIN";
}
3/6 — Script parser: raw text → sentences (clean + stable)

// tools/format/script-parse.ts
export function splitSentences(raw: string) {
  const cleaned = raw
    .replace(/\r/g, "")
    .replace(/[“”]/g, '"')
    .replace(/[’]/g, "'")
    .trim();

  // naive sentence splitting (good enough for creator scripts)
  const parts = cleaned
    .split(/(?<=[.!?])\s+|\n+/g)
    .map((s) => s.trim())
    .filter(Boolean);

  // merge very short fragments into the previous sentence
  const out: string[] = [];
  for (const p of parts) {
    if (p.length < 18 && out.length > 0) out[out.length - 1] += " " + p;
    else out.push(p);
  }
  return out;
}
4/6 — Outline builder: sentences → data/outline.txt (ordered)

This guarantees a solid hybrid structure even if the script is chaotic.

// tools/format/build-outline-from-script.ts
import fs from "node:fs";
import path from "node:path";
import { splitSentences } from "./script-parse";
import { classifySentence, Bucket } from "./script-classify";

function pickFirst(list: string[], fallback: string) {
  return list[0]?.trim() || fallback;
}

function joinTop(list: string[], n: number) {
  return list.slice(0, n).join(" ").trim();
}

const root = process.cwd();
const scriptPath = path.join(root, "data", "script.txt");
const outOutlinePath = path.join(root, "data", "outline.txt");

if (!fs.existsSync(scriptPath)) throw new Error(`Missing ${scriptPath}`);
const raw = fs.readFileSync(scriptPath, "utf-8");

const sentences = splitSentences(raw);
const buckets: Record<Bucket, string[]> = {
  HOOK: [], PROBLEM: [], ERROR: [], REVEAL: [], EXPLAIN: [], CODE: [], SUCCESS: [], CTA: [], OTHER: []
};

for (const s of sentences) buckets[classifySentence(s)].push(s);

// Build a stable narrative order
const hook = pickFirst(buckets.HOOK, pickFirst(sentences, "Today I built something."));
const problem = pickFirst(buckets.PROBLEM, "The problem is it gets messy to keep everything in sync.");
const error = pickFirst(buckets.ERROR, "It failed in a way that was hard to debug.");
const reveal = pickFirst(buckets.REVEAL, "Here’s the fix.");
const explain = joinTop(buckets.EXPLAIN, 3) || "So here’s how it works.";
const code = pickFirst(buckets.CODE, "pnpm mc:hybrid:final");
const success = pickFirst(buckets.SUCCESS, "Now it works and stays locked.");
const cta = pickFirst(buckets.CTA, "Comment TECH if you want the template.");

// Write outline.txt in the format your existing generator consumes
const outline = [
  `HOOK: ${hook}`,
  `PROBLEM: ${problem}`,
  `ERROR: ${error}`,
  `REVEAL: ${reveal}`,
  ...explain.split(/(?<=[.!?])\s+/).filter(Boolean).slice(0, 3).map(s => `EXPLAIN: ${s}`),
  `CODE: ${code}`,
  `SUCCESS: ${success}`,
  `CTA: ${cta}`
].join("\n");

fs.mkdirSync(path.dirname(outOutlinePath), { recursive: true });
fs.writeFileSync(outOutlinePath, outline, "utf-8");
console.log(`✅ Wrote ${outOutlinePath}`);
5/6 — One generator entrypoint: script → outline → HybridFormat + beats

This just chains the outline builder + your previous generate-hybrid.ts.

// tools/format/generate-from-script.ts
import { spawnSync } from "node:child_process";

function run(cmd: string, args: string[]) {
  const r = spawnSync(cmd, args, { stdio: "inherit" });
  if (r.status !== 0) process.exit(r.status ?? 1);
}

const bin = process.platform === "win32" ? "pnpm.cmd" : "pnpm";

// 1) script -> outline
run(bin, ["ts-node", "tools/format/build-outline-from-script.ts"]);

// 2) outline -> hybrid_format.json + beats.sec.json
run(bin, ["ts-node", "tools/format/generate-hybrid.ts"]);
6/6 — Add scripts: single command from raw script → final Motion Canvas render

{
  "scripts": {
    "format:from-script": "ts-node tools/format/generate-from-script.ts",

    "mc:render:auto": "ts-node scripts/mc-render.ts",
    "sfx:policy": "ts-node tools/sfx/write-macro-cues-from-policy.ts",
    "sfx:compile": "ts-node tools/sfx/compile-macro-cues.ts",
    "audio:bus": "ts-node tools/sfx/build_audio_bus.ts",

    "mc:hybrid:final": "pnpm format:from-script && pnpm mc:render:auto && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus && pnpm mc:render:auto"
  }
}
Run:

pnpm mc:hybrid:final
If you want to make the classifier noticeably smarter (still no LLM), the best next addition is a tiny keyword dictionary for your niche (Remotion, Motion Canvas, n8n, Make, Supabase, RLS, Stripe, PostHog) so “EXPLAIN” vs “REVEAL” becomes more accurate.

You said:

yes that would be cool
ChatGPT said:

Cool — here’s the no-LLM keyword dictionary upgrade that makes your script classifier way smarter for your niche (Remotion / Motion Canvas / n8n / Make / Supabase / RLS / Stripe / PostHog / etc.). It’s lightweight, fast, and deterministic.

1/6 — Add a domain dictionary (data/domain_dict.json)

This drives:

better bucket classification (EXPLAIN vs REVEAL vs CODE)

better beat actions (transition/reveal moments)

optional auto “keyword” inserts later

{
  "version": "1.0.0",
  "domains": [
    {
      "name": "video_tooling",
      "keywords": ["motion canvas", "remotion", "ffmpeg", "timeline", "render", "exporter", "audio bus", "sfx", "cue sheet", "whoosh", "riser"]
    },
    {
      "name": "automation",
      "keywords": ["n8n", "make.com", "zapier", "webhook", "workflow", "trigger", "agent", "pipeline"]
    },
    {
      "name": "backend",
      "keywords": ["supabase", "rls", "postgres", "stripe", "revenuecat", "oauth", "jwt", "vercel", "api"]
    },
    {
      "name": "analytics",
      "keywords": ["posthog", "events", "funnel", "retention", "session replay", "cohort", "utm"]
    }
  ],
  "signals": {
    "reveal_phrases": ["here's the fix", "the fix is", "the trick is", "the solution is", "what changed", "this is the key"],
    "transition_phrases": ["next", "now", "then", "after that", "moving on", "so anyway"],
    "error_phrases": ["failed", "error", "broke", "crashed", "drift", "offset", "bug", "issue"],
    "success_phrases": ["fixed", "works", "working", "passed", "deployed", "ship", "solved", "done"],
    "cta_phrases": ["comment", "dm", "download", "template", "follow", "subscribe", "join", "waitlist", "link in bio"],
    "code_markers": ["pnpm", "npm", "yarn", "bun", "git", "ffmpeg", "ts-node", "$", ">", "`"]
  }
}
2/6 — Dictionary loader + matcher (tools/format/domain-dict.ts)

import fs from "node:fs";
import path from "node:path";

export type DomainDict = {
  version: string;
  domains: Array<{ name: string; keywords: string[] }>;
  signals: Record<string, string[]>;
};

export function loadDomainDict(root = process.cwd()): DomainDict {
  const p = path.join(root, "data", "domain_dict.json");
  if (!fs.existsSync(p)) {
    // safe default if missing
    return {
      version: "1.0.0",
      domains: [],
      signals: {}
    };
  }
  return JSON.parse(fs.readFileSync(p, "utf-8")) as DomainDict;
}

export function countKeywordHits(text: string, keywords: string[]) {
  const t = text.toLowerCase();
  let hits = 0;
  for (const k of keywords) {
    if (!k.trim()) continue;
    if (t.includes(k.toLowerCase())) hits++;
  }
  return hits;
}

export function hasAnyPhrase(text: string, phrases: string[]) {
  const t = text.toLowerCase();
  return phrases.some((p) => p && t.includes(p.toLowerCase()));
}
3/6 — Smarter classifier (uses dictionary + scoring)

This replaces your earlier simple regex classifier.

// tools/format/script-classify-smart.ts
import { loadDomainDict, hasAnyPhrase, countKeywordHits } from "./domain-dict";

export type Bucket =
  | "HOOK"
  | "PROBLEM"
  | "ERROR"
  | "REVEAL"
  | "EXPLAIN"
  | "CODE"
  | "SUCCESS"
  | "CTA"
  | "TRANSITION"
  | "OTHER";

const dict = loadDomainDict();

function looksLikeCodeLine(s: string) {
  const t = s.trim();
  if (/^\s*[$>]/.test(t)) return true;
  const markers = dict.signals.code_markers ?? [];
  return markers.some((m) => m && t.toLowerCase().includes(m.toLowerCase()));
}

export function classifySentenceSmart(s: string): Bucket {
  const t = s.trim();
  if (!t) return "OTHER";

  const lower = t.toLowerCase();

  // Strong signals (priority)
  if (looksLikeCodeLine(t)) return "CODE";
  if (hasAnyPhrase(lower, dict.signals.cta_phrases ?? [])) return "CTA";
  if (hasAnyPhrase(lower, dict.signals.reveal_phrases ?? [])) return "REVEAL";
  if (hasAnyPhrase(lower, dict.signals.error_phrases ?? [])) return "ERROR";
  if (hasAnyPhrase(lower, dict.signals.success_phrases ?? [])) return "SUCCESS";
  if (hasAnyPhrase(lower, dict.signals.transition_phrases ?? [])) return "TRANSITION";

  // Score domain keyword density
  const domainHits = dict.domains.reduce((sum, d) => sum + countKeywordHits(lower, d.keywords), 0);

  // Hook heuristics
  const isHookish =
    /\b(i tried|today i|i built|i was trying|i attempted|watch this)\b/i.test(t) ||
    (t.length < 90 && domainHits > 0);

  if (isHookish) return "HOOK";

  // Problem vs explain
  const isProblem =
    /\b(problem|issue|hard part|nightmare|messy|pain|annoying|confusing)\b/i.test(t);

  if (isProblem) return "PROBLEM";

  // If it's dense with domain keywords, it’s likely explanation
  if (domainHits >= 2) return "EXPLAIN";

  // Default explain
  return "EXPLAIN";
}
4/6 — Outline builder updated to use smart classifier

Swap this into your build-outline-from-script.ts.

// tools/format/build-outline-from-script.ts (PATCH)
import { classifySentenceSmart, Bucket } from "./script-classify-smart";
// (rest stays the same, just replace classifySentence(...) with classifySentenceSmart(...))

// inside loop:
for (const s of sentences) buckets[classifySentenceSmart(s)].push(s);
Also, insert transitions if you have them:

const transitions = buckets.TRANSITION.slice(0, 1); // keep it light
// After REVEAL or between explain chunks, you can add:
if (transitions.length) outlineLines.splice(4, 0, `TRANSITION: ${transitions[0]}`);
(Keep transitions optional so you don’t spam whooshes.)

5/6 — (Optional but powerful) Auto-extract “keyword reveals” list for visuals

This helps your reveal policy even before visual capture exists.

// tools/format/extract-keywords.ts
import { loadDomainDict } from "./domain-dict";

export function extractDomainKeywords(text: string, max = 6) {
  const dict = loadDomainDict();
  const lower = text.toLowerCase();
  const hits: { k: string; score: number }[] = [];

  for (const d of dict.domains) {
    for (const k of d.keywords) {
      const key = k.toLowerCase();
      if (lower.includes(key)) hits.push({ k, score: key.length });
    }
  }

  // prefer longer (more specific) keywords, dedupe
  const uniq = new Map<string, number>();
  for (const h of hits) uniq.set(h.k, Math.max(uniq.get(h.k) ?? 0, h.score));

  return [...uniq.entries()]
    .sort((a, b) => b[1] - a[1])
    .slice(0, max)
    .map(([k]) => k);
}
You can later use these to auto-add keyword blocks if your script didn’t explicitly call them out.

6/6 — One command remains unchanged (it just got smarter)

Your existing pipeline still works:

pnpm mc:hybrid:final
Because internally:
script.txt → outline.txt → hybrid_format.json + beats → visual_reveals → sfx policy → audio_bus → final render

If you want the next big win: I can add a “target duration normalizer” that automatically trims or merges EXPLAIN lines to hit exactly 58s (short-form sweet spot), while keeping HOOK/REVEAL/CTA intact.

You said:

yes that would be cool
ChatGPT said:

Awesome — here’s the target-duration normalizer (no LLM) that trims/merges EXPLAIN lines to hit (say) 58.0s while preserving HOOK / REVEAL / CTA.

It works by:

estimating time per line (same model you already use)

locking “must keep” lines

compressing EXPLAIN by:

dropping lowest-value explain lines first

merging remaining explain lines into fewer lines

shortening long explain sentences (keep first clause + key nouns)

6 segments.

1/6 — Config (tools/format/normalize-config.ts)

export type NormalizeConfig = {
  targetSeconds: number;
  toleranceSeconds: number; // acceptable +/- window
  maxExplainLines: number;  // after merging
  keepKeys: string[];       // always keep these outline keys
};

export const DEFAULT_NORMALIZE: NormalizeConfig = {
  targetSeconds: 58.0,
  toleranceSeconds: 1.5,
  maxExplainLines: 3,
  keepKeys: ["HOOK", "REVEAL", "CTA", "ERROR", "SUCCESS", "CODE", "PROBLEM"],
};
2/6 — Outline parser + estimator (tools/format/outline-utils.ts)

import type { BeatAction } from "../../src/format/hybrid-types";
import { estimateSeconds } from "./timing";

export type OutlineLine = {
  key: string;     // HOOK, EXPLAIN, etc.
  text: string;
  action: BeatAction;
};

const keyToAction: Record<string, BeatAction> = {
  HOOK: "hook",
  PROBLEM: "problem",
  TRANSITION: "transition",
  REVEAL: "reveal",
  EXPLAIN: "explain",
  CODE: "code",
  ERROR: "error",
  SUCCESS: "success",
  PUNCHLINE: "punchline",
  CTA: "cta",
  OUTRO: "outro",
};

export function parseOutline(outlineText: string): OutlineLine[] {
  const lines = outlineText
    .split(/\r?\n/)
    .map((l) => l.trim())
    .filter(Boolean);

  const out: OutlineLine[] = [];
  for (const l of lines) {
    const m = l.match(/^([A-Z_]+)\s*:\s*(.+)$/);
    if (!m) continue;
    const key = m[1].toUpperCase();
    const text = m[2].trim();
    const action = keyToAction[key];
    if (!action) continue;
    out.push({ key, text, action });
  }
  return out;
}

export function outlineToText(lines: OutlineLine[]) {
  return lines.map((l) => `${l.key}: ${l.text}`).join("\n");
}

export function estimateOutlineSeconds(lines: OutlineLine[]) {
  return lines.reduce((s, l) => s + estimateSeconds(l.text, l.action), 0);
}
3/6 — Explain “value score” + shortening (tools/format/explain-compress.ts)

function tokenize(s: string) {
  return s
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, " ")
    .split(/\s+/)
    .filter(Boolean);
}

// Score: prefer lines with concrete nouns/tech keywords & verbs (very cheap)
export function explainScore(text: string) {
  const toks = tokenize(text);
  const tech = ["motion", "canvas", "remotion", "ffmpeg", "audio", "bus", "macro", "cue", "policy", "render", "timeline"];
  const verbs = ["build", "mix", "generate", "render", "lock", "prevent", "add", "compile", "export"];

  let score = 0;
  for (const t of toks) {
    if (tech.includes(t)) score += 3;
    if (verbs.includes(t)) score += 2;
    if (t.length >= 8) score += 1; // longer words often more specific
  }
  // prefer shorter explain lines (snappier)
  score += Math.max(0, 40 - text.length) * 0.05;
  return score;
}

// Shorten: keep first clause, keep “because/so/then” clause if present
export function shortenExplain(text: string) {
  const t = text.trim();

  // split by punctuation into clauses
  const parts = t.split(/[.;:—-]\s+/).map((x) => x.trim()).filter(Boolean);
  if (parts.length <= 1) return t.length > 120 ? t.slice(0, 118).trim() + "…" : t;

  // keep first clause + optionally second if it starts with "so/then/because"
  const first = parts[0];
  const second = parts[1] ?? "";
  const keepSecond = /^(so|then|because|which|that)/i.test(second);

  const out = keepSecond ? `${first}. ${second}` : first;
  return out.length > 140 ? out.slice(0, 138).trim() + "…" : out;
}

// Merge explain lines into up to maxExplain lines
export function mergeExplains(lines: string[], maxExplainLines: number) {
  if (lines.length <= maxExplainLines) return lines;

  const merged: string[] = [];
  const per = Math.ceil(lines.length / maxExplainLines);

  for (let i = 0; i < lines.length; i += per) {
    const chunk = lines.slice(i, i + per).map(shortenExplain);
    merged.push(chunk.join(" "));
  }
  return merged;
}
4/6 — Normalizer (tools/format/normalize-outline.ts)

import type { NormalizeConfig } from "./normalize-config";
import { DEFAULT_NORMALIZE } from "./normalize-config";
import { parseOutline, outlineToText, estimateOutlineSeconds, type OutlineLine } from "./outline-utils";
import { explainScore, shortenExplain, mergeExplains } from "./explain-compress";

export function normalizeOutlineToDuration(outlineText: string, cfg: NormalizeConfig = DEFAULT_NORMALIZE) {
  let lines = parseOutline(outlineText);

  const isKeep = (k: string) => cfg.keepKeys.includes(k);

  // Separate explains
  let explains = lines.filter((l) => l.key === "EXPLAIN");
  let others = lines.filter((l) => l.key !== "EXPLAIN");

  // Early shorten explains
  explains = explains.map((e) => ({ ...e, text: shortenExplain(e.text) }));

  // Helper to rebuild full outline
  const rebuild = (newExplains: OutlineLine[]) => {
    // Keep original order: replace EXPLAIN positions with new list
    const out: OutlineLine[] = [];
    let ei = 0;

    for (const l of lines) {
      if (l.key === "EXPLAIN") {
        if (ei < newExplains.length) out.push(newExplains[ei++]);
      } else {
        out.push(l);
      }
    }

    // if we have extra explains (rare), append after reveal/explain section
    while (ei < newExplains.length) out.push(newExplains[ei++]);
    return out;
  };

  // Start with shortened explains
  let current = rebuild(explains);
  let total = estimateOutlineSeconds(current);

  // If already within target window, done
  if (Math.abs(total - cfg.targetSeconds) <= cfg.toleranceSeconds) {
    return { outline: outlineToText(current), seconds: total };
  }

  // If too long: drop low-value explains first
  if (total > cfg.targetSeconds + cfg.toleranceSeconds) {
    // sort explains by score ascending (drop lowest)
    let scored = explains
      .map((e) => ({ e, s: explainScore(e.text) }))
      .sort((a, b) => a.s - b.s);

    // Keep removing until within window or only 1 explain remains
    while (scored.length > 1) {
      scored.shift(); // drop lowest
      const kept = scored.map((x) => x.e);
      current = rebuild(kept);
      total = estimateOutlineSeconds(current);
      if (total <= cfg.targetSeconds + cfg.toleranceSeconds) break;
    }

    // If still too long, merge explains into fewer lines
    if (total > cfg.targetSeconds + cfg.toleranceSeconds) {
      const explainTexts = scored.map((x) => x.e.text);
      const mergedTexts = mergeExplains(explainTexts, cfg.maxExplainLines);

      const mergedExplains: OutlineLine[] = mergedTexts.map((t, i) => ({
        key: "EXPLAIN",
        action: "explain",
        text: t,
      }));

      current = rebuild(mergedExplains);
      total = estimateOutlineSeconds(current);
    }
  }

  // If too short: expand explains by re-inserting trimmed ones (if present in original parse)
  // (simple approach: do nothing; short is fine for shorts. You can add fillers later if desired.)

  return { outline: outlineToText(current), seconds: total };
}
5/6 — CLI: script → outline → normalize → write files

This replaces your “script → outline” step with a normalized outline.

// tools/format/build-outline-from-script-normalized.ts
import fs from "node:fs";
import path from "node:path";
import { splitSentences } from "./script-parse";
import { classifySentenceSmart, Bucket } from "./script-classify-smart";
import { normalizeOutlineToDuration } from "./normalize-outline";
import { DEFAULT_NORMALIZE } from "./normalize-config";

function pickFirst(list: string[], fallback: string) {
  return list[0]?.trim() || fallback;
}

function joinTop(list: string[], n: number) {
  return list.slice(0, n).join(" ").trim();
}

const root = process.cwd();
const scriptPath = path.join(root, "data", "script.txt");
const outOutlinePath = path.join(root, "data", "outline.txt");

if (!fs.existsSync(scriptPath)) throw new Error(`Missing ${scriptPath}`);
const raw = fs.readFileSync(scriptPath, "utf-8");

const sentences = splitSentences(raw);
const buckets: Record<Bucket, string[]> = {
  HOOK: [], PROBLEM: [], ERROR: [], REVEAL: [], EXPLAIN: [], CODE: [], SUCCESS: [], CTA: [], TRANSITION: [], OTHER: []
};

for (const s of sentences) buckets[classifySentenceSmart(s)].push(s);

const hook = pickFirst(buckets.HOOK, pickFirst(sentences, "Today I built something."));
const problem = pickFirst(buckets.PROBLEM, "The problem is it gets messy to keep everything in sync.");
const error = pickFirst(buckets.ERROR, "It failed in a way that was hard to debug.");
const reveal = pickFirst(buckets.REVEAL, "Here’s the fix.");
const explain = joinTop(buckets.EXPLAIN, 5) || "So here’s how it works.";
const code = pickFirst(buckets.CODE, "pnpm mc:hybrid:final");
const success = pickFirst(buckets.SUCCESS, "Now it works and stays locked.");
const cta = pickFirst(buckets.CTA, "Comment TECH if you want the template.");

const outlineLines: string[] = [
  `HOOK: ${hook}`,
  `PROBLEM: ${problem}`,
  `ERROR: ${error}`,
  `REVEAL: ${reveal}`,
  ...explain.split(/(?<=[.!?])\s+/).filter(Boolean).slice(0, 5).map(s => `EXPLAIN: ${s}`),
  `CODE: ${code}`,
  `SUCCESS: ${success}`,
  `CTA: ${cta}`
];

const rawOutline = outlineLines.join("\n");
const normalized = normalizeOutlineToDuration(rawOutline, DEFAULT_NORMALIZE);

fs.mkdirSync(path.dirname(outOutlinePath), { recursive: true });
fs.writeFileSync(outOutlinePath, normalized.outline, "utf-8");

console.log(`✅ Wrote ${outOutlinePath}`);
console.log(`⏱️ Normalized duration estimate: ${normalized.seconds.toFixed(1)}s (target ${DEFAULT_NORMALIZE.targetSeconds}s ± ${DEFAULT_NORMALIZE.toleranceSeconds}s)`);
6/6 — Update your format:from-script chain to use normalization

// tools/format/generate-from-script.ts (PATCH)
import { spawnSync } from "node:child_process";

function run(cmd: string, args: string[]) {
  const r = spawnSync(cmd, args, { stdio: "inherit" });
  if (r.status !== 0) process.exit(r.status ?? 1);
}

const bin = process.platform === "win32" ? "pnpm.cmd" : "pnpm";

// 1) script -> normalized outline
run(bin, ["ts-node", "tools/format/build-outline-from-script-normalized.ts"]);

// 2) outline -> hybrid_format.json + beats.sec.json
run(bin, ["ts-node", "tools/format/generate-hybrid.ts"]);
Now your top-level command still works:

pnpm mc:hybrid:final
If you want, I can add one more deterministic feature: “duration hard clamp” that trims the longest EXPLAIN line by removing trailing clauses until it lands inside the tolerance window (so it always hits 58 ± 1.5 no matter what).

You said:

yes that would be cool
ChatGPT said:

Done — here’s the hard clamp so your outline always lands inside target ± tolerance by trimming the longest EXPLAIN lines (deterministic, no LLM). It only touches EXPLAIN: lines, and it never deletes HOOK / REVEAL / CTA etc.

1/6 — Hard clamp strategy

When estimate is too long:

find the longest EXPLAIN: line by estimated duration

trim it by removing trailing clauses (split on . ; : — -)

if still too long, truncate to N chars + ellipsis

repeat until within tolerance or explains are minimal

2/6 — Add a “trim trailing clauses” helper

// tools/format/hard-clamp.ts
import { estimateSeconds } from "./timing";
import type { OutlineLine } from "./outline-utils";

function splitClauses(text: string) {
  return text
    .split(/[.;:—-]\s+/)
    .map((x) => x.trim())
    .filter(Boolean);
}

function trimOneStep(text: string) {
  const clauses = splitClauses(text);
  if (clauses.length <= 1) {
    // fallback truncate
    const hard = text.trim();
    if (hard.length <= 90) return hard; // can’t shorten safely
    return hard.slice(0, 88).trim() + "…";
  }
  // drop last clause
  return clauses.slice(0, -1).join(". ").trim();
}

export function hardClampExplains(args: {
  lines: OutlineLine[];
  target: number;
  tol: number;
}) {
  let lines = [...args.lines];

  const total = () =>
    lines.reduce((s, l) => s + estimateSeconds(l.text, l.action), 0);

  let t = total();
  let guard = 0;

  while (t > args.target + args.tol && guard++ < 40) {
    const explainIdxs = lines
      .map((l, i) => ({ l, i }))
      .filter((x) => x.l.key === "EXPLAIN");

    if (explainIdxs.length === 0) break;

    // pick the explain line with max estimated seconds
    explainIdxs.sort(
      (a, b) =>
        estimateSeconds(b.l.text, b.l.action) - estimateSeconds(a.l.text, a.l.action)
    );

    const pick = explainIdxs[0];
    const before = pick.l.text;
    const after = trimOneStep(before);

    // if trimming no longer changes anything, stop
    if (after === before) break;

    lines[pick.i] = { ...pick.l, text: after };
    t = total();
  }

  return { lines, seconds: t };
}
3/6 — Patch your normalizer to call hard clamp at the end

// tools/format/normalize-outline.ts (PATCH)
import { hardClampExplains } from "./hard-clamp";

// ... inside normalizeOutlineToDuration, right before returning:
const clamped = hardClampExplains({
  lines: current,
  target: cfg.targetSeconds,
  tol: cfg.toleranceSeconds
});

return { outline: outlineToText(clamped.lines), seconds: clamped.seconds };
Now even if the script is huge, it will keep shaving EXPLAIN until it hits the window.

4/6 — Optional “minimum explain protection”

So you don’t over-trim into nonsense, add a floor:

// inside hardClampExplains loop, before trimming:
const explainCount = explainIdxs.length;
if (explainCount <= 1 && splitClauses(explainIdxs[0].l.text).length <= 1) break;
(This prevents it from turning your last explain into a stump.)

5/6 — Update the CLI output to show clamp result

// tools/format/build-outline-from-script-normalized.ts (PATCH)
console.log(`⏱️ Final duration estimate: ${normalized.seconds.toFixed(1)}s (target ${DEFAULT_NORMALIZE.targetSeconds}s ± ${DEFAULT_NORMALIZE.toleranceSeconds}s)`);
6/6 — Final behavior (what you get now)

From data/script.txt you get:

a normalized data/outline.txt that always fits ~58s

hybrid_format.json and beats.sec.json derived from it

then your SFX pipeline uses beats.sec.json + visual_reveals.json

Run:

pnpm mc:hybrid:final
If you want the mirror feature too: I can add a hard “pad to target” that adds 1–2 ultra-safe EXPLAIN fillers (like “So the pipeline is: cues → compile → audio bus → render.”) when the script is too short.

You said:

yes that would be cool
ChatGPT said:

Sweet — here’s the hard pad to target (no LLM) that bumps you up into 58 ± 1.5s when the script is too short, without messing with the narrative.

It works by:

inserting 1–3 “safe explain fillers” (domain-aware) right after REVEAL

choosing fillers based on your domain_dict.json (video_tooling/automation/backend/analytics)

stopping as soon as you’re inside the window

1/6 — Add a filler library (tools/format/fillers.ts)

import { loadDomainDict, countKeywordHits } from "./domain-dict";

export type Filler = {
  id: string;
  text: string;
  tags: string[]; // used to match to script topic
};

export function buildFillers() : Filler[] {
  return [
    {
      id: "pipeline_overview",
      text: "Pipeline is simple: cues → compile → mix one audio bus → final render.",
      tags: ["pipeline", "audio bus", "ffmpeg", "render", "sfx"]
    },
    {
      id: "macros_explained",
      text: "Macros let you tag intent like transition or emphasis instead of picking files by hand.",
      tags: ["macro", "cues", "intent", "transition"]
    },
    {
      id: "policy_guard",
      text: "Policy guards keep spacing clean so effects never spam or overlap awkwardly.",
      tags: ["policy", "spacing", "cooldown"]
    },
    {
      id: "timeline_editable",
      text: "Because timing is event-based, you can slide markers on the timeline and everything stays synced.",
      tags: ["timeline", "markers", "waitUntil"]
    },
    {
      id: "devlog_value",
      text: "This is perfect for dev logs: errors get a buzz, wins get a ding, and the story stays tight.",
      tags: ["devlog", "error", "success"]
    }
  ];
}

export function pickFillersForText(text: string, max = 3) {
  const dict = loadDomainDict();
  const fillers = buildFillers();

  const lower = text.toLowerCase();

  // score fillers based on overlap with domain keywords + filler tags
  const allDomainKeywords = dict.domains.flatMap(d => d.keywords);
  return fillers
    .map(f => {
      const score =
        countKeywordHits(lower, f.tags) * 3 +
        countKeywordHits(lower, allDomainKeywords) * 0.25 +
        countKeywordHits(f.text.toLowerCase(), allDomainKeywords) * 0.5;
      return { f, score };
    })
    .sort((a, b) => b.score - a.score)
    .map(x => x.f)
    .slice(0, max);
}
2/6 — Padder: insert EXPLAIN fillers until within window

// tools/format/hard-pad.ts
import { estimateSeconds } from "./timing";
import type { OutlineLine } from "./outline-utils";
import type { NormalizeConfig } from "./normalize-config";
import { pickFillersForText } from "./fillers";

function estimateTotal(lines: OutlineLine[]) {
  return lines.reduce((s, l) => s + estimateSeconds(l.text, l.action), 0);
}

export function hardPadToTarget(args: {
  lines: OutlineLine[];
  cfg: NormalizeConfig;
}) {
  let lines = [...args.lines];
  let total = estimateTotal(lines);

  if (total >= args.cfg.targetSeconds - args.cfg.toleranceSeconds) {
    return { lines, seconds: total };
  }

  const joinedText = lines.map(l => l.text).join(" ");
  const fillers = pickFillersForText(joinedText, 4);

  // Insert after REVEAL if possible, otherwise after PROBLEM, otherwise near top
  const idxReveal = lines.findIndex(l => l.key === "REVEAL");
  const insertAt =
    idxReveal >= 0 ? idxReveal + 1 :
    Math.max(0, lines.findIndex(l => l.key === "PROBLEM") + 1);

  let added = 0;
  for (const f of fillers) {
    if (total >= args.cfg.targetSeconds - args.cfg.toleranceSeconds) break;

    lines.splice(insertAt + added, 0, {
      key: "EXPLAIN",
      action: "explain",
      text: f.text
    });

    added++;
    total = estimateTotal(lines);

    // Safety: don’t add more than 3 fillers
    if (added >= 3) break;
  }

  return { lines, seconds: total };
}
3/6 — Patch normalizer: clamp if too long, pad if too short

Add padding at the end of normalizeOutlineToDuration.

// tools/format/normalize-outline.ts (PATCH)
import { hardPadToTarget } from "./hard-pad";

// ... after hardClampExplains, before return:
const padded = hardPadToTarget({
  lines: clamped.lines,
  cfg
});

// If padding overshoots slightly, clamp again (rare but clean)
const reclamped = padded.seconds > cfg.targetSeconds + cfg.toleranceSeconds
  ? hardClampExplains({ lines: padded.lines, target: cfg.targetSeconds, tol: cfg.toleranceSeconds })
  : { lines: padded.lines, seconds: padded.seconds };

return { outline: outlineToText(reclamped.lines), seconds: reclamped.seconds };
4/6 — Why this padding won’t mess up your SFX policy

Because fillers are EXPLAIN, they map to:

text_ping (when visuals reveal bullets/keywords)

and they don’t trigger heavy structural SFX like reveal_riser or cta_sparkle

So pacing stays stable and the narrative doesn’t get diluted.

5/6 — Optional: “micro filler” mode (super minimal)

If you want ultra short fillers, add a second filler set and only insert those when you’re within ~5 seconds of target. (I can generate that list too.)

6/6 — Result

Now your pipeline guarantees:

If script is long → trim to window

If script is short → pad to window

Always ends near 58s without touching core beats

Run as usual:

pnpm mc:hybrid:final
If you want the next upgrade: I can make the padder “visual-aware” so every filler inserted automatically becomes a keyword + bullet pair (stronger visuals + better SFX timing).

You said:

yes that would be cool
ChatGPT said:

Nice — let’s make padding visual-aware so when we insert filler, it creates (keyword → bullet) pairs automatically. That means:

stronger visuals (big term + supporting line)

better SFX (keyword gets text_ping + the bullet gets another ping)

your reveal capture stays automatic via Keyword/Bullet components

Below is the deterministic implementation.

1/6 — Add a “visual filler” library (keyword + bullet)

// tools/format/visual-fillers.ts
import { loadDomainDict, countKeywordHits } from "./domain-dict";

export type VisualFiller = {
  id: string;
  keyword: string;   // becomes KEYWORD block
  bullet: string;    // becomes EXPLAIN/BULLET block
  tags: string[];
};

export function buildVisualFillers(): VisualFiller[] {
  return [
    {
      id: "audio_bus",
      keyword: "One Audio Bus",
      bullet: "Mix voice, music, and SFX into one audio_bus.wav so timing never drifts.",
      tags: ["audio bus", "ffmpeg", "mix", "render", "timing"]
    },
    {
      id: "macro_intent",
      keyword: "Macro Cues",
      bullet: "Use intent tags like transition_fast and text_ping instead of picking files by hand.",
      tags: ["macro", "cues", "intent", "transition"]
    },
    {
      id: "policy_guards",
      keyword: "Policy Guards",
      bullet: "Cooldown + spacing rules prevent spam and keep the mix clean.",
      tags: ["policy", "cooldown", "spacing"]
    },
    {
      id: "timeline_events",
      keyword: "Timeline Events",
      bullet: "Time markers stay editable—slide events and the whole soundtrack re-aligns.",
      tags: ["timeline", "waitUntil", "events", "markers"]
    },
    {
      id: "devlog_beats",
      keyword: "Devlog Beats",
      bullet: "Errors get a buzz, wins get a ding—story stays tight and readable.",
      tags: ["devlog", "error", "success"]
    }
  ];
}

export function pickVisualFillersForText(text: string, max = 2) {
  const dict = loadDomainDict();
  const fillers = buildVisualFillers();

  const lower = text.toLowerCase();
  const allDomainKeywords = dict.domains.flatMap(d => d.keywords);

  return fillers
    .map(v => {
      const score =
        countKeywordHits(lower, v.tags) * 3 +
        countKeywordHits(lower, allDomainKeywords) * 0.25 +
        countKeywordHits((v.keyword + " " + v.bullet).toLowerCase(), allDomainKeywords) * 0.5;
      return { v, score };
    })
    .sort((a, b) => b.score - a.score)
    .map(x => x.v)
    .slice(0, max);
}
2/6 — Upgrade the padder: insert keyword + explain pair

Instead of inserting a single EXPLAIN, we insert:

REVEAL: stays untouched

Insert a KEYWORD: (mapped to keyword block)

Then an EXPLAIN: bullet line

// tools/format/hard-pad-visual.ts
import { estimateSeconds } from "./timing";
import type { OutlineLine } from "./outline-utils";
import type { NormalizeConfig } from "./normalize-config";
import { pickVisualFillersForText } from "./visual-fillers";

function estimateTotal(lines: OutlineLine[]) {
  return lines.reduce((s, l) => s + estimateSeconds(l.text, l.action), 0);
}

export function hardPadToTargetVisual(args: {
  lines: OutlineLine[];
  cfg: NormalizeConfig;
}) {
  let lines = [...args.lines];
  let total = estimateTotal(lines);

  if (total >= args.cfg.targetSeconds - args.cfg.toleranceSeconds) {
    return { lines, seconds: total };
  }

  const joined = lines.map(l => l.text).join(" ");
  const picks = pickVisualFillersForText(joined, 3);

  const idxReveal = lines.findIndex(l => l.key === "REVEAL");
  const insertAt =
    idxReveal >= 0 ? idxReveal + 1 :
    Math.max(0, lines.findIndex(l => l.key === "PROBLEM") + 1);

  let addedPairs = 0;

  for (const p of picks) {
    if (total >= args.cfg.targetSeconds - args.cfg.toleranceSeconds) break;

    // KEYWORD line (maps to keyword block)
    lines.splice(insertAt + addedPairs * 2, 0, {
      key: "REVEAL",            // IMPORTANT: keep as REVEAL action for stronger pacing
      action: "reveal",
      text: p.keyword
    });

    // EXPLAIN bullet line
    lines.splice(insertAt + addedPairs * 2 + 1, 0, {
      key: "EXPLAIN",
      action: "explain",
      text: p.bullet
    });

    addedPairs++;
    total = estimateTotal(lines);

    if (addedPairs >= 2) break; // keep it tight (usually enough padding)
  }

  return { lines, seconds: total };
}
Why I used key:"REVEAL" for the keyword line:
It ensures your SFX policy triggers reveal_riser + impact_soft on the inserted keyword, making the filler feel like a legit “mini-reveal” rather than fluff.

If you want them softer, change it to key:"EXPLAIN", action:"explain".

3/6 — Patch the normalizer to use visual padding first

// tools/format/normalize-outline.ts (PATCH)
import { hardPadToTargetVisual } from "./hard-pad-visual";

// ... after clamped:
const padded = hardPadToTargetVisual({
  lines: clamped.lines,
  cfg
});

// If we overshoot, clamp again
const reclamped = padded.seconds > cfg.targetSeconds + cfg.toleranceSeconds
  ? hardClampExplains({ lines: padded.lines, target: cfg.targetSeconds, tol: cfg.toleranceSeconds })
  : { lines: padded.lines, seconds: padded.seconds };

return { outline: outlineToText(reclamped.lines), seconds: reclamped.seconds };
4/6 — Make sure your HybridFormat generator maps REVEAL → keyword block

You already have:

REVEAL action → keyword block
So inserted “keyword fillers” become actual keyword visuals automatically.

5/6 — Resulting outline example (what gets generated)

If script is short, the outline might become:

HOOK: ...
PROBLEM: ...
ERROR: ...
REVEAL: The fix is simple.
REVEAL: One Audio Bus
EXPLAIN: Mix voice, music, and SFX into one audio_bus.wav so timing never drifts.
CODE: pnpm mc:hybrid:final
SUCCESS: ...
CTA: ...
That produces:

keyword reveal visuals

bullet explanation visuals

stronger SFX moments

6/6 — Keep the same command

No change to your workflow:

pnpm mc:hybrid:final
If you want the next level: I can make the visual padder also emit a data/visual_reveals.seed.json so even on the first render (before capture) your SFX policy already has reliable reveal timestamps.

You said:

yes that would be cool
ChatGPT said:

Perfect — we can generate a data/visual_reveals.seed.json before the first render so your SFX policy already has reliable “visual” timestamps, then later the real capture (visual_reveals.json) will override it automatically.

This gives you:

Run 1: seed reveals → policy cues → audio bus → render

Run 2: capture real reveals → re-run policy for tighter sync (optional)

Here’s the drop-in implementation.

1/6 — Seeding rule (simple + consistent)

We’ll seed reveals from beats.sec.json + hybrid_format.json:

If a block has an event, we place the reveal at the beat time for that event.

If not, we approximate reveal time using block order + per-block “lead/lag” offsets.

We generate reveal kinds:

keyword blocks → keyword

bullet blocks → bullet

code blocks → code

error blocks → error

success blocks → success

cta blocks → cta

2/6 — Seeder script: tools/format/seed-visual-reveals.ts

import fs from "node:fs";
import path from "node:path";

type BeatSec = { t: number; text: string; action: string; event?: string; beatId?: string };
type HybridFormat = {
  version: string;
  fps: number;
  blocks: Array<{ id: string; type: string; text: string; event?: string }>;
};

type VisualReveal = {
  t: number;
  kind: "keyword" | "bullet" | "code" | "chart" | "cta" | "error" | "success";
  key?: string;
};

function clamp(n: number, lo: number, hi: number) {
  return Math.max(lo, Math.min(hi, n));
}

function toKind(blockType: string): VisualReveal["kind"] {
  if (blockType === "keyword") return "keyword";
  if (blockType === "bullet") return "bullet";
  if (blockType === "code") return "code";
  if (blockType === "error") return "error";
  if (blockType === "success") return "success";
  if (blockType === "cta") return "cta";
  return "bullet";
}

// small offsets so reveals feel aligned with animations/SFX
function kindOffsetSec(kind: VisualReveal["kind"]) {
  if (kind === "keyword") return 0.00;
  if (kind === "bullet") return 0.03;
  if (kind === "code") return 0.05;
  if (kind === "error") return 0.02;
  if (kind === "success") return 0.02;
  if (kind === "cta") return 0.00;
  return 0.03;
}

const root = process.cwd();
const beatsPath = path.join(root, "data", "beats.sec.json");
const formatPath = path.join(root, "data", "hybrid_format.json");
const outPath = path.join(root, "data", "visual_reveals.seed.json");

if (!fs.existsSync(beatsPath)) throw new Error(`Missing ${beatsPath}`);
if (!fs.existsSync(formatPath)) throw new Error(`Missing ${formatPath}`);

const beats = JSON.parse(fs.readFileSync(beatsPath, "utf-8")) as BeatSec[];
const format = JSON.parse(fs.readFileSync(formatPath, "utf-8")) as HybridFormat;

const eventToTime = new Map<string, number>();
for (const b of beats) {
  if (b.event) eventToTime.set(b.event, b.t);
}

// fallback timeline if events are missing: evenly spread based on beats span
const maxBeatT = beats.length ? beats[beats.length - 1].t : 0;
const span = Math.max(maxBeatT, 1);

const reveals: VisualReveal[] = format.blocks.map((blk, idx) => {
  const kind = toKind(blk.type);
  const baseT =
    (blk.event && eventToTime.has(blk.event))
      ? (eventToTime.get(blk.event) as number)
      : (span * (idx / Math.max(1, format.blocks.length - 1)));

  const t = Number((baseT + kindOffsetSec(kind)).toFixed(3));

  // key is for AI context + later analytics
  const key = blk.text?.slice(0, 80);

  return { t, kind, key };
});

// de-dupe by (kind + key + ~same time bucket)
const deduped: VisualReveal[] = [];
const seen = new Set<string>();
for (const r of reveals.sort((a, b) => a.t - b.t)) {
  const bucket = Math.round(r.t / 0.08); // 80ms buckets
  const sig = `${r.kind}|${r.key ?? ""}|${bucket}`;
  if (seen.has(sig)) continue;
  seen.add(sig);
  deduped.push({ ...r, t: clamp(r.t, 0, span + 5) });
}

fs.mkdirSync(path.dirname(outPath), { recursive: true });
fs.writeFileSync(outPath, JSON.stringify({ version: "1.0.0", reveals: deduped }, null, 2));
console.log(`✅ Wrote ${outPath} (${deduped.length} seeded reveals)`);
3/6 — Make your SFX policy prefer real capture, else seed

Patch your policy loader (where you currently read data/visual_reveals.json) to:

use visual_reveals.json if it exists (real)

else use visual_reveals.seed.json

else fall back to empty

// tools/sfx/load-visual-reveals.ts
import fs from "node:fs";
import path from "node:path";

export function loadVisualReveals() {
  const root = process.cwd();
  const realPath = path.join(root, "data", "visual_reveals.json");
  const seedPath = path.join(root, "data", "visual_reveals.seed.json");

  if (fs.existsSync(realPath)) {
    return JSON.parse(fs.readFileSync(realPath, "utf-8"));
  }
  if (fs.existsSync(seedPath)) {
    return JSON.parse(fs.readFileSync(seedPath, "utf-8"));
  }
  return { version: "1.0.0", reveals: [] };
}
Then in your write-macro-cues-from-policy.ts, replace the reveal-file read with:

import { loadVisualReveals } from "./load-visual-reveals";
// ...
const revealsFile = loadVisualReveals();
4/6 — Add scripts to chain seeding into your existing generator

{
  "scripts": {
    "format:seed-reveals": "ts-node tools/format/seed-visual-reveals.ts",
    "format:from-script": "ts-node tools/format/generate-from-script.ts",
    "format:prep": "pnpm format:from-script && pnpm format:seed-reveals"
  }
}
Now format:prep creates:

outline.txt

hybrid_format.json

beats.sec.json

visual_reveals.seed.json

5/6 — Upgrade your one-command pipeline to use seeding on run 1

{
  "scripts": {
    "mc:hybrid:final": "pnpm format:prep && pnpm mc:render:auto && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus && pnpm mc:render:auto"
  }
}
What happens:

First mc:render:auto can render with seed-based cues (good enough)

Second render is the final after audio bus

If you want, you can optionally re-run policy after the first render capture for even tighter sync (see next).

6/6 — Optional “ultra tight” mode (recompute after capture)

If you want to always refine after real capture:

{
  "scripts": {
    "mc:hybrid:final:tight": "pnpm format:prep && pnpm mc:render:auto && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus && pnpm mc:render:auto && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus && pnpm mc:render:auto"
  }
}
That gives:

seed → render → capture real

real → rebuild audio → final render

If you want the next upgrade: I can make the seeder use your exact block durations (from the timing estimator) instead of evenly spreading blocks, so seed timestamps match the “spoken pacing” even closer on run 1.

You said:

yes that would be cool
ChatGPT said:

Awesome — here’s the pacing-accurate seeder: instead of evenly spreading blocks, it uses your same timing estimator (words/sec + pauses) to generate realistic reveal timestamps on Run 1.

Result: visual_reveals.seed.json lines up much closer to where visuals will actually land.

1/6 — What changes

We compute a “virtual timeline” from hybrid_format.json blocks:

tCursor = 0

for each block:

set reveal time = tCursor + offset(kind)

advance tCursor += estimateSeconds(block.text, action)

clamp + dedupe like before

This mirrors your beats generation pacing and keeps seed reveals consistent.

2/6 — Map block type → beat action (for timing model)

// tools/format/block-action.ts
import type { BeatAction } from "../../src/format/hybrid-types";

export function blockTypeToAction(type: string): BeatAction {
  if (type === "keyword") return "reveal";
  if (type === "bullet") return "explain";
  if (type === "code") return "code";
  if (type === "error") return "error";
  if (type === "success") return "success";
  if (type === "cta") return "cta";
  return "explain";
}
3/6 — Replace the seeder with pacing-based time cursor

// tools/format/seed-visual-reveals-paced.ts
import fs from "node:fs";
import path from "node:path";
import { estimateSeconds } from "./timing";
import { blockTypeToAction } from "./block-action";

type HybridFormat = {
  version: string;
  fps: number;
  blocks: Array<{ id: string; type: string; text: string; event?: string }>;
};

type VisualReveal = {
  t: number;
  kind: "keyword" | "bullet" | "code" | "chart" | "cta" | "error" | "success";
  key?: string;
};

function clamp(n: number, lo: number, hi: number) {
  return Math.max(lo, Math.min(hi, n));
}

function toKind(blockType: string): VisualReveal["kind"] {
  if (blockType === "keyword") return "keyword";
  if (blockType === "bullet") return "bullet";
  if (blockType === "code") return "code";
  if (blockType === "error") return "error";
  if (blockType === "success") return "success";
  if (blockType === "cta") return "cta";
  return "bullet";
}

// offsets to feel like visuals land with animation
function kindOffsetSec(kind: VisualReveal["kind"]) {
  if (kind === "keyword") return 0.00;
  if (kind === "bullet") return 0.05;
  if (kind === "code") return 0.07;
  if (kind === "error") return 0.03;
  if (kind === "success") return 0.03;
  if (kind === "cta") return 0.00;
  return 0.05;
}

const root = process.cwd();
const formatPath = path.join(root, "data", "hybrid_format.json");
const outPath = path.join(root, "data", "visual_reveals.seed.json");

if (!fs.existsSync(formatPath)) throw new Error(`Missing ${formatPath}`);
const format = JSON.parse(fs.readFileSync(formatPath, "utf-8")) as HybridFormat;

let tCursor = 0;

const reveals: VisualReveal[] = [];
for (const blk of format.blocks) {
  const kind = toKind(blk.type);
  const action = blockTypeToAction(blk.type);

  const t = Number((tCursor + kindOffsetSec(kind)).toFixed(3));
  const key = blk.text?.slice(0, 80);

  reveals.push({ t, kind, key });

  // advance cursor by estimated duration of this block (speech pacing)
  const dur = estimateSeconds(blk.text, action);
  tCursor += dur;
}

const totalSpan = Math.max(tCursor, 1);

// de-dupe by (kind + key + time bucket)
const deduped: VisualReveal[] = [];
const seen = new Set<string>();
for (const r of reveals.sort((a, b) => a.t - b.t)) {
  const bucket = Math.round(r.t / 0.08); // 80ms buckets
  const sig = `${r.kind}|${r.key ?? ""}|${bucket}`;
  if (seen.has(sig)) continue;
  seen.add(sig);
  deduped.push({ ...r, t: clamp(r.t, 0, totalSpan) });
}

fs.mkdirSync(path.dirname(outPath), { recursive: true });
fs.writeFileSync(outPath, JSON.stringify({ version: "1.0.0", reveals: deduped, meta: { method: "paced_cursor", totalSpan: Number(totalSpan.toFixed(2)) } }, null, 2));
console.log(`✅ Wrote ${outPath} (${deduped.length} paced reveals, span ~${totalSpan.toFixed(1)}s)`);
4/6 — Update scripts to use the paced seeder

{
  "scripts": {
    "format:seed-reveals": "ts-node tools/format/seed-visual-reveals-paced.ts",
    "format:prep": "pnpm format:from-script && pnpm format:seed-reveals"
  }
}
5/6 — Why this works better than beat-based seeding

Beat-based seeding depends on event matches and ordering.
Pacing-based seeding always works because it uses:

your block order

your timing estimator

the same cadence your video format already expects

So even on Run 1 (before any capture), SFX cues land close to where the visuals will land.

6/6 — Optional: seed BOTH reveals and beats from the same cursor (perfect consistency)

If you want absolute determinism: we can generate beats.sec.json directly from the block cursor too (so beats + seed reveals share the same timestamps). That removes any mismatch between “beats” vs “format blocks.”

If you say “do it,” I’ll give you the replacement generate-hybrid.ts that produces:

hybrid_format.json

beats.sec.json (from blocks)

visual_reveals.seed.json (from blocks)
all in one pass.

You said:

yes that would be cool
ChatGPT said:

Locked in — here’s the one-pass generator that produces all three from the same pacing cursor:

data/hybrid_format.json

data/beats.sec.json

data/visual_reveals.seed.json

So beats + seed reveals can’t drift, ever.

Below is the drop-in replacement.

1/6 — New “single source of truth” rule

We treat hybrid_format.json blocks as the truth.

From blocks we derive:

beat action + beat time (cursor)

seed reveal time (cursor + kindOffset)

No more “outline → beats” and “format → seed” separately.

2/6 — Replace your generator with: tools/format/generate-pack.ts

This reads data/outline.txt and writes all 3 outputs.

// tools/format/generate-pack.ts
import fs from "node:fs";
import path from "node:path";
import crypto from "node:crypto";
import type { HybridFormat, FormatBlock, BeatSec, BeatAction } from "../../src/format/hybrid-types";
import { estimateSeconds } from "./timing";

type VisualReveal = {
  t: number;
  kind: "keyword" | "bullet" | "code" | "chart" | "cta" | "error" | "success";
  key?: string;
};

function slug(s: string) {
  return s
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, "_")
    .replace(/^_+|_+$/g, "")
    .slice(0, 48);
}

function idHash(s: string) {
  return crypto.createHash("md5").update(s).digest("hex").slice(0, 6);
}

function parseOutlineLine(line: string): { key: string; action: BeatAction; text: string } | null {
  const m = line.match(/^([A-Z_]+)\s*:\s*(.+)$/);
  if (!m) return null;

  const key = m[1].toUpperCase();
  const text = m[2].trim();

  const map: Record<string, BeatAction> = {
    HOOK: "hook",
    PROBLEM: "problem",
    TRANSITION: "transition",
    REVEAL: "reveal",
    EXPLAIN: "explain",
    CODE: "code",
    ERROR: "error",
    SUCCESS: "success",
    PUNCHLINE: "punchline",
    CTA: "cta",
    OUTRO: "outro"
  };

  const action = map[key];
  if (!action) return null;
  return { key, action, text };
}

function actionToBlockType(action: BeatAction): FormatBlock["type"] {
  if (action === "hook" || action === "reveal") return "keyword";
  if (action === "explain" || action === "problem" || action === "transition" || action === "outro" || action === "punchline")
    return "bullet";
  if (action === "code") return "code";
  if (action === "error") return "error";
  if (action === "success") return "success";
  if (action === "cta") return "cta";
  return "bullet";
}

function blockTypeToRevealKind(type: FormatBlock["type"]): VisualReveal["kind"] {
  if (type === "keyword") return "keyword";
  if (type === "bullet") return "bullet";
  if (type === "code") return "code";
  if (type === "error") return "error";
  if (type === "success") return "success";
  if (type === "cta") return "cta";
  return "bullet";
}

function kindOffsetSec(kind: VisualReveal["kind"]) {
  if (kind === "keyword") return 0.0;
  if (kind === "bullet") return 0.05;
  if (kind === "code") return 0.07;
  if (kind === "error") return 0.03;
  if (kind === "success") return 0.03;
  if (kind === "cta") return 0.0;
  return 0.05;
}

function clamp(n: number, lo: number, hi: number) {
  return Math.max(lo, Math.min(hi, n));
}

const root = process.cwd();

const outlinePath = path.join(root, "data", "outline.txt");
const outFormatPath = path.join(root, "data", "hybrid_format.json");
const outBeatsPath = path.join(root, "data", "beats.sec.json");
const outSeedRevealsPath = path.join(root, "data", "visual_reveals.seed.json");

if (!fs.existsSync(outlinePath)) throw new Error(`Missing ${outlinePath}`);

const fps = 30;

const rawLines = fs
  .readFileSync(outlinePath, "utf-8")
  .split(/\r?\n/)
  .map((l) => l.trim())
  .filter((l) => l.length > 0 && !l.startsWith("#"));

const parsed = rawLines
  .map(parseOutlineLine)
  .filter((x): x is { key: string; action: BeatAction; text: string } => Boolean(x));

if (parsed.length === 0) throw new Error("No valid outline lines found.");

let tCursor = 0;

const blocks: FormatBlock[] = [];
const beats: BeatSec[] = [];
const reveals: VisualReveal[] = [];

for (let i = 0; i < parsed.length; i++) {
  const { key, action, text } = parsed[i];

  const blockType = actionToBlockType(action);
  const h = idHash(text);
  const event = `${action}_${slug(text)}_${h}`;

  const blockId = `${blockType}_${slug(text)}_${h}`;
  blocks.push({ id: blockId, type: blockType as any, text, event });

  const beatId = `b${String(i + 1).padStart(2, "0")}_${action}`;
  beats.push({
    beatId,
    t: Number(tCursor.toFixed(3)),
    text,
    action,
    event
  });

  const kind = blockTypeToRevealKind(blockType);
  reveals.push({
    t: Number((tCursor + kindOffsetSec(kind)).toFixed(3)),
    kind,
    key: text.slice(0, 80)
  });

  const dur = estimateSeconds(text, action);
  tCursor += dur;
}

const totalSpan = Math.max(tCursor, 1);

// dedupe seed reveals into 80ms buckets
const deduped: VisualReveal[] = [];
const seen = new Set<string>();
for (const r of reveals.sort((a, b) => a.t - b.t)) {
  const bucket = Math.round(r.t / 0.08);
  const sig = `${r.kind}|${r.key ?? ""}|${bucket}`;
  if (seen.has(sig)) continue;
  seen.add(sig);
  deduped.push({ ...r, t: clamp(r.t, 0, totalSpan) });
}

const format: HybridFormat = {
  version: "1.0.0",
  fps,
  style: { theme: "clean_dark", fontScale: 1.0 },
  blocks
};

fs.mkdirSync(path.dirname(outFormatPath), { recursive: true });
fs.writeFileSync(outFormatPath, JSON.stringify(format, null, 2));
fs.writeFileSync(outBeatsPath, JSON.stringify(beats, null, 2));
fs.writeFileSync(
  outSeedRevealsPath,
  JSON.stringify(
    { version: "1.0.0", reveals: deduped, meta: { method: "paced_cursor", totalSpan: Number(totalSpan.toFixed(2)) } },
    null,
    2
  )
);

console.log(`✅ Wrote ${outFormatPath}`);
console.log(`✅ Wrote ${outBeatsPath}`);
console.log(`✅ Wrote ${outSeedRevealsPath}`);
console.log(`⏱️ Estimated span: ${totalSpan.toFixed(1)}s (${Math.round(totalSpan * fps)} frames @ ${fps}fps)`);
3/6 — Update format:from-script to call this pack generator

Replace the second step (previously generate-hybrid.ts) with generate-pack.ts.

// tools/format/generate-from-script.ts (PATCH)
import { spawnSync } from "node:child_process";

function run(cmd: string, args: string[]) {
  const r = spawnSync(cmd, args, { stdio: "inherit" });
  if (r.status !== 0) process.exit(r.status ?? 1);
}

const bin = process.platform === "win32" ? "pnpm.cmd" : "pnpm";

// 1) script -> normalized outline
run(bin, ["ts-node", "tools/format/build-outline-from-script-normalized.ts"]);

// 2) outline -> format + beats + seed reveals (single pass)
run(bin, ["ts-node", "tools/format/generate-pack.ts"]);
4/6 — Remove the old seeding step from scripts

Because generate-pack.ts already writes visual_reveals.seed.json, you can simplify:

{
  "scripts": {
    "format:from-script": "ts-node tools/format/generate-from-script.ts",
    "format:prep": "pnpm format:from-script"
  }
}
5/6 — Make SFX policy loader prefer real capture, else seed (unchanged)

Keep your reveal loader logic:

use visual_reveals.json if present

else fallback to visual_reveals.seed.json

So on a fresh run (no capture yet) the policy still has reveals.

6/6 — Your one command stays clean

pnpm mc:hybrid:final
Flow:

script.txt → outline (normalized + clamped/padded)

outline → hybrid_format + beats + seed_reveals (same cursor)

render (captures real reveals)

policy uses real reveals if available, else seed

compile → audio bus → final render

If you want the next upgrade: I can make the pack generator also write data/timeline.events.json (a clean list of eventName → timeSec) so you can auto-construct Motion Canvas Time Events (or overlay markers) without manual placement.

You said:

yes that would be cool
ChatGPT said:

Alright — here’s the upgrade that outputs a data/timeline.events.json (eventName → timeSec + metadata) in the same one-pass generator, so you can:

auto-create Motion Canvas Time Events (or at least auto-display markers)

keep event names consistent with waitUntil(eventName)

avoid manual time-marking

1) Output format: data/timeline.events.json

Example shape:

{
  "version": "1.0.0",
  "fps": 30,
  "events": [
    { "name": "hook_i_tried_to_automate_sfx_in_motion_canvas_1a2b3c", "t": 0.0, "action": "hook", "blockId": "keyword_i_tried_to_automate_sfx_in_motion_canvas_1a2b3c", "text": "I tried to automate SFX in Motion Canvas…" },
    { "name": "error_build_failed_audio_drift_9f8e7d", "t": 12.84, "action": "error", "blockId": "error_build_failed_audio_drift_9f8e7d", "text": "Build failed: audio drift 😭" }
  ]
}
2) Patch tools/format/generate-pack.ts to also write timeline events

Add this timeline events type and writer inside the same script.

// ADD near top of generate-pack.ts
type TimelineEvent = {
  name: string;
  t: number;         // seconds
  action: BeatAction;
  blockId: string;
  text: string;
};

const outTimelinePath = path.join(root, "data", "timeline.events.json");
Then inside the loop (right after you create event, blockId, and push beats):

// ADD inside the for-loop after beats.push(...)
const timelineEvents: TimelineEvent[] = []; // <-- move this ABOVE the loop (once)

timelineEvents.push({
  name: event,
  t: Number(tCursor.toFixed(3)),
  action,
  blockId,
  text: text.slice(0, 140)
});
And after writing the other files, write the timeline file:

// ADD at bottom of generate-pack.ts, after seed reveals write
fs.writeFileSync(
  outTimelinePath,
  JSON.stringify(
    { version: "1.0.0", fps, events: timelineEvents },
    null,
    2
  )
);

console.log(`✅ Wrote ${outTimelinePath}`);
Important: make sure timelineEvents is declared once before the loop:

const timelineEvents: TimelineEvent[] = [];
for (...) { ... }
3) What you can do with this immediately (no editor automation required)

Even if you don’t auto-inject timeline events into the editor yet, this file is already useful for:

drawing a debug overlay (“current event name / next event / time to next”)

generating a “marker list” UI in your app

building SFX/visual QA reports

4) Optional: Auto-overlay markers in the render (zero editor hacks)

If you want visible markers in preview renders, add a tiny overlay scene component that reads timeline.events.json and shows:

next 3 events

countdown to each

current beat action

That makes tuning formats way faster.

5) Optional: Auto-create Motion Canvas Time Events

Motion Canvas “Time Events” are editor-side, but you can still automate them in two practical ways:

Option A (reliable): “event-driven only”

Never rely on editor time events. Instead:

waitUntil(eventName) drives timing

events exist because your generator emits the same names

your scene plays in deterministic sequence

Option B (advanced): Scripted editor injection

If you want the editor timeline populated:

use Playwright to open the editor page

programmatically paste/import the events (depends on current editor UI and API surface)

not guaranteed stable across versions

Given you want scale + reliability, Option A is usually the move.

6) Next best upgrade (high leverage)

Want me to also output:

data/timeline.events.csv (easy eyeballing)

data/qa.timeline_report.json (gaps, overlaps, “too dense SFX risk” warnings)

data/marker_overlay_config.json for the on-video overlay

Say the word and I’ll drop the generator patches for those too.

You said:

yes that would be cool
ChatGPT said:

Cool — here are the three extra outputs wired into the same one-pass generator:

data/timeline.events.csv (easy eyeballing / import anywhere)

data/qa.timeline_report.json (gaps, density warnings, coverage)

data/marker_overlay_config.json (what to show on the debug overlay)

I’ll give you the drop-in code patches + a tiny overlay component you can toggle on/off.

1) CSV writer helper (tools/format/write-csv.ts)

import fs from "node:fs";

function esc(v: any) {
  const s = String(v ?? "");
  if (/[,"\n]/.test(s)) return `"${s.replace(/"/g, '""')}"`;
  return s;
}

export function writeCsv(filePath: string, rows: Array<Record<string, any>>) {
  if (rows.length === 0) {
    fs.writeFileSync(filePath, "");
    return;
  }
  const headers = Object.keys(rows[0]);
  const lines = [
    headers.map(esc).join(","),
    ...rows.map(r => headers.map(h => esc(r[h])).join(","))
  ];
  fs.writeFileSync(filePath, lines.join("\n"), "utf-8");
}
2) QA report generator (tools/format/timeline-qa.ts)

This checks:

event spacing (min gap)

“dense zones” (too many events in a short window)

distribution by action

import type { BeatAction } from "../../src/format/hybrid-types";

export type TimelineEvent = {
  name: string;
  t: number;
  action: BeatAction;
  blockId: string;
  text: string;
};

export type TimelineQA = {
  version: "1.0.0";
  totalEvents: number;
  durationSec: number;
  minGapSec: number;
  avgGapSec: number;
  denseZones: Array<{ start: number; end: number; count: number }>;
  actionCounts: Record<string, number>;
  gapWarnings: Array<{ a: string; b: string; gap: number }>;
};

export function buildTimelineQA(events: TimelineEvent[], durationSec: number): TimelineQA {
  const sorted = [...events].sort((a, b) => a.t - b.t);
  const gaps: number[] = [];
  const gapWarnings: TimelineQA["gapWarnings"] = [];

  const MIN_GAP = 0.35;          // tweakable: “too fast to read”
  const DENSE_WINDOW = 3.0;      // seconds
  const DENSE_THRESHOLD = 6;     // 6+ events inside 3s = dense zone

  for (let i = 1; i < sorted.length; i++) {
    const gap = sorted[i].t - sorted[i - 1].t;
    gaps.push(gap);
    if (gap < MIN_GAP) {
      gapWarnings.push({ a: sorted[i - 1].name, b: sorted[i].name, gap: Number(gap.toFixed(3)) });
    }
  }

  const minGapSec = gaps.length ? Math.min(...gaps) : 0;
  const avgGapSec = gaps.length ? gaps.reduce((a, b) => a + b, 0) / gaps.length : 0;

  // Dense zones
  const denseZones: TimelineQA["denseZones"] = [];
  let left = 0;
  for (let right = 0; right < sorted.length; right++) {
    while (sorted[right].t - sorted[left].t > DENSE_WINDOW) left++;
    const count = right - left + 1;
    if (count >= DENSE_THRESHOLD) {
      denseZones.push({
        start: Number(sorted[left].t.toFixed(3)),
        end: Number(sorted[right].t.toFixed(3)),
        count
      });
    }
  }

  // action counts
  const actionCounts: Record<string, number> = {};
  for (const e of sorted) actionCounts[e.action] = (actionCounts[e.action] ?? 0) + 1;

  return {
    version: "1.0.0",
    totalEvents: sorted.length,
    durationSec: Number(durationSec.toFixed(2)),
    minGapSec: Number(minGapSec.toFixed(3)),
    avgGapSec: Number(avgGapSec.toFixed(3)),
    denseZones,
    actionCounts,
    gapWarnings
  };
}
3) Marker overlay config (tools/format/marker-overlay-config.ts)

This defines what the debug overlay should show.

export type MarkerOverlayConfig = {
  version: "1.0.0";
  enabledByDefault: boolean;
  showNextN: number;
  showCountdown: boolean;
  showActionBadges: boolean;
  opacity: number;
  scale: number;
};

export const DEFAULT_MARKER_OVERLAY: MarkerOverlayConfig = {
  version: "1.0.0",
  enabledByDefault: false,
  showNextN: 3,
  showCountdown: true,
  showActionBadges: true,
  opacity: 0.85,
  scale: 1.0
};
4) Patch generate-pack.ts to write CSV + QA + overlay config

Add these imports at top of tools/format/generate-pack.ts:

import { writeCsv } from "./write-csv";
import { buildTimelineQA, type TimelineEvent } from "./timeline-qa";
import { DEFAULT_MARKER_OVERLAY } from "./marker-overlay-config";
Add output paths:

const outTimelineCsvPath = path.join(root, "data", "timeline.events.csv");
const outQAPath = path.join(root, "data", "qa.timeline_report.json");
const outOverlayCfgPath = path.join(root, "data", "marker_overlay_config.json");
Make sure you already have timelineEvents: TimelineEvent[] = []; and push inside the loop as previously described.

Then after your existing writes, add:

// CSV
writeCsv(outTimelineCsvPath, timelineEvents.map(e => ({
  t: e.t,
  name: e.name,
  action: e.action,
  blockId: e.blockId,
  text: e.text
})));
console.log(`✅ Wrote ${outTimelineCsvPath}`);

// QA
const qa = buildTimelineQA(timelineEvents, totalSpan);
fs.writeFileSync(outQAPath, JSON.stringify(qa, null, 2));
console.log(`✅ Wrote ${outQAPath}`);

// Overlay config (static)
fs.writeFileSync(outOverlayCfgPath, JSON.stringify(DEFAULT_MARKER_OVERLAY, null, 2));
console.log(`✅ Wrote ${outOverlayCfgPath}`);
That’s it — now your generator produces 6 artifacts total.

5) Debug overlay component (Motion Canvas)

This reads timeline.events.json + marker_overlay_config.json and overlays “next events” live.

// src/overlays/MarkerOverlay.tsx
import React from "react";
import fs from "node:fs";
import path from "node:path";
import {Rect, Txt} from "@motion-canvas/2d";
import {useTime} from "@motion-canvas/core";

type TimelineEvent = { name: string; t: number; action: string; text: string };
type OverlayCfg = {
  enabledByDefault: boolean;
  showNextN: number;
  showCountdown: boolean;
  showActionBadges: boolean;
  opacity: number;
  scale: number;
};

function readJson<T>(p: string, fallback: T): T {
  try {
    if (!fs.existsSync(p)) return fallback;
    return JSON.parse(fs.readFileSync(p, "utf-8")) as T;
  } catch {
    return fallback;
  }
}

export function MarkerOverlay() {
  const tNow = useTime();
  const root = process.cwd();

  const timelinePath = path.join(root, "data", "timeline.events.json");
  const cfgPath = path.join(root, "data", "marker_overlay_config.json");

  const timeline = readJson<{events: TimelineEvent[]}>(timelinePath, {events: []});
  const cfg = readJson<OverlayCfg>(cfgPath, {
    enabledByDefault: false, showNextN: 3, showCountdown: true, showActionBadges: true, opacity: 0.85, scale: 1.0
  });

  if (!cfg.enabledByDefault) return null;

  const events = [...timeline.events].sort((a, b) => a.t - b.t);
  const next = events.filter(e => e.t >= tNow).slice(0, cfg.showNextN);

  const lines = next.map(e => {
    const dt = Math.max(0, e.t - tNow);
    const countdown = cfg.showCountdown ? ` (+${dt.toFixed(2)}s)` : "";
    const badge = cfg.showActionBadges ? `[${e.action}] ` : "";
    return `${badge}${e.name}${countdown}`;
  });

  return (
    <Rect
      x={-800}
      y={-420}
      width={900}
      height={260}
      radius={18}
      padding={18}
      opacity={cfg.opacity}
      scale={cfg.scale}
    >
      <Txt text={`t=${tNow.toFixed(2)}s`} fontSize={34} />
      <Txt y={60} text={lines.join("\n")} fontSize={26} />
    </Rect>
  );
}
6) Toggle overlay in your scene (one line)

// src/scenes/main.tsx
import {MarkerOverlay} from "../overlays/MarkerOverlay";

// inside scene before/after blocks:
view.add(<MarkerOverlay />);
Turn it on by setting in data/marker_overlay_config.json:

{ "enabledByDefault": true }
If you want, I can also add a “QA gate”: fail the build if qa.timeline_report.json finds (a) any gap < 0.2s or (b) any dense zone count ≥ 8 — so bad pacing never ships.

You said:

yes that would be cool
ChatGPT said:

Nice — here’s the QA gate that can fail your pipeline when pacing gets too chaotic (or just warn, depending on mode).

You’ll get:

pnpm qa:timeline (checks data/qa.timeline_report.json)

pnpm mc:hybrid:final will stop early if QA fails (optional)

1) QA gate rules (sane defaults)

Fail build if any of these trigger:

Ultra-tight gaps: any event gap < 0.20s

Too many dense zones: any dense zone with count >= 8 inside 3s window

Too many gap warnings: more than 12 gaps under 0.35s (readability issue)

All configurable.

2) QA gate script (tools/qa/timeline-gate.ts)

import fs from "node:fs";
import path from "node:path";

type TimelineQA = {
  version: string;
  totalEvents: number;
  durationSec: number;
  minGapSec: number;
  avgGapSec: number;
  denseZones: Array<{ start: number; end: number; count: number }>;
  actionCounts: Record<string, number>;
  gapWarnings: Array<{ a: string; b: string; gap: number }>;
};

type GateConfig = {
  mode: "fail" | "warn";
  minHardGapSec: number;        // fail if any gap below this
  maxDenseZoneCount: number;    // fail if any dense zone count >= this
  maxGapWarnings: number;       // fail if too many "under MIN_GAP" warnings
};

const DEFAULTS: GateConfig = {
  mode: (process.env.QA_MODE as any) === "warn" ? "warn" : "fail",
  minHardGapSec: Number(process.env.QA_MIN_HARD_GAP ?? 0.20),
  maxDenseZoneCount: Number(process.env.QA_MAX_DENSE_COUNT ?? 8),
  maxGapWarnings: Number(process.env.QA_MAX_GAP_WARNINGS ?? 12),
};

const root = process.cwd();
const qaPath = path.join(root, "data", "qa.timeline_report.json");

if (!fs.existsSync(qaPath)) {
  console.error(`❌ Missing QA report: ${qaPath}`);
  process.exit(2);
}

const qa = JSON.parse(fs.readFileSync(qaPath, "utf-8")) as TimelineQA;

const failures: string[] = [];

// Rule 1: hard minimum gap
if (qa.minGapSec < DEFAULTS.minHardGapSec) {
  failures.push(`minGapSec ${qa.minGapSec.toFixed(3)} < hardMin ${DEFAULTS.minHardGapSec.toFixed(2)}`);
}

// Rule 2: dense zones
const worstDense = qa.denseZones.reduce((m, z) => Math.max(m, z.count), 0);
if (worstDense >= DEFAULTS.maxDenseZoneCount) {
  failures.push(`dense zone count ${worstDense} >= maxDense ${DEFAULTS.maxDenseZoneCount}`);
}

// Rule 3: too many warnings
if (qa.gapWarnings.length > DEFAULTS.maxGapWarnings) {
  failures.push(`gapWarnings ${qa.gapWarnings.length} > max ${DEFAULTS.maxGapWarnings}`);
}

// Print summary
console.log("— Timeline QA Gate —");
console.log(`durationSec: ${qa.durationSec.toFixed(2)}`);
console.log(`totalEvents: ${qa.totalEvents}`);
console.log(`minGapSec:   ${qa.minGapSec.toFixed(3)}`);
console.log(`avgGapSec:   ${qa.avgGapSec.toFixed(3)}`);
console.log(`denseZones:  ${qa.denseZones.length} (worst count=${worstDense})`);
console.log(`gapWarnings: ${qa.gapWarnings.length}`);

if (failures.length) {
  console.log("\n⚠️ Gate findings:");
  for (const f of failures) console.log(`- ${f}`);

  if (DEFAULTS.mode === "warn") {
    console.log("\n✅ QA_MODE=warn → continuing");
    process.exit(0);
  } else {
    console.error("\n❌ QA gate failed → stopping pipeline");
    process.exit(1);
  }
}

console.log("\n✅ QA gate passed");
process.exit(0);
3) Add scripts to package.json

{
  "scripts": {
    "qa:timeline": "ts-node tools/qa/timeline-gate.ts",
    "qa:timeline:warn": "QA_MODE=warn ts-node tools/qa/timeline-gate.ts"
  }
}
4) Wire the gate into your hybrid pipeline (recommended)

Put it right after generate-pack.ts wrote the QA report (your generator already writes qa.timeline_report.json).

{
  "scripts": {
    "mc:hybrid:final": "pnpm format:prep && pnpm qa:timeline && pnpm mc:render:auto && pnpm sfx:policy && pnpm sfx:compile && pnpm audio:bus && pnpm mc:render:auto"
  }
}
Now if pacing is insane, you stop before wasting render time.

5) Optional: “warn-only in dev, fail in CI”

Use env vars:

local dev:

QA_MODE=warn pnpm mc:hybrid:final
CI:

QA_MODE=fail pnpm mc:hybrid:final
Or set defaults via your CI pipeline.

6) Optional: Auto-fix mode (next upgrade)

If you want, we can add:

qa:timeline:autofix → when gate fails, automatically re-run:

merge explains

hard clamp explains

re-generate pack

re-check QA
until it passes (max 2 attempts)

Say “autofix mode” and I’ll drop that script too.




ChatGPT can make mistakes. Check important info.

